{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eric-Chung-0511/Learning-Record/blob/main/Data%20Science%20Projects/VisionScout/(Llama_3_2_3B_Instruct)_Vision_Scout_Model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")  # 已新增至Secret\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "CZc4F5T9e9t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVpfCYfq0HUe",
        "outputId": "0cb4c2a3-1c2d-44d1-8db0-a3af93b58c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHxiomUThoEN",
        "outputId": "4e142671-0407-4f80-dd95-5141c60a9360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.4.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch clip git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0477JcxwzrHV",
        "outputId": "b4e1bb6b-9b9b-4ffc-de2b-e91a330bf6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bytg3gdk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bytg3gdk\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: clip in /usr/local/lib/python3.11/dist-packages (1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.3.128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCodZ3KyPKQ",
        "outputId": "1c8af713-5039-470e-f93c-590fb30cbad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics==8.3.128 in /usr/local/lib/python3.11/dist-packages (8.3.128)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.128) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.128) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.128) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.128) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.128) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.128) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.128) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.128) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.128) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics==8.3.128) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.128) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.128) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x5Wbwj0j1FQ",
        "outputId": "8424b702-5d5a-49e4-f0e9-24a881b043ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te0fPfkgvPKa",
        "outputId": "1854c856-d7c5-4cfe-a73d-dd486938129f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-wICk4sFeRA"
      },
      "outputs": [],
      "source": [
        "# %%writefile detection_model.py\n",
        "from ultralytics import YOLO\n",
        "from typing import Any, List, Dict, Optional\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class DetectionModel:\n",
        "    \"\"\"Core detection model class for object detection using YOLOv8\"\"\"\n",
        "\n",
        "    # Model information dictionary\n",
        "    MODEL_INFO = {\n",
        "        \"yolov8n.pt\": {\n",
        "            \"name\": \"YOLOv8n (Nano)\",\n",
        "            \"description\": \"Fastest model with smallest size (3.2M parameters). Best for speed-critical applications.\",\n",
        "            \"size_mb\": 6,\n",
        "            \"inference_speed\": \"Very Fast\"\n",
        "        },\n",
        "        \"yolov8m.pt\": {\n",
        "            \"name\": \"YOLOv8m (Medium)\",\n",
        "            \"description\": \"Balanced model with good accuracy-speed tradeoff (25.9M parameters). Recommended for general use.\",\n",
        "            \"size_mb\": 25,\n",
        "            \"inference_speed\": \"Medium\"\n",
        "        },\n",
        "        \"yolov8x.pt\": {\n",
        "            \"name\": \"YOLOv8x (XLarge)\",\n",
        "            \"description\": \"Most accurate but slower model (68.2M parameters). Best for accuracy-critical applications.\",\n",
        "            \"size_mb\": 68,\n",
        "            \"inference_speed\": \"Slower\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def __init__(self, model_name: str = 'yolov8m.pt', confidence: float = 0.25, iou: float = 0.25):\n",
        "        \"\"\"\n",
        "        Initialize the detection model\n",
        "\n",
        "        Args:\n",
        "            model_name: Model name or path, default is yolov8m.pt\n",
        "            confidence: Confidence threshold, default is 0.25\n",
        "            iou: IoU threshold for non-maximum suppression, default is 0.45\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.confidence = confidence\n",
        "        self.iou = iou\n",
        "        self.model = None\n",
        "        self.class_names = {}\n",
        "        self.is_model_loaded = False\n",
        "\n",
        "        # Load model on initialization\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the YOLO model\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading model: {self.model_name}\")\n",
        "            self.model = YOLO(self.model_name)\n",
        "            self.class_names = self.model.names\n",
        "            self.is_model_loaded = True\n",
        "            print(f\"Successfully loaded model: {self.model_name}\")\n",
        "            print(f\"Number of classes the model can recognize: {len(self.class_names)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred when loading the model: {e}\")\n",
        "            self.is_model_loaded = False\n",
        "\n",
        "    def change_model(self, new_model_name: str) -> bool:\n",
        "        \"\"\"\n",
        "        Change the currently loaded model\n",
        "\n",
        "        Args:\n",
        "            new_model_name: Name of the new model to load\n",
        "\n",
        "        Returns:\n",
        "            bool: True if model changed successfully, False otherwise\n",
        "        \"\"\"\n",
        "        if self.model_name == new_model_name and self.is_model_loaded:\n",
        "            print(f\"Model {new_model_name} is already loaded\")\n",
        "            return True\n",
        "\n",
        "        print(f\"Changing model from {self.model_name} to {new_model_name}\")\n",
        "\n",
        "        # Unload current model to free memory\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            self.model = None\n",
        "\n",
        "            # Clean GPU memory if available\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Update model name and load new model\n",
        "        self.model_name = new_model_name\n",
        "        self._load_model()\n",
        "\n",
        "        return self.is_model_loaded\n",
        "\n",
        "    def reload_model(self):\n",
        "        \"\"\"Reload the model (useful for changing model or after error)\"\"\"\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            self.model = None\n",
        "\n",
        "            # Clean GPU memory if available\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        self._load_model()\n",
        "\n",
        "    def detect(self, image_input: Any) -> Optional[Any]:\n",
        "        \"\"\"\n",
        "        Perform object detection on a single image\n",
        "\n",
        "        Args:\n",
        "            image_input: Image path (str), PIL Image, or numpy array\n",
        "\n",
        "        Returns:\n",
        "            Detection result object or None if error occurred\n",
        "        \"\"\"\n",
        "        if self.model is None or not self.is_model_loaded:\n",
        "            print(\"Model not found or not loaded. Attempting to reload...\")\n",
        "            self._load_model()\n",
        "            if self.model is None or not self.is_model_loaded:\n",
        "                print(\"Failed to load model. Cannot perform detection.\")\n",
        "                return None\n",
        "\n",
        "        try:\n",
        "            results = self.model(image_input, conf=self.confidence, iou=self.iou)\n",
        "            return results[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred during detection: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_class_names(self, class_id: int) -> str:\n",
        "        \"\"\"Get class name for a given class ID\"\"\"\n",
        "        return self.class_names.get(class_id, \"Unknown Class\")\n",
        "\n",
        "    def get_supported_classes(self) -> Dict[int, str]:\n",
        "        \"\"\"Get all supported classes as a dictionary of {id: class_name}\"\"\"\n",
        "        return self.class_names\n",
        "\n",
        "    @classmethod\n",
        "    def get_available_models(cls) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get list of available models with their information\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing model information\n",
        "        \"\"\"\n",
        "        models = []\n",
        "        for model_file, info in cls.MODEL_INFO.items():\n",
        "            models.append({\n",
        "                \"model_file\": model_file,\n",
        "                \"name\": info[\"name\"],\n",
        "                \"description\": info[\"description\"],\n",
        "                \"size_mb\": info[\"size_mb\"],\n",
        "                \"inference_speed\": info[\"inference_speed\"]\n",
        "            })\n",
        "        return models\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_description(cls, model_name: str) -> str:\n",
        "        \"\"\"Get description for a specific model\"\"\"\n",
        "        if model_name in cls.MODEL_INFO:\n",
        "            info = cls.MODEL_INFO[model_name]\n",
        "            return f\"{info['name']}: {info['description']} (Size: ~{info['size_mb']}MB, Speed: {info['inference_speed']})\"\n",
        "        return \"Model information not available\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApUce5S87og"
      },
      "outputs": [],
      "source": [
        "# %%writefile color_mapper.py\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Union, Any\n",
        "\n",
        "class ColorMapper:\n",
        "    \"\"\"\n",
        "    A class for consistent color mapping of object detection classes\n",
        "    Provides color schemes for visualization in both RGB and hex formats\n",
        "    \"\"\"\n",
        "\n",
        "    # Class categories for better organization\n",
        "    CATEGORIES = {\n",
        "        \"person\": [0],\n",
        "        \"vehicles\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "        \"traffic\": [9, 10, 11, 12],\n",
        "        \"animals\": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
        "        \"outdoor\": [13, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
        "        \"sports\": [34, 35, 36, 37, 38],\n",
        "        \"kitchen\": [39, 40, 41, 42, 43, 44, 45],\n",
        "        \"food\": [46, 47, 48, 49, 50, 51, 52, 53, 54, 55],\n",
        "        \"furniture\": [56, 57, 58, 59, 60, 61],\n",
        "        \"electronics\": [62, 63, 64, 65, 66, 67, 68, 69, 70],\n",
        "        \"household\": [71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
        "    }\n",
        "\n",
        "    # Base colors for each category (in HSV for easier variation)\n",
        "    # HSV:  Hue, Saturation, Value\n",
        "    CATEGORY_COLORS = {\n",
        "        \"person\": (0, 0.8, 0.9),       # Red\n",
        "        \"vehicles\": (210, 0.8, 0.9),   # Blue\n",
        "        \"traffic\": (45, 0.8, 0.9),     # Orange\n",
        "        \"animals\": (120, 0.7, 0.8),    # Green\n",
        "        \"outdoor\": (180, 0.7, 0.9),    # Cyan\n",
        "        \"sports\": (270, 0.7, 0.8),     # Purple\n",
        "        \"kitchen\": (30, 0.7, 0.9),     # Light Orange\n",
        "        \"food\": (330, 0.7, 0.85),      # Pink\n",
        "        \"furniture\": (150, 0.5, 0.85), # Light Green\n",
        "        \"electronics\": (240, 0.6, 0.9), # Light Blue\n",
        "        \"household\": (60, 0.6, 0.9)    # Yellow\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the ColorMapper with COCO class mappings\"\"\"\n",
        "        self.class_names = self._get_coco_classes()\n",
        "        self.color_map = self._generate_color_map()\n",
        "\n",
        "    def _get_coco_classes(self) -> Dict[int, str]:\n",
        "        \"\"\"Get the standard COCO class names with their IDs\"\"\"\n",
        "        return {\n",
        "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
        "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
        "            10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
        "            14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
        "            20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
        "            25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee',\n",
        "            30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat',\n",
        "            35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket',\n",
        "            39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife',\n",
        "            44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich',\n",
        "            49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza',\n",
        "            54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant',\n",
        "            59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop',\n",
        "            64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave',\n",
        "            69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book',\n",
        "            74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier',\n",
        "            79: 'toothbrush'\n",
        "        }\n",
        "\n",
        "    def _hsv_to_rgb(self, h: float, s: float, v: float) -> Tuple[int, int, int]:\n",
        "        \"\"\"\n",
        "        Convert HSV color to RGB\n",
        "\n",
        "        Args:\n",
        "            h: Hue (0-360)\n",
        "            s: Saturation (0-1)\n",
        "            v: Value (0-1)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (R, G, B) values (0-255)\n",
        "        \"\"\"\n",
        "        h = h / 60\n",
        "        i = int(h)\n",
        "        f = h - i\n",
        "        p = v * (1 - s)\n",
        "        q = v * (1 - s * f)\n",
        "        t = v * (1 - s * (1 - f))\n",
        "\n",
        "        if i == 0:\n",
        "            r, g, b = v, t, p\n",
        "        elif i == 1:\n",
        "            r, g, b = q, v, p\n",
        "        elif i == 2:\n",
        "            r, g, b = p, v, t\n",
        "        elif i == 3:\n",
        "            r, g, b = p, q, v\n",
        "        elif i == 4:\n",
        "            r, g, b = t, p, v\n",
        "        else:\n",
        "            r, g, b = v, p, q\n",
        "\n",
        "        return (int(r * 255), int(g * 255), int(b * 255))\n",
        "\n",
        "    def _rgb_to_hex(self, rgb: Tuple[int, int, int]) -> str:\n",
        "        \"\"\"\n",
        "        Convert RGB color to hex color code\n",
        "\n",
        "        Args:\n",
        "            rgb: Tuple of (R, G, B) values (0-255)\n",
        "\n",
        "        Returns:\n",
        "            Hex color code (e.g. '#FF0000')\n",
        "        \"\"\"\n",
        "        return f'#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}'\n",
        "\n",
        "    def _find_category(self, class_id: int) -> str:\n",
        "        \"\"\"\n",
        "        Find the category for a given class ID\n",
        "\n",
        "        Args:\n",
        "            class_id: Class ID (0-79)\n",
        "\n",
        "        Returns:\n",
        "            Category name\n",
        "        \"\"\"\n",
        "        for category, ids in self.CATEGORIES.items():\n",
        "            if class_id in ids:\n",
        "                return category\n",
        "        return \"other\"  # Fallback\n",
        "\n",
        "    def _generate_color_map(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate a color map for all 80 COCO classes\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping class IDs and names to color values\n",
        "        \"\"\"\n",
        "        color_map = {\n",
        "            'by_id': {},      # Map class ID to RGB and hex\n",
        "            'by_name': {},    # Map class name to RGB and hex\n",
        "            'categories': {}  # Map category to base color\n",
        "        }\n",
        "\n",
        "        # Generate colors for categories\n",
        "        for category, hsv in self.CATEGORY_COLORS.items():\n",
        "            rgb = self._hsv_to_rgb(hsv[0], hsv[1], hsv[2])\n",
        "            hex_color = self._rgb_to_hex(rgb)\n",
        "            color_map['categories'][category] = {\n",
        "                'rgb': rgb,\n",
        "                'hex': hex_color\n",
        "            }\n",
        "\n",
        "        # Generate variations for each class within a category\n",
        "        for class_id, class_name in self.class_names.items():\n",
        "            category = self._find_category(class_id)\n",
        "            base_hsv = self.CATEGORY_COLORS.get(category, (0, 0, 0.8))  # Default gray\n",
        "\n",
        "            # Slightly vary the hue and saturation within the category\n",
        "            ids_in_category = self.CATEGORIES.get(category, [])\n",
        "            if ids_in_category:\n",
        "                position = ids_in_category.index(class_id) if class_id in ids_in_category else 0\n",
        "                variation = position / max(1, len(ids_in_category) - 1)  # 0 to 1\n",
        "\n",
        "                # Vary hue slightly (±15°) and saturation\n",
        "                h_offset = 30 * variation - 15  # -15 to +15\n",
        "                s_offset = 0.2 * variation  # 0 to 0.2\n",
        "\n",
        "                h = (base_hsv[0] + h_offset) % 360\n",
        "                s = min(1.0, base_hsv[1] + s_offset)\n",
        "                v = base_hsv[2]\n",
        "            else:\n",
        "                h, s, v = base_hsv\n",
        "\n",
        "            rgb = self._hsv_to_rgb(h, s, v)\n",
        "            hex_color = self._rgb_to_hex(rgb)\n",
        "\n",
        "            # Store in both mappings\n",
        "            color_map['by_id'][class_id] = {\n",
        "                'rgb': rgb,\n",
        "                'hex': hex_color,\n",
        "                'category': category\n",
        "            }\n",
        "\n",
        "            color_map['by_name'][class_name] = {\n",
        "                'rgb': rgb,\n",
        "                'hex': hex_color,\n",
        "                'category': category\n",
        "            }\n",
        "\n",
        "        return color_map\n",
        "\n",
        "    def get_color(self, class_identifier: Union[int, str], format: str = 'hex') -> Any:\n",
        "        \"\"\"\n",
        "        Get color for a specific class\n",
        "\n",
        "        Args:\n",
        "            class_identifier: Class ID (int) or name (str)\n",
        "            format: Color format ('hex', 'rgb', or 'bgr')\n",
        "\n",
        "        Returns:\n",
        "            Color in requested format\n",
        "        \"\"\"\n",
        "        # Determine if identifier is an ID or name\n",
        "        if isinstance(class_identifier, int):\n",
        "            color_info = self.color_map['by_id'].get(class_identifier)\n",
        "        else:\n",
        "            color_info = self.color_map['by_name'].get(class_identifier)\n",
        "\n",
        "        if not color_info:\n",
        "            # Fallback color if not found\n",
        "            return '#CCCCCC' if format == 'hex' else (204, 204, 204)\n",
        "\n",
        "        if format == 'hex':\n",
        "            return color_info['hex']\n",
        "        elif format == 'rgb':\n",
        "            return color_info['rgb']\n",
        "        elif format == 'bgr':\n",
        "            # Convert RGB to BGR for OpenCV\n",
        "            r, g, b = color_info['rgb']\n",
        "            return (b, g, r)\n",
        "        else:\n",
        "            return color_info['rgb']\n",
        "\n",
        "    def get_all_colors(self, format: str = 'hex') -> Dict:\n",
        "        \"\"\"\n",
        "        Get all colors in the specified format\n",
        "\n",
        "        Args:\n",
        "            format: Color format ('hex', 'rgb', or 'bgr')\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping class names to colors\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        for class_id, class_name in self.class_names.items():\n",
        "            result[class_name] = self.get_color(class_id, format)\n",
        "        return result\n",
        "\n",
        "    def get_category_colors(self, format: str = 'hex') -> Dict:\n",
        "        \"\"\"\n",
        "        Get base colors for each category\n",
        "\n",
        "        Args:\n",
        "            format: Color format ('hex', 'rgb', or 'bgr')\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping categories to colors\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        for category, color_info in self.color_map['categories'].items():\n",
        "            if format == 'hex':\n",
        "                result[category] = color_info['hex']\n",
        "            elif format == 'bgr':\n",
        "                r, g, b = color_info['rgb']\n",
        "                result[category] = (b, g, r)\n",
        "            else:\n",
        "                result[category] = color_info['rgb']\n",
        "        return result\n",
        "\n",
        "    def get_category_for_class(self, class_identifier: Union[int, str]) -> str:\n",
        "        \"\"\"\n",
        "        Get the category for a specific class\n",
        "\n",
        "        Args:\n",
        "            class_identifier: Class ID (int) or name (str)\n",
        "\n",
        "        Returns:\n",
        "            Category name\n",
        "        \"\"\"\n",
        "        if isinstance(class_identifier, int):\n",
        "            return self.color_map['by_id'].get(class_identifier, {}).get('category', 'other')\n",
        "        else:\n",
        "            return self.color_map['by_name'].get(class_identifier, {}).get('category', 'other')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZMzmZVD-r6E"
      },
      "outputs": [],
      "source": [
        "# %%writefile visualization_helper.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as path_effects\n",
        "from typing import Any, List, Dict, Tuple, Optional\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "class VisualizationHelper:\n",
        "    \"\"\"Helper class for visualizing detection results\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_detection(image: Any, result: Any, color_mapper: Optional[Any] = None,\n",
        "                            figsize: Tuple[int, int] = (12, 12),\n",
        "                            return_pil: bool = False,\n",
        "                            filter_classes: Optional[List[int]] = None) -> Optional[Image.Image]:\n",
        "        \"\"\"\n",
        "        Visualize detection results on a single image\n",
        "\n",
        "        Args:\n",
        "            image: Image path or numpy array\n",
        "            result: Detection result object\n",
        "            color_mapper: ColorMapper instance for consistent colors\n",
        "            figsize: Figure size\n",
        "            return_pil: If True, returns a PIL Image object\n",
        "\n",
        "        Returns:\n",
        "            PIL Image if return_pil is True, otherwise displays the plot\n",
        "        \"\"\"\n",
        "        if result is None:\n",
        "            print('No data for visualization')\n",
        "            return None\n",
        "\n",
        "        # Read image if path is provided\n",
        "        if isinstance(image, str):\n",
        "            img = cv2.imread(image)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            img = image\n",
        "            if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "                # Check if BGR format (OpenCV) and convert to RGB if needed\n",
        "                if isinstance(img, np.ndarray):\n",
        "                    # Assuming BGR format from OpenCV\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Get bounding boxes, classes and confidences\n",
        "        boxes = result.boxes.xyxy.cpu().numpy()\n",
        "        classes = result.boxes.cls.cpu().numpy()\n",
        "        confs = result.boxes.conf.cpu().numpy()\n",
        "\n",
        "        # Get class names\n",
        "        names = result.names\n",
        "\n",
        "        # Create a default color mapper if none is provided\n",
        "        if color_mapper is None:\n",
        "            # For backward compatibility, fallback to a simple color function\n",
        "            from matplotlib import colormaps\n",
        "            cmap = colormaps['tab10']\n",
        "            def get_color(class_id):\n",
        "                return cmap(class_id % 10)\n",
        "        else:\n",
        "            # Use the provided color mapper\n",
        "            def get_color(class_id):\n",
        "                hex_color = color_mapper.get_color(class_id)\n",
        "                # Convert hex to RGB float values for matplotlib\n",
        "                hex_color = hex_color.lstrip('#')\n",
        "                return tuple(int(hex_color[i:i+2], 16) / 255 for i in (0, 2, 4)) + (1.0,)\n",
        "\n",
        "        # Draw detection results\n",
        "        for box, cls, conf in zip(boxes, classes, confs):\n",
        "            x1, y1, x2, y2 = box\n",
        "            cls_id = int(cls)\n",
        "\n",
        "            if filter_classes and cls_id not in filter_classes:\n",
        "                continue\n",
        "\n",
        "            cls_name = names[cls_id]\n",
        "\n",
        "            # Get color for this class\n",
        "            box_color = get_color(cls_id)\n",
        "\n",
        "            box_width = x2 - x1\n",
        "            box_height = y2 - y1\n",
        "            box_area = box_width * box_height\n",
        "\n",
        "            # 根據框大小調整字體大小，但有限制\n",
        "            adaptive_fontsize = max(10, min(14, int(10 + box_area / 10000)))\n",
        "\n",
        "\n",
        "            ax.text(x1, y1 - 8, f'{cls_name}: {conf:.2f}',\n",
        "                    color='white', fontsize=adaptive_fontsize, fontweight=\"bold\",\n",
        "                    bbox=dict(facecolor=box_color[:3], alpha=0.85, pad=3, boxstyle=\"round,pad=0.3\"),\n",
        "                    path_effects=[path_effects.withStroke(linewidth=1.5, foreground=\"black\")])\n",
        "\n",
        "            # Add bounding box\n",
        "            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                    fill=False, edgecolor=box_color[:3], linewidth=2))\n",
        "\n",
        "        ax.axis('off')\n",
        "        # ax.set_title('Detection Result')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if return_pil:\n",
        "            # Convert plot to PIL Image\n",
        "            buf = io.BytesIO()\n",
        "            fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
        "            buf.seek(0)\n",
        "            pil_img = Image.open(buf)\n",
        "            plt.close(fig)\n",
        "            return pil_img\n",
        "        else:\n",
        "            plt.show()\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def create_summary(result: Any) -> Dict:\n",
        "        \"\"\"\n",
        "        Create a summary of detection results\n",
        "\n",
        "        Args:\n",
        "            result: Detection result object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with detection summary statistics\n",
        "        \"\"\"\n",
        "        if result is None:\n",
        "            return {\"error\": \"No detection result provided\"}\n",
        "\n",
        "        # Get classes and confidences\n",
        "        classes = result.boxes.cls.cpu().numpy().astype(int)\n",
        "        confidences = result.boxes.conf.cpu().numpy()\n",
        "        names = result.names\n",
        "\n",
        "        # Count detections by class\n",
        "        class_counts = {}\n",
        "        for cls, conf in zip(classes, confidences):\n",
        "            cls_name = names[int(cls)]\n",
        "            if cls_name not in class_counts:\n",
        "                class_counts[cls_name] = {\"count\": 0, \"confidences\": []}\n",
        "\n",
        "            class_counts[cls_name][\"count\"] += 1\n",
        "            class_counts[cls_name][\"confidences\"].append(float(conf))\n",
        "\n",
        "        # Calculate average confidence for each class\n",
        "        for cls_name, stats in class_counts.items():\n",
        "            if stats[\"confidences\"]:\n",
        "                stats[\"average_confidence\"] = float(np.mean(stats[\"confidences\"]))\n",
        "                stats.pop(\"confidences\")  # Remove detailed confidences list to keep summary concise\n",
        "\n",
        "        # Prepare summary\n",
        "        summary = {\n",
        "            \"total_objects\": len(classes),\n",
        "            \"class_counts\": class_counts,\n",
        "            \"unique_classes\": len(class_counts)\n",
        "        }\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxZyG9wX80Og"
      },
      "outputs": [],
      "source": [
        "# %%writefile evaluation_metrics.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"Class for computing detection metrics, generating statistics and visualization data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_basic_stats(result: Any) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate basic statistics for a single detection result\n",
        "\n",
        "        Args:\n",
        "            result: Detection result object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with basic statistics\n",
        "        \"\"\"\n",
        "        if result is None:\n",
        "            return {\"error\": \"No detection result provided\"}\n",
        "\n",
        "        # Get classes and confidences\n",
        "        classes = result.boxes.cls.cpu().numpy().astype(int)\n",
        "        confidences = result.boxes.conf.cpu().numpy()\n",
        "        names = result.names\n",
        "\n",
        "        # Count by class\n",
        "        class_counts = {}\n",
        "        for cls, conf in zip(classes, confidences):\n",
        "            cls_name = names[int(cls)]\n",
        "            if cls_name not in class_counts:\n",
        "                class_counts[cls_name] = {\"count\": 0, \"total_confidence\": 0, \"confidences\": []}\n",
        "\n",
        "            class_counts[cls_name][\"count\"] += 1\n",
        "            class_counts[cls_name][\"total_confidence\"] += float(conf)\n",
        "            class_counts[cls_name][\"confidences\"].append(float(conf))\n",
        "\n",
        "        # Calculate average confidence\n",
        "        for cls_name, stats in class_counts.items():\n",
        "            if stats[\"count\"] > 0:\n",
        "                stats[\"average_confidence\"] = stats[\"total_confidence\"] / stats[\"count\"]\n",
        "                stats[\"confidence_std\"] = float(np.std(stats[\"confidences\"])) if len(stats[\"confidences\"]) > 1 else 0\n",
        "                stats.pop(\"total_confidence\")  # Remove intermediate calculation\n",
        "\n",
        "        # Prepare summary\n",
        "        stats = {\n",
        "            \"total_objects\": len(classes),\n",
        "            \"class_statistics\": class_counts,\n",
        "            \"average_confidence\": float(np.mean(confidences)) if len(confidences) > 0 else 0\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_visualization_data(result: Any, class_colors: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate structured data suitable for visualization\n",
        "\n",
        "        Args:\n",
        "            result: Detection result object\n",
        "            class_colors: Dictionary mapping class names to color codes (optional)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with visualization-ready data\n",
        "        \"\"\"\n",
        "        if result is None:\n",
        "            return {\"error\": \"No detection result provided\"}\n",
        "\n",
        "        # Get basic stats first\n",
        "        stats = EvaluationMetrics.calculate_basic_stats(result)\n",
        "\n",
        "        # Create visualization-specific data structure\n",
        "        viz_data = {\n",
        "            \"total_objects\": stats[\"total_objects\"],\n",
        "            \"average_confidence\": stats[\"average_confidence\"],\n",
        "            \"class_data\": []\n",
        "        }\n",
        "\n",
        "        # Sort classes by count (descending)\n",
        "        sorted_classes = sorted(\n",
        "            stats[\"class_statistics\"].items(),\n",
        "            key=lambda x: x[1][\"count\"],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Create class-specific visualization data\n",
        "        for cls_name, cls_stats in sorted_classes:\n",
        "            class_id = -1\n",
        "            # Find the class ID based on the name\n",
        "            for idx, name in result.names.items():\n",
        "                if name == cls_name:\n",
        "                    class_id = idx\n",
        "                    break\n",
        "\n",
        "            cls_data = {\n",
        "                \"name\": cls_name,\n",
        "                \"class_id\": class_id,\n",
        "                \"count\": cls_stats[\"count\"],\n",
        "                \"average_confidence\": cls_stats.get(\"average_confidence\", 0),\n",
        "                \"confidence_std\": cls_stats.get(\"confidence_std\", 0),\n",
        "                \"color\": class_colors.get(cls_name, \"#CCCCCC\") if class_colors else \"#CCCCCC\"\n",
        "            }\n",
        "\n",
        "            viz_data[\"class_data\"].append(cls_data)\n",
        "\n",
        "        return viz_data\n",
        "\n",
        "    @staticmethod\n",
        "    def create_stats_plot(viz_data: Dict, figsize: Tuple[int, int] = (10, 7), max_classes: int = 30) -> plt.Figure:\n",
        "        \"\"\"\n",
        "        Create a horizontal bar chart showing detection statistics\n",
        "\n",
        "        Args:\n",
        "            viz_data: Visualization data generated by generate_visualization_data\n",
        "            figsize: Figure size (width, height) in inches\n",
        "            max_classes: Maximum number of classes to display\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure object\n",
        "        \"\"\"\n",
        "        # Use the enhanced version\n",
        "        return EvaluationMetrics.create_enhanced_stats_plot(viz_data, figsize, max_classes)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_enhanced_stats_plot(viz_data: Dict, figsize: Tuple[int, int] = (10, 7), max_classes: int = 30) -> plt.Figure:\n",
        "        \"\"\"\n",
        "        Create an enhanced horizontal bar chart with larger fonts and better styling\n",
        "\n",
        "        Args:\n",
        "            viz_data: Visualization data dictionary\n",
        "            figsize: Figure size (width, height) in inches\n",
        "            max_classes: Maximum number of classes to display\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure with enhanced styling\n",
        "        \"\"\"\n",
        "        if \"error\" in viz_data:\n",
        "            # Create empty plot if error\n",
        "            fig, ax = plt.subplots(figsize=figsize)\n",
        "            ax.text(0.5, 0.5, viz_data[\"error\"],\n",
        "                    ha='center', va='center', fontsize=14)\n",
        "            ax.set_xlim(0, 1)\n",
        "            ax.set_ylim(0, 1)\n",
        "            ax.axis('off')\n",
        "            return fig\n",
        "\n",
        "        if \"class_data\" not in viz_data or not viz_data[\"class_data\"]:\n",
        "            # Create empty plot if no data\n",
        "            fig, ax = plt.subplots(figsize=figsize)\n",
        "            ax.text(0.5, 0.5, \"No detection data available\",\n",
        "                    ha='center', va='center', fontsize=14)\n",
        "            ax.set_xlim(0, 1)\n",
        "            ax.set_ylim(0, 1)\n",
        "            ax.axis('off')\n",
        "            return fig\n",
        "\n",
        "        # Limit to max_classes\n",
        "        class_data = viz_data[\"class_data\"][:max_classes]\n",
        "\n",
        "        # Extract data for plotting\n",
        "        class_names = [item[\"name\"] for item in class_data]\n",
        "        counts = [item[\"count\"] for item in class_data]\n",
        "        colors = [item[\"color\"] for item in class_data]\n",
        "\n",
        "        # Create figure and horizontal bar chart with improved styling\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "        # Set background color to white\n",
        "        fig.patch.set_facecolor('white')\n",
        "        ax.set_facecolor('white')\n",
        "\n",
        "        y_pos = np.arange(len(class_names))\n",
        "\n",
        "        # Create horizontal bars with class-specific colors\n",
        "        bars = ax.barh(y_pos, counts, color=colors, alpha=0.8, height=0.6)\n",
        "\n",
        "        # Add count values at end of each bar with larger font\n",
        "        for i, bar in enumerate(bars):\n",
        "            width = bar.get_width()\n",
        "            conf = class_data[i][\"average_confidence\"]\n",
        "            ax.text(width + 0.3, bar.get_y() + bar.get_height()/2,\n",
        "                    f\"{width:.0f} (conf: {conf:.2f})\",\n",
        "                    va='center', fontsize=12)\n",
        "\n",
        "        # Customize axis and labels with larger fonts\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(class_names, fontsize=14)\n",
        "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
        "        ax.set_xlabel('Count', fontsize=14)\n",
        "        ax.set_title(f'Objects Detected: {viz_data[\"total_objects\"]} Total',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Add grid for better readability\n",
        "        ax.set_axisbelow(True)\n",
        "        ax.grid(axis='x', linestyle='--', alpha=0.7, color='#E5E7EB')\n",
        "\n",
        "        # Increase tick label font size\n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "        # Add detection summary as a text box with improved styling\n",
        "        summary_text = (\n",
        "            f\"Total Objects: {viz_data['total_objects']}\\n\"\n",
        "            f\"Average Confidence: {viz_data['average_confidence']:.2f}\\n\"\n",
        "            f\"Unique Classes: {len(viz_data['class_data'])}\"\n",
        "        )\n",
        "        plt.figtext(0.02, 0.02, summary_text, fontsize=12,\n",
        "                bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.5',\n",
        "                            edgecolor='#E5E7EB'))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def format_detection_summary(viz_data: Dict) -> str:\n",
        "        if \"error\" in viz_data:\n",
        "            return viz_data[\"error\"]\n",
        "\n",
        "        if \"total_objects\" not in viz_data:\n",
        "            return \"No detection data available.\"\n",
        "\n",
        "        total_objects = viz_data[\"total_objects\"]\n",
        "        avg_confidence = viz_data[\"average_confidence\"]\n",
        "\n",
        "        lines = [\n",
        "            f\"Detected {total_objects} objects.\",\n",
        "            f\"Average confidence: {avg_confidence:.2f}\",\n",
        "            \"Objects by class:\"\n",
        "        ]\n",
        "\n",
        "        if \"class_data\" in viz_data and viz_data[\"class_data\"]:\n",
        "            for item in viz_data[\"class_data\"]:\n",
        "                count = item['count']\n",
        "                item_text = \"item\" if count == 1 else \"items\"\n",
        "                lines.append(f\"• {item['name']}: {count} {item_text} (Confidence: {item['average_confidence']:.2f})\")\n",
        "        else:\n",
        "            lines.append(\"No class information available.\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_distance_metrics(result: Any) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate distance-related metrics for detected objects\n",
        "\n",
        "        Args:\n",
        "            result: Detection result object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with distance metrics\n",
        "        \"\"\"\n",
        "        if result is None:\n",
        "            return {\"error\": \"No detection result provided\"}\n",
        "\n",
        "        boxes = result.boxes.xyxy.cpu().numpy()\n",
        "        classes = result.boxes.cls.cpu().numpy().astype(int)\n",
        "        names = result.names\n",
        "\n",
        "        # Initialize metrics\n",
        "        metrics = {\n",
        "            \"proximity\": {},  # Classes that appear close to each other\n",
        "            \"spatial_distribution\": {},  # Distribution across the image\n",
        "            \"size_distribution\": {}  # Size distribution of objects\n",
        "        }\n",
        "\n",
        "        # Calculate image dimensions (assuming normalized coordinates or extract from result)\n",
        "        img_width, img_height = 1, 1\n",
        "        if hasattr(result, \"orig_shape\"):\n",
        "            img_height, img_width = result.orig_shape[:2]\n",
        "\n",
        "        # Calculate bounding box areas and centers\n",
        "        areas = []\n",
        "        centers = []\n",
        "        class_names = []\n",
        "\n",
        "        for box, cls in zip(boxes, classes):\n",
        "            x1, y1, x2, y2 = box\n",
        "            width, height = x2 - x1, y2 - y1\n",
        "            area = width * height\n",
        "            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "\n",
        "            areas.append(area)\n",
        "            centers.append((center_x, center_y))\n",
        "            class_names.append(names[int(cls)])\n",
        "\n",
        "        # Calculate spatial distribution\n",
        "        if centers:\n",
        "            x_coords = [c[0] for c in centers]\n",
        "            y_coords = [c[1] for c in centers]\n",
        "\n",
        "            metrics[\"spatial_distribution\"] = {\n",
        "                \"x_mean\": float(np.mean(x_coords)) / img_width,\n",
        "                \"y_mean\": float(np.mean(y_coords)) / img_height,\n",
        "                \"x_std\": float(np.std(x_coords)) / img_width,\n",
        "                \"y_std\": float(np.std(y_coords)) / img_height\n",
        "            }\n",
        "\n",
        "        # Calculate size distribution\n",
        "        if areas:\n",
        "            metrics[\"size_distribution\"] = {\n",
        "                \"mean_area\": float(np.mean(areas)) / (img_width * img_height),\n",
        "                \"std_area\": float(np.std(areas)) / (img_width * img_height),\n",
        "                \"min_area\": float(np.min(areas)) / (img_width * img_height),\n",
        "                \"max_area\": float(np.max(areas)) / (img_width * img_height)\n",
        "            }\n",
        "\n",
        "        # Calculate proximity between different classes\n",
        "        class_centers = {}\n",
        "        for cls_name, center in zip(class_names, centers):\n",
        "            if cls_name not in class_centers:\n",
        "                class_centers[cls_name] = []\n",
        "            class_centers[cls_name].append(center)\n",
        "\n",
        "        # Find classes that appear close to each other\n",
        "        proximity_pairs = []\n",
        "        for i, cls1 in enumerate(class_centers.keys()):\n",
        "            for j, cls2 in enumerate(class_centers.keys()):\n",
        "                if i >= j:  # Avoid duplicate pairs and self-comparison\n",
        "                    continue\n",
        "\n",
        "                # Calculate minimum distance between any two objects of these classes\n",
        "                min_distance = float('inf')\n",
        "                for center1 in class_centers[cls1]:\n",
        "                    for center2 in class_centers[cls2]:\n",
        "                        dist = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
        "                        min_distance = min(min_distance, dist)\n",
        "\n",
        "                # Normalize by image diagonal\n",
        "                img_diagonal = np.sqrt(img_width**2 + img_height**2)\n",
        "                norm_distance = min_distance / img_diagonal\n",
        "\n",
        "                proximity_pairs.append({\n",
        "                    \"class1\": cls1,\n",
        "                    \"class2\": cls2,\n",
        "                    \"distance\": float(norm_distance)\n",
        "                })\n",
        "\n",
        "        # Sort by distance and keep the closest pairs\n",
        "        proximity_pairs.sort(key=lambda x: x[\"distance\"])\n",
        "        metrics[\"proximity\"] = proximity_pairs[:5]  # Keep top 5 closest pairs\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHIMZpcZwWGU"
      },
      "outputs": [],
      "source": [
        "# %%writefile style.py\n",
        "\n",
        "class Style:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_css():\n",
        "\n",
        "        css = \"\"\"\n",
        "        /* Base styles and typography */\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            background: linear-gradient(135deg, #f0f9ff, #e1f5fe);\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            min-height: 100vh;\n",
        "        }\n",
        "\n",
        "        /* Typography improvements */\n",
        "        h1, h2, h3, h4, h5, h6, p, span, div, label, button {\n",
        "            font-family: Arial, sans-serif;\n",
        "        }\n",
        "\n",
        "        /* Container styling */\n",
        "        .gradio-container {\n",
        "            max-width: 1200px !important;\n",
        "            margin: auto !important;\n",
        "            padding: 1rem;\n",
        "            width: 100%;\n",
        "        }\n",
        "\n",
        "        /* Header area styling with gradient background */\n",
        "        .app-header {\n",
        "            text-align: center;\n",
        "            margin-bottom: 2rem;\n",
        "            background: linear-gradient(135deg, #f8f9fa, #e9ecef);\n",
        "            padding: 1.5rem;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);\n",
        "            width: 100%;\n",
        "        }\n",
        "\n",
        "        .app-title {\n",
        "            color: #2D3748;\n",
        "            font-size: 2.5rem;\n",
        "            margin-bottom: 0.5rem;\n",
        "            background: linear-gradient(90deg, #38b2ac, #4299e1);\n",
        "            -webkit-background-clip: text;\n",
        "            -webkit-text-fill-color: transparent;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "\n",
        "        .app-subtitle {\n",
        "            color: #4A5568;\n",
        "            font-size: 1.2rem;\n",
        "            font-weight: normal;\n",
        "            margin-top: 0.25rem;\n",
        "        }\n",
        "\n",
        "        .app-divider {\n",
        "            width: 80px;\n",
        "            height: 3px;\n",
        "            background: linear-gradient(90deg, #38b2ac, #4299e1);\n",
        "            margin: 1rem auto;\n",
        "        }\n",
        "\n",
        "        /* Panel styling - gradient background */\n",
        "        .input-panel, .output-panel {\n",
        "            background: white;\n",
        "            border-radius: 10px;\n",
        "            padding: 1.5rem;\n",
        "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);\n",
        "            margin: 0 auto 1rem auto;\n",
        "        }\n",
        "\n",
        "        /* 修改輸出面板確保內容能夠完整顯示 */\n",
        "        .output-panel {\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            width: 100%;\n",
        "            padding: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* 確保輸出面板內的元素寬度可以適應面板 */\n",
        "        .output-panel > * {\n",
        "            width: 100%;\n",
        "        }\n",
        "\n",
        "        /* How-to-use section with gradient background */\n",
        "        .how-to-use {\n",
        "            background: linear-gradient(135deg, #f8fafc, #e8f4fd);\n",
        "            border-radius: 10px;\n",
        "            padding: 1.5rem;\n",
        "            margin-top: 1rem;\n",
        "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);\n",
        "            color: #2d3748;\n",
        "        }\n",
        "\n",
        "        /* Detection button styling */\n",
        "        .detect-btn {\n",
        "            background: linear-gradient(90deg, #38b2ac, #4299e1) !important;\n",
        "            color: white !important;\n",
        "            border: none !important;\n",
        "            border-radius: 8px !important;\n",
        "            transition: transform 0.3s, box-shadow 0.3s !important;\n",
        "            font-weight: bold !important;\n",
        "            letter-spacing: 0.5px !important;\n",
        "            padding: 0.75rem 1.5rem !important;\n",
        "            width: 100%;\n",
        "            margin: 1rem auto !important;\n",
        "            font-family: Arial, sans-serif !important;\n",
        "        }\n",
        "\n",
        "        .detect-btn:hover {\n",
        "            transform: translateY(-2px) !important;\n",
        "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2) !important;\n",
        "        }\n",
        "\n",
        "        .detect-btn:active {\n",
        "            transform: translateY(1px) !important;\n",
        "            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2) !important;\n",
        "        }\n",
        "\n",
        "        /* JSON display improvements */\n",
        "        .json-display {\n",
        "            width: 98% !important;\n",
        "            margin: 0.5rem auto 1.5rem auto !important;\n",
        "            padding: 1rem !important;\n",
        "            border-radius: 8px !important;\n",
        "            background-color: white !important;\n",
        "            border: 1px solid #E2E8F0 !important;\n",
        "            box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.05) !important;\n",
        "        }\n",
        "\n",
        "        .json-key {\n",
        "            color: #e53e3e;\n",
        "        }\n",
        "\n",
        "        .json-value {\n",
        "            color: #2b6cb0;\n",
        "        }\n",
        "\n",
        "        .json-string {\n",
        "            color: #38a169;\n",
        "        }\n",
        "\n",
        "        /* Chart/plot styling improvements */\n",
        "        .plot-container {\n",
        "            background: white;\n",
        "            border-radius: 8px;\n",
        "            padding: 0.5rem;\n",
        "            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);\n",
        "        }\n",
        "\n",
        "        /* Larger font for plots */\n",
        "        .plot-container text {\n",
        "            font-family: Arial, sans-serif !important;\n",
        "            font-size: 14px !important;\n",
        "        }\n",
        "\n",
        "        /* Title styling for charts */\n",
        "        .plot-title {\n",
        "            font-family: Arial, sans-serif !important;\n",
        "            font-size: 16px !important;\n",
        "            font-weight: bold !important;\n",
        "        }\n",
        "\n",
        "        /* Tab styling with subtle gradient */\n",
        "        .tabs {\n",
        "            width: 100%;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "        }\n",
        "\n",
        "        .tabs > div:first-child {\n",
        "            background: linear-gradient(to right, #f8fafc, #e8f4fd) !important;\n",
        "            border-radius: 8px 8px 0 0;\n",
        "        }\n",
        "\n",
        "        /* Tab content styling - 確保內容區域有足夠寬度 */\n",
        "        .tab-content {\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* Footer styling with gradient background */\n",
        "        .footer {\n",
        "            text-align: center;\n",
        "            margin-top: 2rem;\n",
        "            font-size: 0.9rem;\n",
        "            color: #4A5568;\n",
        "            padding: 1rem;\n",
        "            background: linear-gradient(135deg, #f8f9fa, #e1effe);\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);\n",
        "            width: 100%;\n",
        "        }\n",
        "\n",
        "        /* Ensure centering works for all elements */\n",
        "        .container, .gr-container, .gr-row, .gr-col {\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            align-items: center;\n",
        "            justify-content: center;\n",
        "            width: 100%;\n",
        "        }\n",
        "\n",
        "        /* 統一文本框樣式，確保寬度一致 */\n",
        "        .gr-textbox, .gr-textarea, .gr-text-input {\n",
        "            width: 100% !important;\n",
        "            max-width: 100% !important;\n",
        "            min-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* 確保文本區域可以適應容器寬度 */\n",
        "        textarea.gr-textarea, .gr-textbox textarea, .gr-text-input textarea {\n",
        "            width: 100% !important;\n",
        "            max-width: 100% !important;\n",
        "            min-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 16px !important;\n",
        "            font-family: 'Arial', sans-serif !important;\n",
        "            font-size: 14px !important;\n",
        "            line-height: 1.6 !important;\n",
        "            white-space: pre-wrap !important;\n",
        "            word-wrap: break-word !important;\n",
        "            word-break: normal !important;\n",
        "        }\n",
        "\n",
        "        /* 特別針對場景描述文本框樣式增強 */\n",
        "        #scene-description-text, #detection-details {\n",
        "            width: 100% !important;\n",
        "            min-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 16px !important;\n",
        "            line-height: 1.8 !important;\n",
        "            white-space: pre-wrap !important;\n",
        "            word-wrap: break-word !important;\n",
        "            border-radius: 8px !important;\n",
        "            min-height: 250px !important;\n",
        "            overflow-y: auto !important;\n",
        "            border: 1px solid #e2e8f0 !important;\n",
        "            background-color: white !important;\n",
        "            display: block !important;\n",
        "            font-family: 'Arial', sans-serif !important;\n",
        "            font-size: 14px !important;\n",
        "            margin: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* 針對場景描述容器的樣式 */\n",
        "        .scene-description-container {\n",
        "            width: 100% !important;\n",
        "            max-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 0 !important;\n",
        "            margin: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* Scene Understanding Tab 特定樣式 */\n",
        "        .scene-understanding-tab .result-details-box {\n",
        "            display: flex !important;\n",
        "            flex-direction: column !important;\n",
        "            align-items: stretch !important;\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* 場景分析描述區域樣式 */\n",
        "        .scene-description-box {\n",
        "            background-color: #f8f9fa !important;\n",
        "            border: 1px solid #e2e8f0 !important;\n",
        "            border-radius: 8px !important;\n",
        "            padding: 15px !important;\n",
        "            margin: 10px 0 20px 0 !important;\n",
        "            box-shadow: 0 1px 3px rgba(0,0,0,0.05) !important;\n",
        "            font-family: Arial, sans-serif !important;\n",
        "            line-height: 1.7 !important;\n",
        "            color: #2D3748 !important;\n",
        "            font-size: 16px !important;\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        #scene_analysis_description_text {\n",
        "            background-color: #f0f0f0 !important; /* 淺灰色背景 */\n",
        "            padding: 15px !important;             /* 內邊距，讓文字和邊框有點空間 */\n",
        "            border-radius: 8px !important;        /* 圓角 */\n",
        "            margin: 10px 0 20px 0 !important;     /* 其他元素的間距，特別是上下的part */\n",
        "            display: block !important;\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        #scene_analysis_description_text p {\n",
        "            margin: 0 !important;\n",
        "            color: #2D3748 !important; /* 確保文字顏色 */\n",
        "            font-family: Arial, sans-serif !important;\n",
        "            font-size: 16px !important; /* 你可以調整文字大小 */\n",
        "            line-height: 1.7 !important;\n",
        "        }\n",
        "\n",
        "        /* 結果容器樣式 */\n",
        "        .result-container {\n",
        "            width: 100% !important;\n",
        "            padding: 1rem !important;\n",
        "            border-radius: 8px !important;\n",
        "            border: 1px solid #E2E8F0 !important;\n",
        "            margin-bottom: 1.5rem !important;\n",
        "            background-color: #F8FAFC !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* 結果文本框的樣式 */\n",
        "        .wide-result-text {\n",
        "            width: 100% !important;\n",
        "            min-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 0 !important;\n",
        "            margin: 0 !important;\n",
        "        }\n",
        "\n",
        "        /* 片段標題樣式 */\n",
        "        .section-heading {\n",
        "            font-size: 1.25rem !important;\n",
        "            font-weight: 600 !important;\n",
        "            color: #2D3748 !important;\n",
        "            margin: 1rem auto !important;\n",
        "            padding: 0.75rem 1rem !important;\n",
        "            background: linear-gradient(to right, #e6f3fc, #f0f9ff) !important;\n",
        "            border-radius: 8px !important;\n",
        "            width: 98% !important;\n",
        "            display: inline-block !important;\n",
        "            box-sizing: border-box !important;\n",
        "            text-align: center !important;\n",
        "            overflow: visible !important;\n",
        "            line-height: 1.5 !important;\n",
        "            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "\n",
        "        /* JSON 顯示區域樣式 */\n",
        "        .json-box {\n",
        "            width: 100% !important;\n",
        "            min-height: 200px !important;\n",
        "            overflow-y: auto !important;\n",
        "            background: white !important;\n",
        "            padding: 1rem !important;\n",
        "            border-radius: 8px !important;\n",
        "            box-shadow: inset 0 0 6px rgba(0, 0, 0, 0.1) !important;\n",
        "            font-family: monospace !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* 欄佈局調整 */\n",
        "        .plot-column, .stats-column {\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            padding: 1rem;\n",
        "            box-sizing: border-box !important;\n",
        "            width: 100% !important;\n",
        "        }\n",
        "\n",
        "        /* statistics plot */\n",
        "        .large-plot-container {\n",
        "            width: 100% !important;\n",
        "            min-height: 400px !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* 增強 JSON 顯示 */\n",
        "        .enhanced-json-display {\n",
        "            background: white !important;\n",
        "            border-radius: 8px !important;\n",
        "            padding: 1rem !important;\n",
        "            box-shadow: inset 0 0 6px rgba(0, 0, 0, 0.1) !important;\n",
        "            width: 100% !important;\n",
        "            min-height: 300px !important;\n",
        "            max-height: 500px !important;\n",
        "            overflow-y: auto !important;\n",
        "            font-family: monospace !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* 確保全寬元素真正占滿整個寬度 */\n",
        "        .full-width-element {\n",
        "            width: 100% !important;\n",
        "            max-width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "        }\n",
        "\n",
        "        /* Video summary HTML 容器與內容樣式 */\n",
        "        #video-summary-html-output {\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            padding: 0 !important;\n",
        "            margin: 0 !important;\n",
        "        }\n",
        "\n",
        "        .video-summary-content-wrapper {\n",
        "            width: 100% !important;\n",
        "            padding: 16px !important;\n",
        "            line-height: 1.8 !important;\n",
        "            white-space: pre-wrap !important;\n",
        "            word-wrap: break-word !important;\n",
        "            border-radius: 8px !important;\n",
        "            min-height: 250px !important;\n",
        "            max-height: 600px !important;\n",
        "            overflow-y: auto !important;\n",
        "            border: 1px solid #e2e8f0 !important;\n",
        "            background-color: white !important;\n",
        "            display: block !important;\n",
        "            font-family: 'Arial', sans-serif !important;\n",
        "            font-size: 14px !important;\n",
        "            margin: 0 !important;\n",
        "        }\n",
        "\n",
        "        .video-summary-content-wrapper pre {\n",
        "            white-space: pre-wrap !important;\n",
        "            word-wrap: break-word !important;\n",
        "            margin: 0 !important;\n",
        "            padding: 0 !important;\n",
        "            font-family: 'Arial', sans-serif !important;\n",
        "            font-size: 14px !important;\n",
        "            line-height: 1.8 !important;\n",
        "            color: #2D3748 !important;\n",
        "        }\n",
        "\n",
        "        /* 視頻結果面板相關樣式 */\n",
        "        .video-result-panel {\n",
        "            padding: 1rem !important;\n",
        "            background: white !important;\n",
        "            border-radius: 10px !important;\n",
        "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08) !important;\n",
        "        }\n",
        "\n",
        "        .video-output-container {\n",
        "            width: 100% !important;\n",
        "            margin-bottom: 1.5rem !important;\n",
        "            border-radius: 8px !important;\n",
        "            overflow: hidden !important;\n",
        "            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1) !important;\n",
        "        }\n",
        "\n",
        "        /* 視頻統計資料顯示增強 */\n",
        "        .video-stats-display {\n",
        "            background: white !important;\n",
        "            border-radius: 8px !important;\n",
        "            padding: 1rem !important;\n",
        "            box-shadow: inset 0 0 6px rgba(0, 0, 0, 0.1) !important;\n",
        "            width: 100% !important;\n",
        "            min-height: 200px !important;\n",
        "            max-height: 400px !important;\n",
        "            overflow-y: auto !important;\n",
        "            font-family: monospace !important;\n",
        "            box-sizing: border-box !important;\n",
        "            color: #2D3748 !important;\n",
        "        }\n",
        "\n",
        "        .custom-video-url-input {\n",
        "            width: 100% !important;\n",
        "        }\n",
        "\n",
        "        .custom-video-url-input textarea {\n",
        "            width: 100% !important;\n",
        "            min-height: 120px !important;\n",
        "            padding: 15px !important;\n",
        "            font-size: 16px !important;\n",
        "            line-height: 1.6 !important;\n",
        "            background-color: #F7FAFC !important;\n",
        "            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1) !important;\n",
        "            border: 2px solid #CBD5E0 !important;\n",
        "            border-radius: 8px !important;\n",
        "        }\n",
        "\n",
        "        .custom-video-url-input textarea:focus {\n",
        "            border-color: #4299E1 !important;\n",
        "            box-shadow: 0 0 0 3px rgba(66, 153, 225, 0.2) !important;\n",
        "        }\n",
        "\n",
        "        /* 輸入框容器100%寬度 */\n",
        "        .custom-video-url-input > div {\n",
        "            width: 100% !important;\n",
        "            max-width: 100% !important;\n",
        "        }\n",
        "\n",
        "        /* LLM 增強描述樣式 */\n",
        "        #llm_enhanced_description_text {\n",
        "            padding: 15px !important;\n",
        "            background-color: #ffffff !important;\n",
        "            border-radius: 8px !important;\n",
        "            border: 1px solid #e2e8f0 !important;\n",
        "            margin-bottom: 20px !important;\n",
        "            box-shadow: 0 1px 3px rgba(0,0,0,0.05) !important;\n",
        "            font-family: Arial, sans-serif !important;\n",
        "            line-height: 1.7 !important;\n",
        "            color: #2D3748 !important;\n",
        "            font-size: 16px !important;\n",
        "            width: 100% !important;\n",
        "            box-sizing: border-box !important;\n",
        "            min-height: 200px !important;\n",
        "        }\n",
        "\n",
        "        /* 原始描述折疊區域樣式 */\n",
        "        #original_scene_analysis_accordion {\n",
        "            margin-top: 10px !important;\n",
        "            margin-bottom: 20px !important;\n",
        "            background-color: #f8f9fa !important;\n",
        "            border-radius: 8px !important;\n",
        "            border: 1px solid #e2e8f0 !important;\n",
        "        }\n",
        "\n",
        "        /* 確保折疊區域內容與頁面樣式協調 */\n",
        "        #original_scene_analysis_accordion > div:nth-child(2) {\n",
        "            padding: 15px !important;\n",
        "        }\n",
        "\n",
        "        /* 動畫效果, 增加互動感 */\n",
        "        @keyframes fadeIn {\n",
        "            from { opacity: 0; }\n",
        "            to { opacity: 1; }\n",
        "        }\n",
        "\n",
        "        .video-result-panel > * {\n",
        "            animation: fadeIn 0.5s ease-in-out;\n",
        "        }\n",
        "\n",
        "        /* 響應式調整 */\n",
        "        @media (max-width: 768px) {\n",
        "            .app-title {\n",
        "                font-size: 2rem;\n",
        "            }\n",
        "\n",
        "            .app-subtitle {\n",
        "                font-size: 1rem;\n",
        "            }\n",
        "\n",
        "            .gradio-container {\n",
        "                padding: 0.5rem;\n",
        "            }\n",
        "\n",
        "            /* 在小螢幕上調整文本區域的高度 */\n",
        "            #scene-description-text, #detection-details {\n",
        "                min-height: 150px !important;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        \"\"\"\n",
        "        return css"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile scene_type.py\n",
        "\n",
        "SCENE_TYPES = {\n",
        "    \"living_room\": {\n",
        "        \"name\": \"Living Room\",\n",
        "        \"required_objects\": [57, 62],  # couch, tv\n",
        "        \"optional_objects\": [56, 60, 73, 75],  # chair, dining table, book, vase\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A living room area with furniture for relaxation and entertainment\"\n",
        "    },\n",
        "    \"bedroom\": {\n",
        "        \"name\": \"Bedroom\",\n",
        "        \"required_objects\": [59],  # bed\n",
        "        \"optional_objects\": [56, 60, 73, 74, 75],  # chair, dining table, book, clock, vase\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A bedroom with sleeping furniture\"\n",
        "    },\n",
        "    \"dining_area\": {\n",
        "        \"name\": \"Dining Area\",\n",
        "        \"required_objects\": [60],  # dining table\n",
        "        \"optional_objects\": [56, 39, 41, 42, 43, 44, 45],  # chair, bottle, cup, fork, knife, spoon, bowl\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A dining area for meals\"\n",
        "    },\n",
        "    \"kitchen\": {\n",
        "        \"name\": \"Kitchen\",\n",
        "        \"required_objects\": [72, 68, 69, 71],  # refrigerator, microwave, oven, sink\n",
        "        \"optional_objects\": [39, 41, 42, 43, 44, 45],  # bottle, cup, fork, knife, spoon, bowl\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A kitchen area for food preparation\"\n",
        "    },\n",
        "    \"office_workspace\": {\n",
        "        \"name\": \"Office Workspace\",\n",
        "        \"required_objects\": [56, 63, 66, 64, 73],  # chair, laptop, keyboard, mouse, book\n",
        "        \"optional_objects\": [60, 74, 75, 67],  # dining table, clock, vase, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A workspace with computer equipment for office work\"\n",
        "    },\n",
        "    \"meeting_room\": {\n",
        "        \"name\": \"Meeting Room\",\n",
        "        \"required_objects\": [56, 60],  # chair, dining table\n",
        "        \"optional_objects\": [63, 62, 67],  # laptop, tv, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A room set up for meetings with multiple seating\"\n",
        "    },\n",
        "    \"city_street\": {\n",
        "        \"name\": \"City Street\",\n",
        "        \"required_objects\": [0, 1, 2, 3, 5, 7, 9],  # person, bicycle, car, motorcycle, bus, truck, traffic light\n",
        "        \"optional_objects\": [10, 11, 12, 24, 25, 26, 28],  # fire hydrant, stop sign, parking meter, backpack, umbrella, handbag, suitcase\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A city street with traffic and pedestrians\"\n",
        "    },\n",
        "    \"parking_lot\": {\n",
        "        \"name\": \"Parking Lot\",\n",
        "        \"required_objects\": [2, 3, 5, 7],  # car, motorcycle, bus, truck\n",
        "        \"optional_objects\": [0, 11, 12],  # person, stop sign, parking meter\n",
        "        \"minimum_required\": 3,\n",
        "        \"description\": \"A parking area with multiple vehicles\"\n",
        "    },\n",
        "    \"park_area\": {\n",
        "        \"name\": \"Park or Recreation Area\",\n",
        "        \"required_objects\": [0, 13],  # person, bench\n",
        "        \"optional_objects\": [1, 14, 16, 25, 33],  # bicycle, bird, dog, umbrella, kite\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"An outdoor recreational area for leisure activities\"\n",
        "    },\n",
        "    \"retail_store\": {\n",
        "        \"name\": \"Retail Store\",\n",
        "        \"required_objects\": [0, 24, 26, 28],  # person, backpack, handbag, suitcase\n",
        "        \"optional_objects\": [39, 45, 67],  # bottle, bowl, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A retail environment with shoppers and merchandise\"\n",
        "    },\n",
        "    \"supermarket\": {\n",
        "        \"name\": \"Supermarket\",\n",
        "        \"required_objects\": [0, 24, 39, 46, 47, 49],  # person, backpack, bottle, banana, apple, orange\n",
        "        \"optional_objects\": [26, 37, 45, 48, 51, 52, 53, 54, 55],  # handbag, surfboard, bowl, sandwich, carrot, hot dog, pizza, donut, cake\n",
        "        \"minimum_required\": 3,\n",
        "        \"description\": \"A supermarket with food items and shoppers\"\n",
        "    },\n",
        "    \"classroom\": {\n",
        "        \"name\": \"Classroom\",\n",
        "        \"required_objects\": [56, 60, 73],  # chair, dining table, book\n",
        "        \"optional_objects\": [63, 66, 67],  # laptop, keyboard, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A classroom environment set up for educational activities\"\n",
        "    },\n",
        "    \"conference_room\": {\n",
        "        \"name\": \"Conference Room\",\n",
        "        \"required_objects\": [56, 60, 63],  # chair, dining table, laptop\n",
        "        \"optional_objects\": [62, 67, 73],  # tv, cell phone, book\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A conference room designed for meetings and presentations\"\n",
        "    },\n",
        "    \"cafe\": {\n",
        "        \"name\": \"Cafe\",\n",
        "        \"required_objects\": [56, 60, 41],  # chair, dining table, cup\n",
        "        \"optional_objects\": [39, 40, 63, 67, 73],  # bottle, wine glass, laptop, cell phone, book\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A cafe setting with seating and beverages\"\n",
        "    },\n",
        "    \"library\": {\n",
        "        \"name\": \"Library\",\n",
        "        \"required_objects\": [56, 60, 73],  # chair, dining table, book\n",
        "        \"optional_objects\": [63, 67, 75],  # laptop, cell phone, vase\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A library with books and reading areas\"\n",
        "    },\n",
        "    \"gym\": {\n",
        "        \"name\": \"Gym\",\n",
        "        \"required_objects\": [0, 32],  # person, sports ball\n",
        "        \"optional_objects\": [24, 25, 28, 38],  # backpack, umbrella, suitcase, tennis racket\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A gym or fitness area for physical activities\"\n",
        "    },\n",
        "    \"beach\": {\n",
        "        \"name\": \"Beach\",\n",
        "        \"required_objects\": [0, 25, 29, 33, 37],  # person, umbrella, frisbee, kite, surfboard\n",
        "        \"optional_objects\": [1, 24, 26, 38],  # bicycle, backpack, handbag, tennis racket\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A beach area with people and recreational items\"\n",
        "    },\n",
        "    \"restaurant\": {\n",
        "        \"name\": \"Restaurant\",\n",
        "        \"required_objects\": [56, 60, 41, 42, 43, 44, 45],  # chair, dining table, cup, fork, knife, spoon, bowl\n",
        "        \"optional_objects\": [39, 40, 48, 49, 50, 51, 52, 53, 54, 55],  # bottle, wine glass, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake\n",
        "        \"minimum_required\": 3,\n",
        "        \"description\": \"A restaurant setting for dining with tables and eating utensils\"\n",
        "    },\n",
        "    \"train_station\": {\n",
        "        \"name\": \"Train Station\",\n",
        "        \"required_objects\": [0, 6],  # person, train\n",
        "        \"optional_objects\": [1, 2, 24, 28, 67],  # bicycle, car, backpack, suitcase, cell phone\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A train station with train and passengers\"\n",
        "    },\n",
        "    \"airport\": {\n",
        "        \"name\": \"Airport\",\n",
        "        \"required_objects\": [0, 4, 28],  # person, airplane, suitcase\n",
        "        \"optional_objects\": [24, 25, 26, 67],  # backpack, umbrella, handbag, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"An airport with planes and travelers carrying luggage\"\n",
        "    },\n",
        "      \"upscale_dining\": {\n",
        "        \"name\": \"Upscale Dining Area\",\n",
        "        \"required_objects\": [56, 60, 40, 41],  # chair, dining table, wine glass, cup\n",
        "        \"optional_objects\": [39, 42, 43, 44, 45, 62, 75],  # bottle, fork, knife, spoon, bowl, tv, vase\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"An elegantly designed dining space with refined furniture and decorative elements\"\n",
        "    },\n",
        "    \"asian_commercial_street\": {\n",
        "        \"name\": \"Asian Commercial Street\",\n",
        "        \"required_objects\": [0, 67],  # person, cell phone\n",
        "        \"optional_objects\": [1, 2, 3, 24, 25, 26, 28],  # bicycle, car, motorcycle, backpack, umbrella, handbag, suitcase\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A bustling commercial street with shops, signage, and pedestrians in an Asian urban setting\"\n",
        "    },\n",
        "    \"financial_district\": {\n",
        "        \"name\": \"Financial District\",\n",
        "        \"required_objects\": [2, 5, 7, 9],  # car, bus, truck, traffic light\n",
        "        \"optional_objects\": [0, 1, 3, 8],  # person, bicycle, motorcycle, boat\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A major thoroughfare in a business district with high-rise buildings and traffic\"\n",
        "    },\n",
        "    \"urban_intersection\": {\n",
        "        \"name\": \"Urban Intersection\",\n",
        "        \"required_objects\": [0, 9],  # person, traffic light\n",
        "        \"optional_objects\": [1, 2, 3, 5, 7],  # bicycle, car, motorcycle, bus, truck\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A busy urban crossroad with pedestrian crossings and multiple traffic flows\"\n",
        "    },\n",
        "    \"transit_hub\": {\n",
        "        \"name\": \"Transit Hub\",\n",
        "        \"required_objects\": [0, 5, 6, 7],  # person, bus, train, truck\n",
        "        \"optional_objects\": [1, 2, 3, 9, 24, 28],  # bicycle, car, motorcycle, traffic light, backpack, suitcase\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A transportation center where multiple modes of transit converge\"\n",
        "    },\n",
        "    \"shopping_district\": {\n",
        "        \"name\": \"Shopping District\",\n",
        "        \"required_objects\": [0, 24, 26],  # person, backpack, handbag\n",
        "        \"optional_objects\": [1, 2, 3, 25, 27, 28, 39, 67],  # bicycle, car, motorcycle, umbrella, tie, suitcase, bottle, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A retail-focused area with shops, pedestrians, and commercial activity\"\n",
        "    },\n",
        "     \"bus_stop\": {\n",
        "        \"name\": \"Bus Stop\",\n",
        "        \"required_objects\": [0, 5],  # person, bus\n",
        "        \"optional_objects\": [1, 2, 7, 24],  # bicycle, car, truck, backpack\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A roadside bus stop with waiting passengers and buses\"\n",
        "    },\n",
        "    \"bus_station\": {\n",
        "        \"name\": \"Bus Station\",\n",
        "        \"required_objects\": [0, 5, 7],  # person, bus, truck\n",
        "        \"optional_objects\": [24, 28, 67],  # backpack, suitcase, cell phone\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A bus terminal with multiple buses and travelers\"\n",
        "    },\n",
        "    \"zoo\": {\n",
        "        \"name\": \"Zoo\",\n",
        "        \"required_objects\": [20, 22, 23],  # elephant, zebra, giraffe\n",
        "        \"optional_objects\": [0, 14, 16],  # person, bird, dog\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A zoo environment featuring large animal exhibits and visitors\"\n",
        "    },\n",
        "    \"harbor\": {\n",
        "        \"name\": \"Harbor\",\n",
        "        \"required_objects\": [8],  # boat\n",
        "        \"optional_objects\": [0, 2, 3, 39],  # person, car, motorcycle, bottle\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A harbor area with boats docked and surrounding traffic\"\n",
        "    },\n",
        "    \"playground\": {\n",
        "        \"name\": \"Playground\",\n",
        "        \"required_objects\": [0, 32],  # person, sports ball\n",
        "        \"optional_objects\": [33, 24, 1],  # kite, backpack, bicycle\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"An outdoor playground with people playing sports and games\"\n",
        "    },\n",
        "    \"sports_field\": {\n",
        "        \"name\": \"Sports Field\",\n",
        "        \"required_objects\": [32],  # sports ball\n",
        "        \"optional_objects\": [38, 34, 35],  # tennis racket, baseball bat, baseball glove\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A sports field set up for various ball games\"\n",
        "    },\n",
        "     \"narrow_commercial_alley\": {\n",
        "        \"name\": \"Narrow Commercial Alley\",\n",
        "        \"required_objects\": [0, 3],  # person, motorcycle\n",
        "        \"optional_objects\": [2, 7, 24, 26],  # car, truck, backpack, handbag\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A tight urban alley lined with shops, with pedestrians and light vehicles\"\n",
        "    },\n",
        "    \"daytime_shopping_street\": {\n",
        "        \"name\": \"Daytime Shopping Street\",\n",
        "        \"required_objects\": [0, 2],  # person, car\n",
        "        \"optional_objects\": [1, 3, 24, 26],  # bicycle, motorcycle, backpack, handbag\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A busy pedestrian street during daytime, featuring shops, vehicles, and shoppers\"\n",
        "    },\n",
        "    \"urban_pedestrian_crossing\": {\n",
        "        \"name\": \"Urban Pedestrian Crossing\",\n",
        "        \"required_objects\": [0, 9],  # person, traffic light\n",
        "        \"optional_objects\": [2, 3, 5],  # car, motorcycle, bus\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A city street crossing with pedestrians and traffic signals\"\n",
        "    },\n",
        "    \"aerial_view_intersection\": {\n",
        "    \"name\": \"Aerial View Intersection\",\n",
        "    \"required_objects\": [0, 9],  # person, traffic light\n",
        "    \"optional_objects\": [1, 2, 3, 5, 7],  # bicycle, car, motorcycle, bus, truck\n",
        "    \"minimum_required\": 1,\n",
        "    \"description\": \"An intersection viewed from above, showing crossing patterns and pedestrian movement\"\n",
        "    },\n",
        "    \"aerial_view_commercial_area\": {\n",
        "        \"name\": \"Aerial View Commercial Area\",\n",
        "        \"required_objects\": [0, 2],  # person, car\n",
        "        \"optional_objects\": [1, 3, 5, 7, 24, 26],  # bicycle, motorcycle, bus, truck, backpack, handbag\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A commercial or shopping area viewed from above showing pedestrians and urban layout\"\n",
        "    },\n",
        "    \"aerial_view_plaza\": {\n",
        "        \"name\": \"Aerial View Plaza\",\n",
        "        \"required_objects\": [0],  # person\n",
        "        \"optional_objects\": [1, 2, 24, 25, 26],  # bicycle, car, backpack, umbrella, handbag\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"An urban plaza or public square viewed from above with pedestrian activity\"\n",
        "    },\n",
        "\n",
        "    # specific cultural item\n",
        "    \"asian_night_market\": {\n",
        "        \"name\": \"Asian Night Market\",\n",
        "        \"required_objects\": [0, 67],  # person, cell phone\n",
        "        \"optional_objects\": [1, 3, 24, 26, 39, 41],  # bicycle, motorcycle, backpack, handbag, bottle, cup\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A vibrant night market scene typical in Asian cities with food stalls and crowds\"\n",
        "    },\n",
        "    \"asian_temple_area\": {\n",
        "        \"name\": \"Asian Temple Area\",\n",
        "        \"required_objects\": [0],  # person\n",
        "        \"optional_objects\": [24, 25, 26, 67, 75],  # backpack, umbrella, handbag, cell phone, vase\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A traditional Asian temple complex with visitors and cultural elements\"\n",
        "    },\n",
        "\n",
        "    # specific time item\n",
        "    \"nighttime_street\": {\n",
        "        \"name\": \"Nighttime Street\",\n",
        "        \"required_objects\": [0, 9],  # person, traffic light\n",
        "        \"optional_objects\": [1, 2, 3, 5, 7, 67],  # bicycle, car, motorcycle, bus, truck, cell phone\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"An urban street at night with artificial lighting and nighttime activity\"\n",
        "    },\n",
        "    \"nighttime_commercial_district\": {\n",
        "        \"name\": \"Nighttime Commercial District\",\n",
        "        \"required_objects\": [0, 67],  # person, cell phone\n",
        "        \"optional_objects\": [1, 2, 3, 24, 26],  # bicycle, car, motorcycle, backpack, handbag\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A commercial district illuminated at night with neon signs and evening activity\"\n",
        "    },\n",
        "\n",
        "    # mixture enviroment item\n",
        "    \"indoor_outdoor_cafe\": {\n",
        "        \"name\": \"Indoor-Outdoor Cafe\",\n",
        "        \"required_objects\": [56, 60, 41],  # chair, dining table, cup\n",
        "        \"optional_objects\": [39, 40, 63, 67, 73],  # bottle, wine glass, laptop, cell phone, book\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A cafe setting with both indoor elements and outdoor patio or sidewalk seating\"\n",
        "    },\n",
        "    \"transit_station_platform\": {\n",
        "        \"name\": \"Transit Station Platform\",\n",
        "        \"required_objects\": [0],  # person\n",
        "        \"optional_objects\": [5, 6, 7, 24, 28, 67],  # bus, train, truck, backpack, suitcase, cell phone\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A transit platform with waiting passengers and arriving/departing vehicles\"\n",
        "    },\n",
        "    \"sports_stadium\": {\n",
        "        \"name\": \"Sports Stadium\",\n",
        "        \"required_objects\": [0, 32],  # person, sports ball\n",
        "        \"optional_objects\": [24, 38, 39, 41, 67],  # backpack, tennis racket, bottle, cup, cell phone\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A sports stadium or arena with spectators and athletic activities\"\n",
        "    },\n",
        "    \"construction_site\": {\n",
        "        \"name\": \"Construction Site\",\n",
        "        \"required_objects\": [0, 7],  # person, truck\n",
        "        \"optional_objects\": [2, 3, 11, 76, 77, 78],  # car, motorcycle, fire hydrant, scissors, teddy bear, hair drier\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"A construction site with workers, equipment, and building materials\"\n",
        "    },\n",
        "    \"medical_facility\": {\n",
        "        \"name\": \"Medical Facility\",\n",
        "        \"required_objects\": [0, 56, 60],  # person, chair, dining table\n",
        "        \"optional_objects\": [63, 64, 66, 67, 73],  # laptop, mouse, keyboard, cell phone, book\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"A medical facility such as hospital, clinic or doctor's office with medical staff and patients\"\n",
        "    },\n",
        "    \"educational_setting\": {\n",
        "        \"name\": \"Educational Setting\",\n",
        "        \"required_objects\": [0, 56, 60, 73],  # person, chair, dining table, book\n",
        "        \"optional_objects\": [63, 64, 66, 67, 74],  # laptop, mouse, keyboard, cell phone, clock\n",
        "        \"minimum_required\": 2,\n",
        "        \"description\": \"An educational environment such as classroom, lecture hall or study area\"\n",
        "    },\n",
        "    \"aerial_view_intersection\": {\n",
        "        \"name\": \"Aerial View Intersection\",\n",
        "        \"required_objects\": [0, 9],  # person, traffic light\n",
        "        \"optional_objects\": [1, 2, 3, 5, 7],  # bicycle, car, motorcycle, bus, truck\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"An intersection viewed from above, showing crossing patterns and pedestrian movement\",\n",
        "        \"viewpoint_indicator\": \"aerial\", # view side\n",
        "        \"key_features\": [\"crosswalk_pattern\", \"pedestrian_flow\", \"intersection_layout\"],  # key feature\n",
        "        \"detection_priority\": 10  # priority\n",
        "    },\n",
        "    \"perpendicular_crosswalk_intersection\": {\n",
        "        \"name\": \"Perpendicular Crosswalk Intersection\",\n",
        "        \"required_objects\": [0],  # person\n",
        "        \"optional_objects\": [1, 2, 3, 5, 7, 9],  # bicycle, car, motorcycle, bus, truck, traffic light\n",
        "        \"minimum_required\": 1,\n",
        "        \"description\": \"An intersection with perpendicular crosswalks where pedestrians cross in multiple directions\",\n",
        "        \"viewpoint_indicator\": \"aerial\",\n",
        "        \"key_features\": [\"perpendicular_crosswalks\", \"pedestrian_crossing\", \"multi_directional_movement\"],\n",
        "        \"pattern_detection\": True, # specific pattern\n",
        "        \"detection_priority\": 15  #\n",
        "    },\n",
        "    \"beach_water_recreation\": {\n",
        "    \"name\": \"Beach/Water Recreation Area\",\n",
        "    \"required_objects\": [0, 37],  # person, surfboard\n",
        "    \"optional_objects\": [25, 33, 1, 8, 29, 24, 26, 39, 41],  # umbrella, kite, bicycle, boat, frisbee, backpack, handbag, bottle, cup\n",
        "    \"minimum_required\": 2,\n",
        "    \"description\": \"A beach or water recreation area with water sports equipment and beach accessories\"\n",
        "    },\n",
        "    \"sports_venue\": {\n",
        "    \"name\": \"Sports Venue\",\n",
        "    \"required_objects\": [0, 32],  # person, sports ball\n",
        "    \"optional_objects\": [34, 35, 38, 25, 24, 26, 39, 41],  # baseball bat, baseball glove, tennis racket, umbrella, backpack, handbag, bottle, cup\n",
        "    \"minimum_required\": 2,\n",
        "    \"description\": \"A professional sports venue with specialized sports equipment and spectator areas\"\n",
        "    },\n",
        "    \"professional_kitchen\": {\n",
        "    \"name\": \"Professional Kitchen\",\n",
        "    \"required_objects\": [43, 44, 45],  # knife, spoon, bowl\n",
        "    \"optional_objects\": [42, 39, 41, 68, 69, 71, 72, 0],  # fork, bottle, cup, microwave, oven, sink, refrigerator, person\n",
        "    \"minimum_required\": 3,\n",
        "    \"description\": \"A commercial kitchen with professional cooking equipment and food preparation areas\"\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "DBOELeMbrVBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDHYIPUCXGMF"
      },
      "outputs": [],
      "source": [
        "# %%writefile confifence_templates.py\n",
        "\n",
        "CONFIDENCE_TEMPLATES = {\n",
        "    \"high\": \"{description} {details}\",\n",
        "    \"medium\": \"This appears to be {description} {details}\",\n",
        "    \"low\": \"This might be {description}, but the confidence is low. {details}\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUwQa_4KqHbm"
      },
      "outputs": [],
      "source": [
        "# %%writefile scene_detail_templates.py\n",
        "\n",
        "SCENE_DETAIL_TEMPLATES = {\n",
        "            \"living_room\": [\n",
        "                \"The space is arranged for relaxation with {furniture}.\",\n",
        "                \"There is {electronics} for entertainment.\",\n",
        "                \"The room has a seating area with {seating}.\"\n",
        "            ],\n",
        "            \"bedroom\": [\n",
        "                \"The room contains {bed_type} in the {bed_location}.\",\n",
        "                \"This sleeping area has {bed_description}.\",\n",
        "                \"A personal space with {bed_type} and {extras}.\"\n",
        "            ],\n",
        "            \"dining_area\": [\n",
        "                \"A space set up for meals with {table_setup}.\",\n",
        "                \"The dining area contains {table_description}.\",\n",
        "                \"A place for eating with {dining_items}.\"\n",
        "            ],\n",
        "            \"kitchen\": [\n",
        "                \"A food preparation area with {appliances}.\",\n",
        "                \"The kitchen contains {kitchen_items}.\",\n",
        "                \"A cooking space equipped with {cooking_equipment}.\"\n",
        "            ],\n",
        "            \"office_workspace\": [\n",
        "                \"A work environment with {office_equipment}.\",\n",
        "                \"A space designed for productivity with {desk_setup}.\",\n",
        "                \"A workspace containing {computer_equipment}.\"\n",
        "            ],\n",
        "            \"city_street\": [\n",
        "                \"An urban thoroughfare with {traffic_description}.\",\n",
        "                \"A street scene with {people_and_vehicles}.\",\n",
        "                \"A city path with {street_elements}.\"\n",
        "            ],\n",
        "            \"park_area\": [\n",
        "                \"An outdoor recreational space with {park_features}.\",\n",
        "                \"A leisure area featuring {outdoor_elements}.\",\n",
        "                \"A public outdoor space with {park_description}.\"\n",
        "            ],\n",
        "            \"retail_store\": [\n",
        "                \"A shopping environment with {store_elements}.\",\n",
        "                \"A commercial space where {shopping_activity}.\",\n",
        "                \"A retail area containing {store_items}.\"\n",
        "            ],\n",
        "            \"upscale_dining\": [\n",
        "            \"The space features {furniture} with {design_elements} for an elegant dining experience.\",\n",
        "            \"This sophisticated dining area includes {lighting} illuminating {table_setup}.\",\n",
        "            \"A stylish dining environment with {seating} arranged around {table_description}.\"\n",
        "            ],\n",
        "            \"asian_commercial_street\": [\n",
        "                \"A vibrant street lined with {storefront_features} and filled with {pedestrian_flow}.\",\n",
        "                \"This urban commercial area displays {asian_elements} with {cultural_elements}.\",\n",
        "                \"A lively shopping street characterized by {signage} and busy with {street_activities}.\"\n",
        "            ],\n",
        "            \"financial_district\": [\n",
        "                \"A canyon of {buildings} with {traffic_elements} moving through the urban landscape.\",\n",
        "                \"This business district features {skyscrapers} along {road_features}.\",\n",
        "                \"A downtown corridor with {architectural_elements} framing views of {city_landmarks}.\"\n",
        "            ],\n",
        "            \"urban_intersection\": [\n",
        "                \"A busy crossroad with {crossing_pattern} where {pedestrian_behavior} is observed.\",\n",
        "                \"This urban junction features {pedestrian_density} navigating the {traffic_pattern}.\",\n",
        "                \"A well-marked intersection designed for {pedestrian_flow} across multiple directions.\"\n",
        "            ],\n",
        "            \"transit_hub\": [\n",
        "                \"A transportation nexus where {transit_vehicles} arrive and depart amid {passenger_activity}.\",\n",
        "                \"This transit center accommodates {transportation_modes} with facilities for {passenger_needs}.\",\n",
        "                \"A busy transport hub featuring {transit_infrastructure} and areas for {passenger_movement}.\"\n",
        "            ],\n",
        "            \"shopping_district\": [\n",
        "                \"A commercial zone filled with {retail_elements} and {shopping_activity}.\",\n",
        "                \"This shopping area features {store_types} along {walkway_features}.\",\n",
        "                \"A retail district characterized by {commercial_signage} and {consumer_behavior}.\"\n",
        "            ],\n",
        "            \"bus_stop\": [\n",
        "                \"Passengers waiting at a roadside stop served by {transit_vehicles}.\",\n",
        "                \"A designated bus stop with shelters and {passenger_activity}.\",\n",
        "                \"Commuters boarding or alighting from {transit_vehicles} at the curb.\"\n",
        "            ],\n",
        "            \"bus_station\": [\n",
        "                \"Multiple buses parked in a terminal where {passenger_activity}.\",\n",
        "                \"A busy station hub featuring {transit_vehicles} and traveler luggage.\",\n",
        "                \"A transit center with waiting areas and various {transportation_modes}.\"\n",
        "            ],\n",
        "            \"zoo\": [\n",
        "                \"Enclosures showcasing elephants, zebras, and giraffes with visitors observing.\",\n",
        "                \"A wildlife exhibit area where families watch animal displays.\",\n",
        "                \"A recreational space featuring large animal exhibits and strolling guests.\"\n",
        "            ],\n",
        "            \"harbor\": [\n",
        "                \"Boats docked along the waterfront with nearby vehicular traffic.\",\n",
        "                \"A maritime area where vessels anchor beside roads busy with cars and motorcycles.\",\n",
        "                \"A coastal dock featuring moored boats and passing traffic elements.\"\n",
        "            ],\n",
        "            \"playground\": [\n",
        "                \"An open play area equipped with balls and recreational structures.\",\n",
        "                \"People engaging in games and sports in a communal space.\",\n",
        "                \"A leisure area featuring playground equipment and active participants.\"\n",
        "            ],\n",
        "            \"sports_field\": [\n",
        "                \"An athletic field marked for various ball games and matches.\",\n",
        "                \"Players using equipment like bats, gloves, and rackets on a grassy pitch.\",\n",
        "                \"A designated sports area with goalposts or markings for competitive play.\"\n",
        "            ],\n",
        "            \"narrow_commercial_alley\": [\n",
        "                \"A tight alley lined with {storefront_features} and light vehicles.\",\n",
        "                \"Pedestrians navigate a confined lane flanked by shops and {street_activities}.\",\n",
        "                \"An urban passage featuring {storefront_features} with {people_and_vehicles}.\"\n",
        "            ],\n",
        "            \"daytime_shopping_street\": [\n",
        "                \"A bustling street during daytime with {storefront_features} and {pedestrian_flow}.\",\n",
        "                \"Shoppers and vehicles move along a retail strip marked by {signage}.\",\n",
        "                \"An open commercial avenue filled with {people_and_vehicles} amid shops.\"\n",
        "            ],\n",
        "            \"urban_pedestrian_crossing\": [\n",
        "                \"A marked crosswalk with {crossing_pattern} under {lighting_modifier} sky.\",\n",
        "                \"Pedestrians use designated crossing with {traffic_pattern} at the intersection.\",\n",
        "                \"People waiting at a signal-controlled crossing next to {street_elements}.\"\n",
        "            ],\n",
        "            \"aerial_view_intersection\": [\n",
        "                \"The crossing pattern shows {crossing_pattern} with {pedestrian_flow} across multiple directions.\",\n",
        "                \"From above, this intersection reveals {traffic_pattern} with {pedestrian_density} navigating through defined paths.\",\n",
        "                \"This bird's-eye view shows {street_elements} converging at a junction where {pedestrian_behavior} is visible.\"\n",
        "            ],\n",
        "            \"aerial_view_commercial_area\": [\n",
        "                \"From above, this commercial zone shows {storefront_features} with {pedestrian_flow} moving between establishments.\",\n",
        "                \"This overhead view reveals {shopping_activity} amid {walkway_features} connecting different businesses.\",\n",
        "                \"The aerial perspective captures {retail_elements} organized along {commercial_layout} with visible customer activity.\"\n",
        "            ],\n",
        "            \"aerial_view_plaza\": [\n",
        "                \"This overhead view of the plaza shows {pedestrian_pattern} across an open public space.\",\n",
        "                \"From above, the plaza reveals {gathering_features} where people congregate in {movement_pattern}.\",\n",
        "                \"The aerial perspective captures {urban_elements} arranged around a central area where {public_activity} occurs.\"\n",
        "            ],\n",
        "            \"asian_night_market\": [\n",
        "                \"This bustling night market features {stall_elements} illuminated by {lighting_features} with crowds enjoying {food_elements}.\",\n",
        "                \"Rows of {vendor_stalls} line this vibrant market where {nighttime_activity} continues under {cultural_lighting}.\",\n",
        "                \"The market atmosphere is created by {asian_elements} and {night_market_sounds} amid {evening_crowd_behavior}.\"\n",
        "            ],\n",
        "            \"asian_temple_area\": [\n",
        "                \"This sacred space features {architectural_elements} displaying {cultural_symbols} with visitors engaging in {ritual_activities}.\",\n",
        "                \"The temple area contains {religious_structures} adorned with {decorative_features} where people practice {cultural_practices}.\",\n",
        "                \"Traditional {temple_architecture} creates a spiritual atmosphere enhanced by {sensory_elements} and {visitor_activities}.\"\n",
        "            ],\n",
        "            \"european_plaza\": [\n",
        "                \"This historic plaza is framed by {architectural_style} surrounding an open space where {public_activities} take place.\",\n",
        "                \"The European square features {historic_elements} and {urban_design} creating a space for {social_behaviors}.\",\n",
        "                \"Classical {european_features} define this public space where {tourist_activities} blend with {local_customs}.\"\n",
        "            ],\n",
        "            \"nighttime_street\": [\n",
        "                \"The night transforms this street with {lighting_effects} casting {shadow_patterns} across {urban_features}.\",\n",
        "                \"After dark, this urban corridor is defined by {illuminated_elements} with {evening_activities} visible in the artificial light.\",\n",
        "                \"The nocturnal street scene captures {light_sources} creating contrast between {lit_areas} and {shadowed_zones}.\"\n",
        "            ],\n",
        "            \"nighttime_commercial_district\": [\n",
        "                \"After sunset, this commercial area comes alive with {illuminated_signage} and {evening_activities} under {colorful_lighting}.\",\n",
        "                \"The district's nighttime character is defined by {neon_elements} highlighting {storefront_features} amid {night_crowd_behavior}.\",\n",
        "                \"Evening transforms this zone through {light_displays} that accentuate {building_features} and frame {nightlife_activities}.\"\n",
        "            ],\n",
        "            \"indoor_outdoor_cafe\": [\n",
        "                \"This cafe blends indoor comfort with outdoor atmosphere through {transitional_elements} connecting {indoor_features} with {outdoor_setting}.\",\n",
        "                \"Customers enjoy both {interior_amenities} and {exterior_features} in this space that bridges indoor comfort and outdoor ambiance.\",\n",
        "                \"The cafe design creates flow between {inside_elements} and {outside_spaces} allowing patrons to experience {dual_environment_benefits}.\"\n",
        "            ],\n",
        "            \"transit_station_platform\": [\n",
        "                \"This transit platform combines covered areas with open sections where {passenger_activities} occur while awaiting {transportation_types}.\",\n",
        "                \"The station design balances {sheltered_elements} with {exposed_areas} for passengers engaged in {waiting_behaviors}.\",\n",
        "                \"Commuters navigate between {indoor_facilities} and {platform_features} while {transit_routines} unfold around arriving vehicles.\"\n",
        "            ],\n",
        "            \"sports_stadium\": [\n",
        "                \"This athletic venue features {seating_arrangement} surrounding {playing_surface} where {sporting_activities} take place.\",\n",
        "                \"The stadium design incorporates {spectator_facilities} overlooking {competition_space} designed for {sports_events}.\",\n",
        "                \"Fans occupy {viewing_areas} arranged to maximize visibility of {field_elements} where athletes engage in {game_activities}.\"\n",
        "            ],\n",
        "            \"construction_site\": [\n",
        "                \"This development area shows {construction_equipment} amid {building_materials} where workers conduct {construction_activities}.\",\n",
        "                \"The construction process is visible through {work_elements} positioned around {structural_components} in various stages of completion.\",\n",
        "                \"Workers utilize {site_equipment} to transform {raw_materials} following {construction_process} stages.\"\n",
        "            ],\n",
        "            \"medical_facility\": [\n",
        "                \"This healthcare environment features {medical_elements} arranged to support {clinical_activities} in a {facility_design}.\",\n",
        "                \"The medical space incorporates {healthcare_features} where {patient_interactions} occur in a controlled environment.\",\n",
        "                \"Professional medical staff utilize {equipment_types} while conducting {care_procedures} in specialized {treatment_spaces}.\"\n",
        "            ],\n",
        "            \"educational_setting\": [\n",
        "                \"This learning environment contains {educational_furniture} arranged to facilitate {learning_activities} through {instructional_design}.\",\n",
        "                \"The educational space features {classroom_elements} organized for {teaching_methods} and {student_engagement}.\",\n",
        "                \"Students and educators interact within {learning_spaces} equipped with {educational_tools} supporting {knowledge_transfer}.\"\n",
        "            ],\n",
        "            \"beach_water_recreation\": [\n",
        "                \"A coastal recreation area with {beach_equipment} and people enjoying {water_activities}.\",\n",
        "                \"This shoreline space features {beach_equipment} where visitors engage in {water_activities}.\",\n",
        "                \"An outdoor water recreation zone with {beach_equipment} set up for {water_activities}.\"\n",
        "            ],\n",
        "            \"sports_venue\": [\n",
        "                \"A professional sports facility with {sports_equipment} arranged for {competitive_activities}.\",\n",
        "                \"This athletics venue features {sports_equipment} with spaces designated for {competitive_activities}.\",\n",
        "                \"A specialized sports arena containing {sports_equipment} designed for {competitive_activities}.\"\n",
        "            ],\n",
        "            \"professional_kitchen\": [\n",
        "                \"A commercial cooking space with {kitchen_equipment} organized for {food_preparation}.\",\n",
        "                \"This professional culinary area contains {kitchen_equipment} arranged in stations for {food_preparation}.\",\n",
        "                \"An industrial kitchen featuring {kitchen_equipment} designed for efficient {food_preparation}.\"\n",
        "            ],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jraYFP_PXgVU"
      },
      "outputs": [],
      "source": [
        "# %%writefile object_template_fillers.py\n",
        "\n",
        "OBJECT_TEMPLATE_FILLERS = {\n",
        "                \"furniture\": [\"designer chairs\", \"wooden dining table\", \"stylish seating\", \"upholstered armchairs\", \"elegant dining furniture\"],\n",
        "                \"design_elements\": [\"art pieces\", \"decorative wreaths\", \"statement lighting\", \"seasonal decorations\", \"sophisticated decor\"],\n",
        "                \"lighting\": [\"pendant lights\", \"decorative fixtures\", \"geometric lighting\", \"modern chandeliers\", \"ambient illumination\"],\n",
        "                \"table_setup\": [\"elegantly set table\", \"tabletop decorations\", \"seasonal centerpieces\", \"formal place settings\", \"floral arrangements\"],\n",
        "                \"seating\": [\"upholstered chairs\", \"accent armchairs\", \"mixed seating styles\", \"designer dining chairs\", \"comfortable dining seats\"],\n",
        "                \"table_description\": [\"solid wood table\", \"designer dining table\", \"expansive dining surface\", \"artisanal table\", \"statement dining table\"],\n",
        "\n",
        "                \"storefront_features\": [\"multi-story shops\", \"illuminated signs\", \"merchandise displays\", \"compact storefronts\", \"vertical retail spaces\"],\n",
        "                \"pedestrian_flow\": [\"people walking\", \"shoppers\", \"pedestrians\", \"locals and tourists\", \"urban foot traffic\"],\n",
        "                \"asian_elements\": [\"Asian language signage\", \"decorative lanterns\", \"local storefronts\", \"character-based text\", \"regional design elements\"],\n",
        "                \"cultural_elements\": [\"red lanterns\", \"local typography\", \"distinctive architecture\", \"cultural symbols\", \"traditional decorations\"],\n",
        "                \"signage\": [\"bright store signs\", \"multilingual text\", \"vertical signboards\", \"neon displays\", \"electronic advertisements\"],\n",
        "                \"street_activities\": [\"shopping\", \"commuting\", \"socializing\", \"vendor transactions\", \"urban navigation\"],\n",
        "\n",
        "                \"buildings\": [\"high-rise office buildings\", \"corporate towers\", \"skyscrapers\", \"financial institutions\", \"commercial headquarters\"],\n",
        "                \"traffic_elements\": [\"vehicle lights\", \"trams/street cars\", \"lane markers\", \"traffic signals\", \"urban transit\"],\n",
        "                \"skyscrapers\": [\"glass and steel buildings\", \"tall structures\", \"modern architecture\", \"office towers\", \"urban high-rises\"],\n",
        "                \"road_features\": [\"wide avenues\", \"tram tracks\", \"traffic lanes\", \"median dividers\", \"urban throughways\"],\n",
        "                \"architectural_elements\": [\"contemporary buildings\", \"urban design\", \"varied architectural styles\", \"corporate architecture\", \"city planning features\"],\n",
        "                \"city_landmarks\": [\"distant bridge\", \"skyline features\", \"iconic structures\", \"urban monuments\", \"signature buildings\"],\n",
        "\n",
        "                \"crossing_pattern\": [\"zebra crosswalks\", \"pedestrian walkways\", \"crosswalk markings\", \"intersection design\", \"safety stripes\"],\n",
        "                \"pedestrian_density\": [\"groups of people\", \"commuters\", \"diverse pedestrians\", \"urban crowds\", \"varying foot traffic\"],\n",
        "                \"pedestrian_behavior\": [\"walking in different directions\", \"crossing together\", \"waiting for signals\", \"navigating intersections\", \"following traffic rules\"],\n",
        "                \"traffic_pattern\": [\"four-way intersection\", \"crossroad\", \"junction\", \"multi-directional traffic\", \"regulated crossing\"],\n",
        "                \"pedestrian_flow\": [\"people crossing\", \"directional movement\", \"coordinated crossing\", \"timed pedestrian traffic\", \"intersection navigation\"],\n",
        "\n",
        "                \"transit_vehicles\": [\"buses\", \"trams\", \"trains\", \"taxis\", \"shuttles\"],\n",
        "                \"passenger_activity\": [\"boarding\", \"waiting\", \"exiting vehicles\", \"checking schedules\", \"navigating stations\"],\n",
        "                \"transportation_modes\": [\"public transit\", \"private vehicles\", \"ride services\", \"light rail\", \"bus systems\"],\n",
        "                \"passenger_needs\": [\"waiting areas\", \"information displays\", \"ticketing services\", \"transit connections\", \"seating\"],\n",
        "                \"transit_infrastructure\": [\"stations\", \"platforms\", \"boarding areas\", \"transit lanes\", \"signaling systems\"],\n",
        "                \"passenger_movement\": [\"transfers\", \"entrances and exits\", \"queueing\", \"platform access\", \"terminal navigation\"],\n",
        "\n",
        "                \"retail_elements\": [\"storefronts\", \"display windows\", \"shopping bags\", \"merchandise\", \"retail signage\"],\n",
        "                \"shopping_activity\": [\"browsing\", \"carrying purchases\", \"window shopping\", \"social shopping\", \"consumer activities\"],\n",
        "                \"store_types\": [\"boutiques\", \"brand stores\", \"local shops\", \"chain retailers\", \"specialty stores\"],\n",
        "                \"walkway_features\": [\"pedestrian paths\", \"shopping promenades\", \"retail corridors\", \"commercial walkways\", \"shopping streets\"],\n",
        "                \"commercial_signage\": [\"brand logos\", \"sale announcements\", \"store names\", \"advertising displays\", \"digital signage\"],\n",
        "                \"consumer_behavior\": [\"shopping in groups\", \"individual browsing\", \"carrying bags\", \"examining products\", \"moving between stores\"],\n",
        "\n",
        "                \"beach_equipment\": [\"beach umbrellas\", \"surfboards\", \"beach towels\", \"sun protection\", \"recreational equipment\"],\n",
        "                \"water_activities\": [\"water sports\", \"surfing\", \"beach recreation\", \"sun bathing\", \"coastal leisure\"],\n",
        "                \"sports_equipment\": [\"game balls\", \"professional equipment\", \"athletic gear\", \"sports apparatus\", \"competition items\"],\n",
        "                \"competitive_activities\": [\"team sports\", \"athletic contests\", \"competitive games\", \"sporting events\", \"professional matches\"],\n",
        "                \"kitchen_equipment\": [\"professional appliances\", \"cooking stations\", \"preparation surfaces\", \"culinary tools\", \"industrial equipment\"],\n",
        "                \"food_preparation\": [\"meal production\", \"culinary operations\", \"food service preparation\", \"commercial cooking\", \"kitchen workflow\"],\n",
        "\n",
        "                \"crossing_pattern\": [\"grid-like pedestrian crossings\", \"multi-directional crosswalks\", \"cross-shaped intersection design\", \"perpendicular crossing lanes\", \"zebra-striped crosswalks viewed from above\"],\n",
        "                \"pedestrian_pattern\": [\"scattered distribution of people\", \"organized flow of pedestrians\", \"clustered gatherings\", \"radial movement patterns\", \"linear procession of individuals\"],\n",
        "                \"commercial_layout\": [\"parallel shopping streets\", \"interconnected shopping blocks\", \"radial marketplace design\", \"grid-like retail arrangement\", \"meandering commercial pathways\"],\n",
        "                \"movement_pattern\": [\"circular crowd motion\", \"directional pedestrian flow\", \"scattered individual movement\", \"converging foot traffic\", \"diverging pedestrian patterns\"],\n",
        "\n",
        "                \"stall_elements\": [\"food vendors with steaming woks\", \"trinket sellers with colorful displays\", \"lantern-lit stalls\", \"bamboo-framed shops\", \"canvas-covered market stands\"],\n",
        "                \"asian_elements\": [\"hanging red lanterns\", \"character-based signage\", \"ornate temple decorations\", \"traditional paper decorations\", \"stylized gateway arches\"],\n",
        "                \"cultural_lighting\": [\"paper lantern illumination\", \"neon character signs\", \"strung festival lights\", \"hanging light chains\", \"colorful shop front lighting\"],\n",
        "                \"architectural_elements\": [\"tiered pagoda roofs\", \"ornate dragon sculptures\", \"stone guardian statues\", \"intricately carved railings\", \"traditional wooden beams\"],\n",
        "                \"cultural_symbols\": [\"dharma wheels\", \"lotus motifs\", \"yin-yang symbols\", \"zodiac animal representations\", \"traditional calligraphy\"],\n",
        "                \"architectural_style\": [\"Baroque facades\", \"Gothic spires\", \"Renaissance colonnades\", \"Neoclassical pediments\", \"Medieval archways\"],\n",
        "                \"european_features\": [\"cobblestone paving\", \"ornate fountains\", \"bronze statuary\", \"wrought iron lampposts\", \"cafe terraces\"],\n",
        "\n",
        "                \"lighting_effects\": [\"streetlamp pools of light\", \"neon sign glow\", \"illuminated window squares\", \"headlight streams\", \"traffic signal flashes\"],\n",
        "                \"illuminated_elements\": [\"lit storefront windows\", \"glowing traffic signals\", \"illuminated advertising\", \"headlight-lit streets\", \"backlit silhouettes\"],\n",
        "                \"neon_elements\": [\"colorful shop signs\", \"animated light displays\", \"illuminated brand logos\", \"glowing storefront outlines\", \"digital advertising screens\"],\n",
        "                \"illuminated_signage\": [\"bright LED displays\", \"glowing brand names\", \"projected light advertisements\", \"illuminated menu boards\", \"digital information screens\"],\n",
        "                \"colorful_lighting\": [\"multi-colored neon\", \"warm ambient illumination\", \"cool blue accent lights\", \"festive string lighting\", \"dynamic color-changing displays\"],\n",
        "\n",
        "                \"transitional_elements\": [\"retractable glass walls\", \"indoor-outdoor bar counters\", \"terraced seating areas\", \"threshold planters\", \"partial canopy coverage\"],\n",
        "                \"indoor_features\": [\"climate-controlled spaces\", \"soft seating arrangements\", \"interior decor accents\", \"mood lighting fixtures\", \"sound-dampened areas\"],\n",
        "                \"outdoor_setting\": [\"sidewalk tables\", \"patio seating\", \"garden furniture\", \"open-air counters\", \"courtyard arrangements\"],\n",
        "                \"seating_arrangement\": [\"tiered spectator stands\", \"premium viewing boxes\", \"courtside seating\", \"general admission benches\", \"stadium chair rows\"],\n",
        "                \"playing_surface\": [\"marked court boundaries\", \"manicured field turf\", \"running tracks\", \"competition equipment\", \"sports field markers\"],\n",
        "                \"construction_equipment\": [\"tower cranes\", \"excavators\", \"cement mixers\", \"scaffolding structures\", \"construction barriers\"],\n",
        "                \"medical_elements\": [\"examination furniture\", \"monitoring equipment\", \"sanitation stations\", \"privacy screens\", \"medical supply carts\"],\n",
        "                \"educational_furniture\": [\"student desks\", \"lecture podiums\", \"laboratory benches\", \"learning stations\", \"collaborative workspace tables\"]\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY-fgRtuXgTP"
      },
      "outputs": [],
      "source": [
        "# %%writefile safety_templates.py\n",
        "SAFETY_TEMPLATES = {\n",
        "    \"general\": \"Pay attention to {safety_element}.\",\n",
        "    \"warning\": \"Be cautious of {hazard} in this environment.\",\n",
        "    \"notice\": \"Note the presence of {element_of_interest}.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqOJTYVxXgQo"
      },
      "outputs": [],
      "source": [
        "# %%writefile activity_templates.py\n",
        "\n",
        "ACTIVITY_TEMPLATES = {\n",
        "            \"living_room\": [\n",
        "                \"Watching TV\",\n",
        "                \"Relaxing on the sofa\",\n",
        "                \"Reading\",\n",
        "                \"Socializing\"\n",
        "            ],\n",
        "            \"bedroom\": [\n",
        "                \"Sleeping\",\n",
        "                \"Resting\",\n",
        "                \"Getting dressed\",\n",
        "                \"Reading in bed\"\n",
        "            ],\n",
        "            \"dining_area\": [\n",
        "                \"Eating a meal\",\n",
        "                \"Having a conversation\",\n",
        "                \"Working at table\"\n",
        "            ],\n",
        "            \"kitchen\": [\n",
        "                \"Cooking\",\n",
        "                \"Food preparation\",\n",
        "                \"Cleaning dishes\"\n",
        "            ],\n",
        "            \"office_workspace\": [\n",
        "                \"Working on computer\",\n",
        "                \"Office work\",\n",
        "                \"Virtual meetings\",\n",
        "                \"Reading documents\"\n",
        "            ],\n",
        "            \"meeting_room\": [\n",
        "                \"Group meeting\",\n",
        "                \"Presentation\",\n",
        "                \"Team discussion\",\n",
        "                \"Collaboration\"\n",
        "            ],\n",
        "            \"city_street\": [\n",
        "                \"Walking\",\n",
        "                \"Commuting\",\n",
        "                \"Shopping\",\n",
        "                \"Waiting for transportation\"\n",
        "            ],\n",
        "            \"parking_lot\": [\n",
        "                \"Parking vehicles\",\n",
        "                \"Loading/unloading items\",\n",
        "                \"Entering/exiting vehicles\"\n",
        "            ],\n",
        "            \"park_area\": [\n",
        "                \"Walking\",\n",
        "                \"Relaxing outdoors\",\n",
        "                \"Exercising\",\n",
        "                \"Social gathering\"\n",
        "            ],\n",
        "            \"retail_store\": [\n",
        "                \"Shopping\",\n",
        "                \"Browsing products\",\n",
        "                \"Purchasing items\"\n",
        "            ],\n",
        "            \"supermarket\": [\n",
        "                \"Grocery shopping\",\n",
        "                \"Selecting products\",\n",
        "                \"Checking out\"\n",
        "            ],\n",
        "            \"upscale_dining\": [\n",
        "                \"Fine dining\",\n",
        "                \"Social gathering\",\n",
        "                \"Special occasion meal\",\n",
        "                \"Family dinner\",\n",
        "                \"Business meeting\",\n",
        "                \"Celebratory meal\"\n",
        "            ],\n",
        "            \"asian_commercial_street\": [\n",
        "                \"Shopping\",\n",
        "                \"Sightseeing\",\n",
        "                \"Walking to destinations\",\n",
        "                \"Visiting local shops\",\n",
        "                \"Cultural exploration\",\n",
        "                \"Urban commuting\",\n",
        "                \"Meeting friends\"\n",
        "            ],\n",
        "            \"financial_district\": [\n",
        "                \"Commuting\",\n",
        "                \"Business travel\",\n",
        "                \"Urban transit\",\n",
        "                \"Sightseeing\",\n",
        "                \"City navigation\",\n",
        "                \"Professional activities\",\n",
        "                \"Corporate meetings\"\n",
        "            ],\n",
        "            \"urban_intersection\": [\n",
        "                \"Street crossing\",\n",
        "                \"Waiting for signals\",\n",
        "                \"Urban navigation\",\n",
        "                \"Commuting\",\n",
        "                \"Group movement\",\n",
        "                \"Following traffic patterns\",\n",
        "                \"Pedestrian coordination\"\n",
        "            ],\n",
        "            \"transit_hub\": [\n",
        "                \"Commuting\",\n",
        "                \"Waiting for transportation\",\n",
        "                \"Transferring between vehicles\",\n",
        "                \"Starting/ending journeys\",\n",
        "                \"Meeting travelers\",\n",
        "                \"Checking transit schedules\",\n",
        "                \"Urban transportation\"\n",
        "            ],\n",
        "            \"shopping_district\": [\n",
        "                \"Retail shopping\",\n",
        "                \"Window browsing\",\n",
        "                \"Social shopping\",\n",
        "                \"Product comparison\",\n",
        "                \"Making purchases\",\n",
        "                \"Brand exploration\",\n",
        "                \"Recreational shopping\"\n",
        "            ],\n",
        "            \"bus_stop\": [\n",
        "                \"Waiting for the bus\",\n",
        "                \"Checking schedules\",\n",
        "                \"Boarding or alighting\",\n",
        "                \"Standing under shelter\"\n",
        "            ],\n",
        "            \"bus_station\": [\n",
        "                \"Navigating between platforms\",\n",
        "                \"Handling luggage\",\n",
        "                \"Boarding buses\",\n",
        "                \"Gathering at waiting areas\"\n",
        "            ],\n",
        "            \"zoo\": [\n",
        "                \"Watching animal exhibits\",\n",
        "                \"Taking photos of wildlife\",\n",
        "                \"Walking along enclosures\",\n",
        "                \"Reading informational signs\"\n",
        "            ],\n",
        "            \"harbor\": [\n",
        "                \"Observing docked boats\",\n",
        "                \"Commuting by watercraft\",\n",
        "                \"Loading or unloading cargo\",\n",
        "                \"Strolling along the pier\"\n",
        "            ],\n",
        "            \"playground\": [\n",
        "                \"Playing ball games\",\n",
        "                \"Swinging or sliding\",\n",
        "                \"Running around\",\n",
        "                \"Socializing with friends\"\n",
        "            ],\n",
        "            \"sports_field\": [\n",
        "                \"Practicing ball drills\",\n",
        "                \"Competing in matches\",\n",
        "                \"Warming up or stretching\",\n",
        "                \"Team training sessions\"\n",
        "            ],\n",
        "            \"narrow_commercial_alley\": [\n",
        "                \"Walking through alley\",\n",
        "                \"Browsing storefronts\",\n",
        "                \"Navigating light traffic\",\n",
        "                \"Carrying shopping bags\"\n",
        "            ],\n",
        "            \"daytime_shopping_street\": [\n",
        "                \"Shopping\",\n",
        "                \"Window browsing\",\n",
        "                \"Street photography\",\n",
        "                \"Commuting by vehicle\"\n",
        "            ],\n",
        "            \"urban_pedestrian_crossing\": [\n",
        "                \"Crossing the street\",\n",
        "                \"Waiting for signal\",\n",
        "                \"Following traffic rules\",\n",
        "                \"Checking for vehicles\"\n",
        "            ],\n",
        "            \"aerial_view_intersection\": [\n",
        "                \"Crossing multiple directions\",\n",
        "                \"Following traffic signals\",\n",
        "                \"Navigating pedestrian paths\",\n",
        "                \"Traffic management\",\n",
        "                \"Multi-directional movement\",\n",
        "                \"Organized crossing patterns\",\n",
        "                \"Waiting at signals\"\n",
        "            ],\n",
        "            \"aerial_view_commercial_area\": [\n",
        "                \"Shopping district navigation\",\n",
        "                \"Retail browsing\",\n",
        "                \"Store-to-store movement\",\n",
        "                \"Commercial zone foot traffic\",\n",
        "                \"Shopping center traversal\",\n",
        "                \"Retail area engagement\",\n",
        "                \"Walking between stores\"\n",
        "            ],\n",
        "            \"aerial_view_plaza\": [\n",
        "                \"Public gathering\",\n",
        "                \"Open space traversal\",\n",
        "                \"Community congregation\",\n",
        "                \"Plaza navigation\",\n",
        "                \"Public square activities\",\n",
        "                \"Urban space utilization\"\n",
        "            ],\n",
        "            \"asian_night_market\": [\n",
        "                \"Street food sampling\",\n",
        "                \"Night market browsing\",\n",
        "                \"Evening shopping\",\n",
        "                \"Cultural food exploration\",\n",
        "                \"Vendor interaction\",\n",
        "                \"Social night dining\",\n",
        "                \"Market stall hopping\"\n",
        "            ],\n",
        "            \"asian_temple_area\": [\n",
        "                \"Temple visiting\",\n",
        "                \"Cultural site exploration\",\n",
        "                \"Spiritual observance\",\n",
        "                \"Traditional rituals\",\n",
        "                \"Historical site appreciation\",\n",
        "                \"Religious tourism\",\n",
        "                \"Cultural photography\"\n",
        "            ],\n",
        "            \"european_plaza\": [\n",
        "                \"Urban sightseeing\",\n",
        "                \"Historical appreciation\",\n",
        "                \"Tourist photography\",\n",
        "                \"Public space relaxation\",\n",
        "                \"Casual strolling\"\n",
        "            ],\n",
        "            \"nighttime_street\": [\n",
        "                \"Evening commuting\",\n",
        "                \"Night walking\",\n",
        "                \"After-hours travel\",\n",
        "                \"Nighttime navigation\",\n",
        "                \"Evening errands\",\n",
        "                \"Late-night transportation\",\n",
        "                \"Nocturnal urban movement\"\n",
        "            ],\n",
        "            \"nighttime_commercial_district\": [\n",
        "                \"Evening shopping\",\n",
        "                \"Nightlife participation\",\n",
        "                \"Nighttime entertainment\",\n",
        "                \"After-dark dining\",\n",
        "                \"Evening social gathering\",\n",
        "                \"Night market browsing\",\n",
        "                \"Illumination appreciation\"\n",
        "            ],\n",
        "            \"indoor_outdoor_cafe\": [\n",
        "                \"Al fresco dining\",\n",
        "                \"Sidewalk coffee enjoyment\",\n",
        "                \"Indoor-outdoor socializing\",\n",
        "                \"Patio relaxation\",\n",
        "                \"Open-air refreshment\",\n",
        "                \"Transitional space usage\",\n",
        "                \"Weather-dependent positioning\"\n",
        "            ],\n",
        "            \"transit_station_platform\": [\n",
        "                \"Transit waiting\",\n",
        "                \"Platform navigation\",\n",
        "                \"Boarding preparation\",\n",
        "                \"Arrival monitoring\",\n",
        "                \"Schedule checking\",\n",
        "                \"Departure positioning\",\n",
        "                \"Platform traversal\"\n",
        "            ],\n",
        "            \"sports_stadium\": [\n",
        "                \"Spectator viewing\",\n",
        "                \"Sports fan cheering\",\n",
        "                \"Game attendance\",\n",
        "                \"Stadium navigation\",\n",
        "                \"Athletic event watching\",\n",
        "                \"Audience participation\",\n",
        "                \"Sports appreciation\"\n",
        "            ],\n",
        "            \"construction_site\": [\n",
        "                \"Construction work\",\n",
        "                \"Building development\",\n",
        "                \"Site management\",\n",
        "                \"Material handling\",\n",
        "                \"Construction supervision\",\n",
        "                \"Safety monitoring\",\n",
        "                \"Building process\"\n",
        "            ],\n",
        "            \"medical_facility\": [\n",
        "                \"Healthcare consultation\",\n",
        "                \"Medical treatment\",\n",
        "                \"Patient waiting\",\n",
        "                \"Healthcare delivery\",\n",
        "                \"Medical examination\",\n",
        "                \"Professional care\",\n",
        "                \"Health monitoring\"\n",
        "            ],\n",
        "            \"educational_setting\": [\n",
        "                \"Classroom learning\",\n",
        "                \"Educational instruction\",\n",
        "                \"Student participation\",\n",
        "                \"Academic engagement\",\n",
        "                \"Knowledge acquisition\",\n",
        "                \"Educational discussion\",\n",
        "                \"Scholastic activities\"\n",
        "            ],\n",
        "            \"beach_water_recreation\": [\n",
        "                \"Surfing\",\n",
        "                \"Sunbathing\",\n",
        "                \"Beach volleyball\",\n",
        "                \"Swimming\",\n",
        "                \"Relaxing by the water\",\n",
        "                \"Flying beach kites\",\n",
        "                \"Beach picnicking\",\n",
        "                \"Coastal walking\"\n",
        "            ],\n",
        "            \"sports_venue\": [\n",
        "                \"Professional game playing\",\n",
        "                \"Sports competition\",\n",
        "                \"Athletic training\",\n",
        "                \"Team practice\",\n",
        "                \"Spectator viewing\",\n",
        "                \"Sports coaching\",\n",
        "                \"Tournament participation\",\n",
        "                \"Athletic performance\"\n",
        "            ],\n",
        "            \"professional_kitchen\": [\n",
        "                \"Professional cooking\",\n",
        "                \"Food preparation\",\n",
        "                \"Meal service coordination\",\n",
        "                \"Kitchen operations\",\n",
        "                \"Culinary production\",\n",
        "                \"Chef activities\",\n",
        "                \"Commercial food handling\",\n",
        "                \"Restaurant meal preparation\"\n",
        "            ]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDPI2oYww-eh"
      },
      "outputs": [],
      "source": [
        "# %%writefile object_categories.py\n",
        "OBJECT_CATEGORIES = {\n",
        "                \"furniture\": [56, 57, 58, 59, 60, 61],\n",
        "                \"electronics\": [62, 63, 64, 65, 66, 67, 68, 69, 70],\n",
        "                \"kitchen_items\": [39, 40, 41, 42, 43, 44, 45],\n",
        "                \"food\": [46, 47, 48, 49, 50, 51, 52, 53, 54, 55],\n",
        "                \"vehicles\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "                \"personal_items\": [24, 25, 26, 27, 28, 73, 78, 79]\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lslRwREno-GS"
      },
      "outputs": [],
      "source": [
        "# %%writefile lighting_conditions.py\n",
        "\n",
        "LIGHTING_CONDITIONS = {\n",
        "    \"time_descriptions\": {\n",
        "        \"day_clear\": {\n",
        "        \"general\": \"The scene is captured during clear daylight hours with bright natural lighting.\",\n",
        "        \"bright\": \"The scene is brightly lit with strong, clear daylight.\",\n",
        "        \"medium\": \"The scene is illuminated with moderate daylight under clear conditions.\",\n",
        "        \"dim\": \"The scene is captured in soft daylight on a clear day.\"\n",
        "        },\n",
        "        \"day_cloudy\": {\n",
        "        \"general\": \"The scene is captured during daytime under overcast conditions.\",\n",
        "        \"bright\": \"The scene has the diffused bright lighting of an overcast day.\",\n",
        "        \"medium\": \"The scene has even, soft lighting typical of a cloudy day.\",\n",
        "        \"dim\": \"The scene has the muted lighting of a heavily overcast day.\"\n",
        "        },\n",
        "        \"sunset/sunrise\": {\n",
        "        \"general\": \"The scene is captured during golden hour with warm lighting.\",\n",
        "        \"bright\": \"The scene is illuminated with bright golden hour light with long shadows.\",\n",
        "        \"medium\": \"The scene has the warm orange-yellow glow typical of sunset or sunrise.\",\n",
        "        \"dim\": \"The scene has soft, warm lighting characteristic of early sunrise or late sunset.\"\n",
        "        },\n",
        "        \"night\": {\n",
        "        \"general\": \"The scene is captured at night with limited natural lighting.\",\n",
        "        \"bright\": \"The scene is captured at night but well-lit with artificial lighting.\",\n",
        "        \"medium\": \"The scene is captured at night with moderate artificial lighting.\",\n",
        "        \"dim\": \"The scene is captured in low-light night conditions with minimal illumination.\"\n",
        "        },\n",
        "        \"indoor_bright\": {\n",
        "        \"general\": \"The scene is captured indoors with ample lighting.\",\n",
        "        \"bright\": \"The indoor space is brightly lit, possibly with natural light from windows.\",\n",
        "        \"medium\": \"The indoor space has good lighting conditions.\",\n",
        "        \"dim\": \"The indoor space has adequate lighting.\"\n",
        "        },\n",
        "        \"indoor_moderate\": {\n",
        "        \"general\": \"The scene is captured indoors with moderate lighting.\",\n",
        "        \"bright\": \"The indoor space has comfortable, moderate lighting.\",\n",
        "        \"medium\": \"The indoor space has standard interior lighting.\",\n",
        "        \"dim\": \"The indoor space has somewhat subdued lighting.\"\n",
        "        },\n",
        "        \"indoor_dim\": {\n",
        "        \"general\": \"The scene is captured indoors with dim or mood lighting.\",\n",
        "        \"bright\": \"The indoor space has dim but sufficient lighting.\",\n",
        "        \"medium\": \"The indoor space has low, atmospheric lighting.\",\n",
        "        \"dim\": \"The indoor space has very dim, possibly mood-oriented lighting.\"\n",
        "        },\n",
        "        \"beach_daylight\": {\n",
        "            \"general\": \"The scene is captured during daytime at a beach with bright natural sunlight.\",\n",
        "            \"bright\": \"The beach scene is intensely illuminated by direct sunlight.\",\n",
        "            \"medium\": \"The coastal area has even natural daylight.\",\n",
        "            \"dim\": \"The beach has softer lighting, possibly from a partially cloudy sky.\"\n",
        "        },\n",
        "        \"sports_arena\": {\n",
        "            \"general\": \"The scene is captured in a sports venue with specialized arena lighting.\",\n",
        "            \"bright\": \"The sports facility is brightly illuminated with powerful overhead lights.\",\n",
        "            \"medium\": \"The venue has standard sports event lighting providing clear visibility.\",\n",
        "            \"dim\": \"The sports area has reduced illumination, possibly before or after an event.\"\n",
        "        },\n",
        "        \"kitchen_working\": {\n",
        "            \"general\": \"The scene is captured in a professional kitchen with task-oriented lighting.\",\n",
        "            \"bright\": \"The kitchen is intensely illuminated with clear, functional lighting.\",\n",
        "            \"medium\": \"The culinary space has standard working lights focused on preparation areas.\",\n",
        "            \"dim\": \"The kitchen has reduced lighting, possibly during off-peak hours.\"\n",
        "        },\n",
        "        \"unknown\": {\n",
        "        \"general\": \"The lighting conditions in this scene are not easily determined.\"\n",
        "        }\n",
        "    },\n",
        "    \"template_modifiers\": {\n",
        "        \"day_clear\": \"brightly-lit\",\n",
        "        \"day_cloudy\": \"softly-lit\",\n",
        "        \"sunset/sunrise\": \"warmly-lit\",\n",
        "        \"night\": \"night-time\",\n",
        "        \"indoor_bright\": \"well-lit indoor\",\n",
        "        \"indoor_moderate\": \"indoor\",\n",
        "        \"indoor_dim\": \"dimly-lit indoor\",\n",
        "        \"indoor_commercial\": \"retail-lit\",\n",
        "        \"indoor_restaurant\": \"atmospherically-lit\",\n",
        "        \"neon_night\": \"neon-illuminated\",\n",
        "        \"stadium_lighting\": \"flood-lit\",\n",
        "        \"mixed_lighting\": \"transitionally-lit\",\n",
        "        \"beach_lighting\": \"sun-drenched\",\n",
        "        \"sports_venue_lighting\": \"arena-lit\",\n",
        "        \"professional_kitchen_lighting\": \"kitchen-task lit\",\n",
        "        \"unknown\": \"\"\n",
        "    },\n",
        "    \"activity_modifiers\": {\n",
        "        \"day_clear\": [\"active\", \"lively\", \"busy\"],\n",
        "        \"day_cloudy\": [\"calm\", \"relaxed\", \"casual\"],\n",
        "        \"sunset/sunrise\": [\"peaceful\", \"transitional\", \"atmospheric\"],\n",
        "        \"night\": [\"quiet\", \"subdued\", \"nocturnal\"],\n",
        "        \"indoor_bright\": [\"focused\", \"productive\", \"engaged\"],\n",
        "        \"indoor_moderate\": [\"comfortable\", \"social\", \"casual\"],\n",
        "        \"indoor_dim\": [\"intimate\", \"relaxed\", \"private\"],\n",
        "        \"indoor_commercial\": [\"shopping\", \"browsing\", \"consumer-oriented\"],\n",
        "        \"indoor_restaurant\": [\"dining\", \"social\", \"culinary\"],\n",
        "        \"neon_night\": [\"vibrant\", \"energetic\", \"night-life\"],\n",
        "        \"stadium_lighting\": [\"event-focused\", \"spectator-oriented\", \"performance-based\"],\n",
        "        \"mixed_lighting\": [\"transitional\", \"adaptable\", \"variable\"],\n",
        "        \"unknown\": []\n",
        "    },\n",
        "    \"indoor_commercial\": {\n",
        "    \"general\": \"The scene is captured inside a commercial setting with retail-optimized lighting.\",\n",
        "    \"bright\": \"The space is brightly illuminated with commercial display lighting to highlight merchandise.\",\n",
        "    \"medium\": \"The commercial interior has standard retail lighting that balances visibility and ambiance.\",\n",
        "    \"dim\": \"The commercial space has subdued lighting creating an upscale or intimate shopping atmosphere.\"\n",
        "    },\n",
        "    \"indoor_restaurant\": {\n",
        "        \"general\": \"The scene is captured inside a restaurant with characteristic dining lighting.\",\n",
        "        \"bright\": \"The restaurant is well-lit with clear illumination emphasizing food presentation.\",\n",
        "        \"medium\": \"The dining space has moderate lighting striking a balance between functionality and ambiance.\",\n",
        "        \"dim\": \"The restaurant features soft, low lighting creating an intimate dining atmosphere.\"\n",
        "    },\n",
        "    \"neon_night\": {\n",
        "        \"general\": \"The scene is captured at night with colorful neon lighting typical of entertainment districts.\",\n",
        "        \"bright\": \"The night scene is illuminated by vibrant neon signs creating a lively, colorful atmosphere.\",\n",
        "        \"medium\": \"The evening setting features moderate neon lighting creating a characteristic urban nightlife scene.\",\n",
        "        \"dim\": \"The night area has subtle neon accents against the darkness, creating a moody urban atmosphere.\"\n",
        "    },\n",
        "    \"stadium_lighting\": {\n",
        "        \"general\": \"The scene is captured under powerful stadium lights designed for spectator events.\",\n",
        "        \"bright\": \"The venue is intensely illuminated by stadium floodlights creating daylight-like conditions.\",\n",
        "        \"medium\": \"The sports facility has standard event lighting providing clear visibility across the venue.\",\n",
        "        \"dim\": \"The stadium has reduced illumination typical of pre-event or post-event conditions.\"\n",
        "    },\n",
        "    \"mixed_lighting\": {\n",
        "        \"general\": \"The scene features a mix of indoor and outdoor lighting creating transitional illumination.\",\n",
        "        \"bright\": \"The space blends bright natural and artificial light sources across indoor-outdoor boundaries.\",\n",
        "        \"medium\": \"The area combines moderate indoor lighting with outdoor illumination in a balanced way.\",\n",
        "        \"dim\": \"The transition space features subtle lighting gradients between indoor and outdoor zones.\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noAkFOxfchU0"
      },
      "outputs": [],
      "source": [
        "# %%writefile viewpoint_templates.py\n",
        "\n",
        "VIEWPOINT_TEMPLATES = {\n",
        "    \"eye_level\": {\n",
        "        \"prefix\": \"From a standard eye-level perspective, \",\n",
        "        \"observation\": \"the scene shows {scene_elements} arranged in a typical front-facing view.\"\n",
        "    },\n",
        "    \"aerial\": {\n",
        "        \"prefix\": \"From an aerial perspective, \",\n",
        "        \"observation\": \"the scene shows {scene_elements} as viewed from above, revealing the spatial layout.\"\n",
        "    },\n",
        "    \"elevated\": {\n",
        "        \"prefix\": \"From an elevated viewpoint, \",\n",
        "        \"observation\": \"the scene presents {scene_elements} with a slight downward angle.\"\n",
        "    },\n",
        "    \"low_angle\": {\n",
        "        \"prefix\": \"From a low angle, \",\n",
        "        \"observation\": \"the scene depicts {scene_elements} from below, emphasizing vertical elements.\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay8jbYNpchSj"
      },
      "outputs": [],
      "source": [
        "# %%writefile cultural_templates.py\n",
        "\n",
        "CULTURAL_TEMPLATES = {\n",
        "    \"asian\": {\n",
        "        \"elements\": [\"character signage\", \"lanterns\", \"dense urban layout\"],\n",
        "        \"description\": \"The scene shows distinctive Asian cultural elements such as {elements}.\"\n",
        "    },\n",
        "    \"european\": {\n",
        "        \"elements\": [\"classical architecture\", \"cobblestone streets\", \"café terraces\"],\n",
        "        \"description\": \"The environment has European characteristics including {elements}.\"\n",
        "    },\n",
        "    \"middle_eastern\": {\n",
        "        \"elements\": [\"ornate archways\", \"geometric patterns\", \"domed structures\"],\n",
        "        \"description\": \"The scene contains Middle Eastern architectural features such as {elements}.\"\n",
        "    },\n",
        "    \"north_american\": {\n",
        "        \"elements\": [\"grid street pattern\", \"modern skyscrapers\", \"wide boulevards\"],\n",
        "        \"description\": \"The layout shows typical North American urban design with {elements}.\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNbiE_7aW_WA"
      },
      "outputs": [],
      "source": [
        "# %%writefile spatial_analyzer.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "# from scene_type import SCENE_TYPES\n",
        "# from enhance_scene_describer import EnhancedSceneDescriber\n",
        "\n",
        "class SpatialAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzes spatial relationships between objects in an image.\n",
        "    Handles region assignment, object positioning, and functional zone identification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_names: Dict[int, str] = None, object_categories=None):\n",
        "        \"\"\"Initialize the spatial analyzer with image regions\"\"\"\n",
        "        # Define regions of the image (3x3 grid)\n",
        "        self.regions = {\n",
        "            \"top_left\": (0, 0, 1/3, 1/3),\n",
        "            \"top_center\": (1/3, 0, 2/3, 1/3),\n",
        "            \"top_right\": (2/3, 0, 1, 1/3),\n",
        "            \"middle_left\": (0, 1/3, 1/3, 2/3),\n",
        "            \"middle_center\": (1/3, 1/3, 2/3, 2/3),\n",
        "            \"middle_right\": (2/3, 1/3, 1, 2/3),\n",
        "            \"bottom_left\": (0, 2/3, 1/3, 1),\n",
        "            \"bottom_center\": (1/3, 2/3, 2/3, 1),\n",
        "            \"bottom_right\": (2/3, 2/3, 1, 1)\n",
        "        }\n",
        "\n",
        "        self.class_names = class_names\n",
        "        self.OBJECT_CATEGORIES = object_categories or {}\n",
        "        self.enhance_descriptor = EnhancedSceneDescriber(scene_types=SCENE_TYPES)\n",
        "\n",
        "        # Distances thresholds for proximity analysis (normalized)\n",
        "        self.proximity_threshold = 0.2\n",
        "\n",
        "\n",
        "    def _determine_region(self, x: float, y: float) -> str:\n",
        "        \"\"\"\n",
        "        Determine which region a point falls into.\n",
        "\n",
        "        Args:\n",
        "            x: Normalized x-coordinate (0-1)\n",
        "            y: Normalized y-coordinate (0-1)\n",
        "\n",
        "        Returns:\n",
        "            Region name\n",
        "        \"\"\"\n",
        "        for region_name, (x1, y1, x2, y2) in self.regions.items():\n",
        "            if x1 <= x < x2 and y1 <= y < y2:\n",
        "                return region_name\n",
        "\n",
        "        return \"unknown\"\n",
        "\n",
        "    def _analyze_regions(self, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze object distribution across image regions.\n",
        "\n",
        "        Args:\n",
        "            detected_objects: List of detected objects with position information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with region analysis\n",
        "        \"\"\"\n",
        "        # Count objects in each region\n",
        "        region_counts = {region: 0 for region in self.regions.keys()}\n",
        "        region_objects = {region: [] for region in self.regions.keys()}\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            region = obj[\"region\"]\n",
        "            if region in region_counts:\n",
        "                region_counts[region] += 1\n",
        "                region_objects[region].append({\n",
        "                    \"class_id\": obj[\"class_id\"],\n",
        "                    \"class_name\": obj[\"class_name\"]\n",
        "                })\n",
        "\n",
        "        # Determine main focus regions (top 1-2 regions by object count)\n",
        "        sorted_regions = sorted(region_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        main_regions = [region for region, count in sorted_regions if count > 0][:2]\n",
        "\n",
        "        return {\n",
        "            \"counts\": region_counts,\n",
        "            \"main_focus\": main_regions,\n",
        "            \"objects_by_region\": region_objects\n",
        "        }\n",
        "\n",
        "    def _extract_detected_objects(self, detection_result: Any, confidence_threshold: float = 0.25) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract detected objects from detection result with position information.\n",
        "\n",
        "        Args:\n",
        "            detection_result: Detection result from YOLOv8\n",
        "            confidence_threshold: Minimum confidence threshold\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with detected object information\n",
        "        \"\"\"\n",
        "        boxes = detection_result.boxes.xyxy.cpu().numpy()\n",
        "        classes = detection_result.boxes.cls.cpu().numpy().astype(int)\n",
        "        confidences = detection_result.boxes.conf.cpu().numpy()\n",
        "\n",
        "        # Image dimensions\n",
        "        img_height, img_width = detection_result.orig_shape[:2]\n",
        "\n",
        "        detected_objects = []\n",
        "        for box, class_id, confidence in zip(boxes, classes, confidences):\n",
        "            # Skip objects with confidence below threshold\n",
        "            if confidence < confidence_threshold:\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = box\n",
        "            width = x2 - x1\n",
        "            height = y2 - y1\n",
        "\n",
        "            # Center point\n",
        "            center_x = (x1 + x2) / 2\n",
        "            center_y = (y1 + y2) / 2\n",
        "\n",
        "            # Normalized positions (0-1)\n",
        "            norm_x = center_x / img_width\n",
        "            norm_y = center_y / img_height\n",
        "            norm_width = width / img_width\n",
        "            norm_height = height / img_height\n",
        "\n",
        "            # Area calculation\n",
        "            area = width * height\n",
        "            norm_area = area / (img_width * img_height)\n",
        "\n",
        "            # Region determination\n",
        "            object_region = self._determine_region(norm_x, norm_y)\n",
        "\n",
        "            detected_objects.append({\n",
        "                \"class_id\": int(class_id),\n",
        "                \"class_name\": self.class_names[int(class_id)],\n",
        "                \"confidence\": float(confidence),\n",
        "                \"box\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                \"center\": [float(center_x), float(center_y)],\n",
        "                \"normalized_center\": [float(norm_x), float(norm_y)],\n",
        "                \"size\": [float(width), float(height)],\n",
        "                \"normalized_size\": [float(norm_width), float(norm_height)],\n",
        "                \"area\": float(area),\n",
        "                \"normalized_area\": float(norm_area),\n",
        "                \"region\": object_region\n",
        "            })\n",
        "\n",
        "        return detected_objects\n",
        "\n",
        "\n",
        "    def _detect_scene_viewpoint(self, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        檢測場景視角並識別特殊場景模式。\n",
        "\n",
        "        Args:\n",
        "            detected_objects: 檢測到的物體列表\n",
        "\n",
        "        Returns:\n",
        "            Dict: 包含視角和場景模式信息的字典\n",
        "        \"\"\"\n",
        "        if not detected_objects:\n",
        "            return {\"viewpoint\": \"eye_level\", \"patterns\": []}\n",
        "\n",
        "        # 從物體位置中提取信息\n",
        "        patterns = []\n",
        "\n",
        "        # 檢測行人位置模式\n",
        "        pedestrian_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "\n",
        "        # 檢查是否有足夠的行人來識別模式\n",
        "        if len(pedestrian_objs) >= 4:\n",
        "            pedestrian_positions = [obj[\"normalized_center\"] for obj in pedestrian_objs]\n",
        "\n",
        "            # 檢測十字交叉模式\n",
        "            if self._detect_cross_pattern(pedestrian_positions):\n",
        "                patterns.append(\"crosswalk_intersection\")\n",
        "\n",
        "            # 檢測多方向行人流\n",
        "            directions = self._analyze_movement_directions(pedestrian_positions)\n",
        "            if len(directions) >= 2:\n",
        "                patterns.append(\"multi_directional_movement\")\n",
        "\n",
        "        # 檢查物體的大小一致性 - 在空中俯視圖中，物體大小通常更一致\n",
        "        if len(detected_objects) >= 5:\n",
        "            sizes = [obj.get(\"normalized_area\", 0) for obj in detected_objects]\n",
        "            size_variance = np.var(sizes) / (np.mean(sizes) ** 2)  # 標準化變異數，不會受到平均值影響\n",
        "\n",
        "            if size_variance < 0.3:  # 低變異表示大小一致\n",
        "                patterns.append(\"consistent_object_size\")\n",
        "\n",
        "        # 基本視角檢測\n",
        "        viewpoint = self.enhance_descriptor._detect_viewpoint(detected_objects)\n",
        "\n",
        "        # 根據檢測到的模式增強視角判斷\n",
        "        if \"crosswalk_intersection\" in patterns and viewpoint != \"aerial\":\n",
        "            # 如果檢測到斑馬線交叉但視角判斷不是空中視角，優先採用模式判斷\n",
        "            viewpoint = \"aerial\"\n",
        "\n",
        "        return {\n",
        "            \"viewpoint\": viewpoint,\n",
        "            \"patterns\": patterns\n",
        "        }\n",
        "\n",
        "    def _detect_cross_pattern(self, positions):\n",
        "        \"\"\"\n",
        "        檢測位置中的十字交叉模式\n",
        "\n",
        "        Args:\n",
        "            positions: 位置列表 [[x1, y1], [x2, y2], ...]\n",
        "\n",
        "        Returns:\n",
        "            bool: 是否檢測到十字交叉模式\n",
        "        \"\"\"\n",
        "        if len(positions) < 8:  # 需要足夠多的點\n",
        "            return False\n",
        "\n",
        "        # 提取 x 和 y 坐標\n",
        "        x_coords = [pos[0] for pos in positions]\n",
        "        y_coords = [pos[1] for pos in positions]\n",
        "\n",
        "        # 檢測 x 和 y 方向的聚類\n",
        "        x_clusters = []\n",
        "        y_clusters = []\n",
        "\n",
        "        # 簡化的聚類分析\n",
        "        x_mean = np.mean(x_coords)\n",
        "        y_mean = np.mean(y_coords)\n",
        "\n",
        "        # 計算在中心線附近的點\n",
        "        near_x_center = sum(1 for x in x_coords if abs(x - x_mean) < 0.1)\n",
        "        near_y_center = sum(1 for y in y_coords if abs(y - y_mean) < 0.1)\n",
        "\n",
        "        # 如果有足夠的點在中心線附近，可能是十字交叉\n",
        "        return near_x_center >= 3 and near_y_center >= 3\n",
        "\n",
        "    def _analyze_movement_directions(self, positions):\n",
        "        \"\"\"\n",
        "        分析位置中的移動方向\n",
        "\n",
        "        Args:\n",
        "            positions: 位置列表 [[x1, y1], [x2, y2], ...]\n",
        "\n",
        "        Returns:\n",
        "            list: 檢測到的主要方向\n",
        "        \"\"\"\n",
        "        if len(positions) < 6:\n",
        "            return []\n",
        "\n",
        "        # extract x 和 y 坐標\n",
        "        x_coords = [pos[0] for pos in positions]\n",
        "        y_coords = [pos[1] for pos in positions]\n",
        "\n",
        "        directions = []\n",
        "\n",
        "        # horizontal move (left --> right)\n",
        "        x_std = np.std(x_coords)\n",
        "        x_range = max(x_coords) - min(x_coords)\n",
        "\n",
        "        # vertical move(up --> down)\n",
        "        y_std = np.std(y_coords)\n",
        "        y_range = max(y_coords) - min(y_coords)\n",
        "\n",
        "        # 足夠大的範圍表示該方向有運動\n",
        "        if x_range > 0.4:\n",
        "            directions.append(\"horizontal\")\n",
        "        if y_range > 0.4:\n",
        "            directions.append(\"vertical\")\n",
        "\n",
        "        return directions\n",
        "\n",
        "    def _identify_functional_zones(self, detected_objects: List[Dict], scene_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones within the scene with improved detection for different viewpoints\n",
        "        and cultural contexts.\n",
        "\n",
        "        Args:\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Identified scene type\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of functional zones with their descriptions\n",
        "        \"\"\"\n",
        "        # Group objects by category and region\n",
        "        category_regions = {}\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            # Find object category\n",
        "            category = \"other\"\n",
        "            for cat_name, cat_ids in self.OBJECT_CATEGORIES.items():\n",
        "                if obj[\"class_id\"] in cat_ids:\n",
        "                    category = cat_name\n",
        "                    break\n",
        "\n",
        "            # Add to category-region mapping\n",
        "            if category not in category_regions:\n",
        "                category_regions[category] = {}\n",
        "\n",
        "            region = obj[\"region\"]\n",
        "            if region not in category_regions[category]:\n",
        "                category_regions[category][region] = []\n",
        "\n",
        "            category_regions[category][region].append(obj)\n",
        "\n",
        "        # Identify zones based on object groupings\n",
        "        zones = {}\n",
        "\n",
        "        # Detect viewpoint to adjust zone identification strategy\n",
        "        viewpoint = self._detect_scene_viewpoint(detected_objects)\n",
        "\n",
        "        # Choose appropriate zone identification strategy based on scene type and viewpoint\n",
        "        if scene_type in [\"living_room\", \"bedroom\", \"dining_area\", \"kitchen\", \"office_workspace\", \"meeting_room\"]:\n",
        "            # Indoor scenes\n",
        "            zones.update(self._identify_indoor_zones(category_regions, detected_objects, scene_type))\n",
        "        elif scene_type in [\"city_street\", \"parking_lot\", \"park_area\"]:\n",
        "            # Outdoor general scenes\n",
        "            zones.update(self._identify_outdoor_general_zones(category_regions, detected_objects, scene_type))\n",
        "        elif \"aerial\" in scene_type or viewpoint == \"aerial\":\n",
        "            # Aerial viewpoint scenes\n",
        "            zones.update(self._identify_aerial_view_zones(category_regions, detected_objects, scene_type))\n",
        "        elif \"asian\" in scene_type:\n",
        "            # Asian cultural context scenes\n",
        "            zones.update(self._identify_asian_cultural_zones(category_regions, detected_objects, scene_type))\n",
        "        elif scene_type == \"urban_intersection\":\n",
        "            # Specific urban intersection logic\n",
        "            zones.update(self._identify_intersection_zones(category_regions, detected_objects, viewpoint))\n",
        "        elif scene_type == \"financial_district\":\n",
        "            # Financial district specific logic\n",
        "            zones.update(self._identify_financial_district_zones(category_regions, detected_objects))\n",
        "        elif scene_type == \"upscale_dining\":\n",
        "            # Upscale dining specific logic\n",
        "            zones.update(self._identify_upscale_dining_zones(category_regions, detected_objects))\n",
        "        else:\n",
        "            # Default zone identification for other scene types\n",
        "            zones.update(self._identify_default_zones(category_regions, detected_objects))\n",
        "\n",
        "        # If no zones were identified, try the default approach\n",
        "        if not zones:\n",
        "            zones.update(self._identify_default_zones(category_regions, detected_objects))\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_indoor_zones(self, category_regions: Dict, detected_objects: List[Dict], scene_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for indoor scenes.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Specific indoor scene type\n",
        "\n",
        "        Returns:\n",
        "            Dict: Indoor functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Seating/social zone\n",
        "        if \"furniture\" in category_regions:\n",
        "            furniture_regions = category_regions[\"furniture\"]\n",
        "            main_furniture_region = max(furniture_regions.items(),\n",
        "                                    key=lambda x: len(x[1]),\n",
        "                                    default=(None, []))\n",
        "\n",
        "            if main_furniture_region[0] is not None and len(main_furniture_region[1]) >= 2:\n",
        "                zone_objects = [obj[\"class_name\"] for obj in main_furniture_region[1]]\n",
        "                zones[\"social_zone\"] = {\n",
        "                    \"region\": main_furniture_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Social or seating area with {', '.join(zone_objects)}\"\n",
        "                }\n",
        "\n",
        "        # Entertainment zone\n",
        "        if \"electronics\" in category_regions:\n",
        "            electronics_items = []\n",
        "            for region_objects in category_regions[\"electronics\"].values():\n",
        "                electronics_items.extend([obj[\"class_name\"] for obj in region_objects])\n",
        "\n",
        "            if electronics_items:\n",
        "                zones[\"entertainment_zone\"] = {\n",
        "                    \"region\": self._find_main_region(category_regions.get(\"electronics\", {})),\n",
        "                    \"objects\": electronics_items,\n",
        "                    \"description\": f\"Entertainment or media area with {', '.join(electronics_items)}\"\n",
        "                }\n",
        "\n",
        "        # Dining/food zone\n",
        "        food_zone_categories = [\"kitchen_items\", \"food\"]\n",
        "        food_items = []\n",
        "        food_regions = {}\n",
        "\n",
        "        for category in food_zone_categories:\n",
        "            if category in category_regions:\n",
        "                for region, objects in category_regions[category].items():\n",
        "                    if region not in food_regions:\n",
        "                        food_regions[region] = []\n",
        "                    food_regions[region].extend(objects)\n",
        "                    food_items.extend([obj[\"class_name\"] for obj in objects])\n",
        "\n",
        "        if food_items:\n",
        "            main_food_region = max(food_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_food_region[0] is not None:\n",
        "                zones[\"dining_zone\"] = {\n",
        "                    \"region\": main_food_region[0],\n",
        "                    \"objects\": list(set(food_items)),\n",
        "                    \"description\": f\"Dining or food preparation area with {', '.join(list(set(food_items))[:3])}\"\n",
        "                }\n",
        "\n",
        "        # Work/study zone - enhanced to detect even when scene_type is not explicitly office\n",
        "        work_items = []\n",
        "        work_regions = {}\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [56, 60, 63, 64, 66, 73]:  # chair, table, laptop, mouse, keyboard, book\n",
        "                region = obj[\"region\"]\n",
        "                if region not in work_regions:\n",
        "                    work_regions[region] = []\n",
        "                work_regions[region].append(obj)\n",
        "                work_items.append(obj[\"class_name\"])\n",
        "\n",
        "        # Check for laptop and table/chair combinations that suggest a workspace\n",
        "        has_laptop = any(obj[\"class_id\"] == 63 for obj in detected_objects)\n",
        "        has_keyboard = any(obj[\"class_id\"] == 66 for obj in detected_objects)\n",
        "        has_table = any(obj[\"class_id\"] == 60 for obj in detected_objects)\n",
        "        has_chair = any(obj[\"class_id\"] == 56 for obj in detected_objects)\n",
        "\n",
        "        # If we have electronics with furniture in the same region, likely a workspace\n",
        "        workspace_detected = (has_laptop or has_keyboard) and (has_table or has_chair)\n",
        "\n",
        "        if (workspace_detected or scene_type in [\"office_workspace\", \"meeting_room\"]) and work_items:\n",
        "            main_work_region = max(work_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_work_region[0] is not None:\n",
        "                zones[\"workspace_zone\"] = {\n",
        "                    \"region\": main_work_region[0],\n",
        "                    \"objects\": list(set(work_items)),\n",
        "                    \"description\": f\"Work or study area with {', '.join(list(set(work_items))[:3])}\"\n",
        "                }\n",
        "\n",
        "        # Bedroom-specific zones\n",
        "        if scene_type == \"bedroom\":\n",
        "            bed_objects = [obj for obj in detected_objects if obj[\"class_id\"] == 59]  # Bed\n",
        "            if bed_objects:\n",
        "                bed_region = bed_objects[0][\"region\"]\n",
        "                zones[\"sleeping_zone\"] = {\n",
        "                    \"region\": bed_region,\n",
        "                    \"objects\": [\"bed\"],\n",
        "                    \"description\": \"Sleeping area with bed\"\n",
        "                }\n",
        "\n",
        "        # Kitchen-specific zones\n",
        "        if scene_type == \"kitchen\":\n",
        "            # Look for appliances (refrigerator, oven, microwave, sink)\n",
        "            appliance_ids = [68, 69, 71, 72]  # microwave, oven, sink, refrigerator\n",
        "            appliance_objects = [obj for obj in detected_objects if obj[\"class_id\"] in appliance_ids]\n",
        "\n",
        "            if appliance_objects:\n",
        "                appliance_regions = {}\n",
        "                for obj in appliance_objects:\n",
        "                    region = obj[\"region\"]\n",
        "                    if region not in appliance_regions:\n",
        "                        appliance_regions[region] = []\n",
        "                    appliance_regions[region].append(obj)\n",
        "\n",
        "                if appliance_regions:\n",
        "                    main_appliance_region = max(appliance_regions.items(),\n",
        "                                            key=lambda x: len(x[1]),\n",
        "                                            default=(None, []))\n",
        "\n",
        "                    if main_appliance_region[0] is not None:\n",
        "                        appliance_names = [obj[\"class_name\"] for obj in main_appliance_region[1]]\n",
        "                        zones[\"kitchen_appliance_zone\"] = {\n",
        "                            \"region\": main_appliance_region[0],\n",
        "                            \"objects\": appliance_names,\n",
        "                            \"description\": f\"Kitchen appliance area with {', '.join(appliance_names)}\"\n",
        "                        }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_intersection_zones(self, category_regions: Dict, detected_objects: List[Dict], viewpoint: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for urban intersections with enhanced spatial awareness.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "            viewpoint: Detected viewpoint\n",
        "\n",
        "        Returns:\n",
        "            Dict: Refined intersection functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Get pedestrians, vehicles and traffic signals\n",
        "        pedestrian_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "        vehicle_objs = [obj for obj in detected_objects if obj[\"class_id\"] in [1, 2, 3, 5, 7]]  # bicycle, car, motorcycle, bus, truck\n",
        "        traffic_light_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 9]\n",
        "\n",
        "        # Create distribution maps for better spatial understanding\n",
        "        regions_distribution = self._create_distribution_map(detected_objects)\n",
        "\n",
        "        # Analyze pedestrian crossing patterns\n",
        "        crossing_zones = self._analyze_crossing_patterns(pedestrian_objs, traffic_light_objs, regions_distribution)\n",
        "        zones.update(crossing_zones)\n",
        "\n",
        "        # Analyze vehicle traffic zones with directional awareness\n",
        "        traffic_zones = self._analyze_traffic_zones(vehicle_objs, regions_distribution)\n",
        "        zones.update(traffic_zones)\n",
        "\n",
        "        # Identify traffic control zones based on signal placement\n",
        "        if traffic_light_objs:\n",
        "            # Group traffic lights by region for better organization\n",
        "            signal_regions = {}\n",
        "            for obj in traffic_light_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in signal_regions:\n",
        "                    signal_regions[region] = []\n",
        "                signal_regions[region].append(obj)\n",
        "\n",
        "            # Create traffic control zones for each region with signals\n",
        "            for idx, (region, signals) in enumerate(signal_regions.items()):\n",
        "                # Check if this region has a directional name\n",
        "                direction = self._get_directional_description(region)\n",
        "\n",
        "                zones[f\"traffic_control_zone_{idx+1}\"] = {\n",
        "                    \"region\": region,\n",
        "                    \"objects\": [\"traffic light\"] * len(signals),\n",
        "                    \"description\": f\"Traffic control area with {len(signals)} traffic signals\" +\n",
        "                                (f\" in {direction} area\" if direction else \"\")\n",
        "                }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _analyze_crossing_patterns(self, pedestrians: List[Dict], traffic_lights: List[Dict],\n",
        "                                region_distribution: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze pedestrian crossing patterns to identify crosswalk zones.\n",
        "\n",
        "        Args:\n",
        "            pedestrians: List of pedestrian objects\n",
        "            traffic_lights: List of traffic light objects\n",
        "            region_distribution: Distribution of objects by region\n",
        "\n",
        "        Returns:\n",
        "            Dict: Identified crossing zones\n",
        "        \"\"\"\n",
        "        crossing_zones = {}\n",
        "\n",
        "        if not pedestrians:\n",
        "            return crossing_zones\n",
        "\n",
        "        # Group pedestrians by region\n",
        "        pedestrian_regions = {}\n",
        "        for p in pedestrians:\n",
        "            region = p[\"region\"]\n",
        "            if region not in pedestrian_regions:\n",
        "                pedestrian_regions[region] = []\n",
        "            pedestrian_regions[region].append(p)\n",
        "\n",
        "        # Sort regions by pedestrian count to find main crossing areas\n",
        "        sorted_regions = sorted(pedestrian_regions.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "        # Create crossing zones for regions with pedestrians\n",
        "        for idx, (region, peds) in enumerate(sorted_regions[:2]):  # Focus on top 2 regions\n",
        "            # Check if there are traffic lights nearby to indicate a crosswalk\n",
        "            has_nearby_signals = any(t[\"region\"] == region for t in traffic_lights)\n",
        "\n",
        "            # Create crossing zone with descriptive naming\n",
        "            zone_name = f\"crossing_zone_{idx+1}\"\n",
        "            direction = self._get_directional_description(region)\n",
        "\n",
        "            description = f\"Pedestrian crossing area with {len(peds)} \"\n",
        "            description += \"person\" if len(peds) == 1 else \"people\"\n",
        "            if direction:\n",
        "                description += f\" in {direction} direction\"\n",
        "            if has_nearby_signals:\n",
        "                description += \" near traffic signals\"\n",
        "\n",
        "            crossing_zones[zone_name] = {\n",
        "                \"region\": region,\n",
        "                \"objects\": [\"pedestrian\"] * len(peds),\n",
        "                \"description\": description\n",
        "            }\n",
        "\n",
        "        return crossing_zones\n",
        "\n",
        "    def _analyze_traffic_zones(self, vehicles: List[Dict], region_distribution: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze vehicle distribution to identify traffic zones with directional awareness.\n",
        "\n",
        "        Args:\n",
        "            vehicles: List of vehicle objects\n",
        "            region_distribution: Distribution of objects by region\n",
        "\n",
        "        Returns:\n",
        "            Dict: Identified traffic zones\n",
        "        \"\"\"\n",
        "        traffic_zones = {}\n",
        "\n",
        "        if not vehicles:\n",
        "            return traffic_zones\n",
        "\n",
        "        # Group vehicles by region\n",
        "        vehicle_regions = {}\n",
        "        for v in vehicles:\n",
        "            region = v[\"region\"]\n",
        "            if region not in vehicle_regions:\n",
        "                vehicle_regions[region] = []\n",
        "            vehicle_regions[region].append(v)\n",
        "\n",
        "        # Create traffic zones for regions with vehicles\n",
        "        main_traffic_region = max(vehicle_regions.items(), key=lambda x: len(x[1]), default=(None, []))\n",
        "\n",
        "        if main_traffic_region[0] is not None:\n",
        "            region = main_traffic_region[0]\n",
        "            vehicles_in_region = main_traffic_region[1]\n",
        "\n",
        "            # Get a list of vehicle types for description\n",
        "            vehicle_types = [v[\"class_name\"] for v in vehicles_in_region]\n",
        "            unique_types = list(set(vehicle_types))\n",
        "\n",
        "            # Get directional description\n",
        "            direction = self._get_directional_description(region)\n",
        "\n",
        "            # Create descriptive zone\n",
        "            traffic_zones[\"vehicle_zone\"] = {\n",
        "                \"region\": region,\n",
        "                \"objects\": vehicle_types,\n",
        "                \"description\": f\"Vehicle traffic area with {', '.join(unique_types[:3])}\" +\n",
        "                            (f\" in {direction} area\" if direction else \"\")\n",
        "            }\n",
        "\n",
        "            # If vehicles are distributed across multiple regions, create secondary zones\n",
        "            if len(vehicle_regions) > 1:\n",
        "                # Get second most populated region\n",
        "                sorted_regions = sorted(vehicle_regions.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "                if len(sorted_regions) > 1:\n",
        "                    second_region, second_vehicles = sorted_regions[1]\n",
        "                    direction = self._get_directional_description(second_region)\n",
        "                    vehicle_types = [v[\"class_name\"] for v in second_vehicles]\n",
        "                    unique_types = list(set(vehicle_types))\n",
        "\n",
        "                    traffic_zones[\"secondary_vehicle_zone\"] = {\n",
        "                        \"region\": second_region,\n",
        "                        \"objects\": vehicle_types,\n",
        "                        \"description\": f\"Secondary traffic area with {', '.join(unique_types[:2])}\" +\n",
        "                                    (f\" in {direction} direction\" if direction else \"\")\n",
        "                    }\n",
        "\n",
        "        return traffic_zones\n",
        "\n",
        "    def _get_directional_description(self, region: str) -> str:\n",
        "        \"\"\"\n",
        "        Convert region name to a directional description.\n",
        "\n",
        "        Args:\n",
        "            region: Region name from the grid\n",
        "\n",
        "        Returns:\n",
        "            str: Directional description\n",
        "        \"\"\"\n",
        "        if \"top\" in region and \"left\" in region:\n",
        "            return \"northwest\"\n",
        "        elif \"top\" in region and \"right\" in region:\n",
        "            return \"northeast\"\n",
        "        elif \"bottom\" in region and \"left\" in region:\n",
        "            return \"southwest\"\n",
        "        elif \"bottom\" in region and \"right\" in region:\n",
        "            return \"southeast\"\n",
        "        elif \"top\" in region:\n",
        "            return \"north\"\n",
        "        elif \"bottom\" in region:\n",
        "            return \"south\"\n",
        "        elif \"left\" in region:\n",
        "            return \"west\"\n",
        "        elif \"right\" in region:\n",
        "            return \"east\"\n",
        "        else:\n",
        "            return \"central\"\n",
        "\n",
        "    def _create_distribution_map(self, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Create a distribution map of objects across regions for spatial analysis.\n",
        "\n",
        "        Args:\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            Dict: Distribution map of objects by region and class\n",
        "        \"\"\"\n",
        "        distribution = {}\n",
        "\n",
        "        # Initialize all regions\n",
        "        for region in self.regions.keys():\n",
        "            distribution[region] = {\n",
        "                \"total\": 0,\n",
        "                \"objects\": {},\n",
        "                \"density\": 0\n",
        "            }\n",
        "\n",
        "        # Populate the distribution\n",
        "        for obj in detected_objects:\n",
        "            region = obj[\"region\"]\n",
        "            class_id = obj[\"class_id\"]\n",
        "            class_name = obj[\"class_name\"]\n",
        "\n",
        "            distribution[region][\"total\"] += 1\n",
        "\n",
        "            if class_id not in distribution[region][\"objects\"]:\n",
        "                distribution[region][\"objects\"][class_id] = {\n",
        "                    \"name\": class_name,\n",
        "                    \"count\": 0,\n",
        "                    \"positions\": []\n",
        "                }\n",
        "\n",
        "            distribution[region][\"objects\"][class_id][\"count\"] += 1\n",
        "\n",
        "            # Store position for spatial relationship analysis\n",
        "            if \"normalized_center\" in obj:\n",
        "                distribution[region][\"objects\"][class_id][\"positions\"].append(obj[\"normalized_center\"])\n",
        "\n",
        "        # Calculate object density for each region\n",
        "        for region, data in distribution.items():\n",
        "            # Assuming all regions are equal size in the grid\n",
        "            data[\"density\"] = data[\"total\"] / 1\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def _identify_asian_cultural_zones(self, category_regions: Dict, detected_objects: List[Dict], scene_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for scenes with Asian cultural context.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Specific scene type\n",
        "\n",
        "        Returns:\n",
        "            Dict: Asian cultural functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Identify storefront zone\n",
        "        storefront_items = []\n",
        "        storefront_regions = {}\n",
        "\n",
        "        # Since storefronts aren't directly detectable, infer from context\n",
        "        # For example, look for regions with signs, people, and smaller objects\n",
        "        sign_regions = set()\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] == 0:  # Person\n",
        "                region = obj[\"region\"]\n",
        "                if region not in storefront_regions:\n",
        "                    storefront_regions[region] = []\n",
        "                storefront_regions[region].append(obj)\n",
        "\n",
        "                # Add regions with people as potential storefront areas\n",
        "                sign_regions.add(region)\n",
        "\n",
        "        # Use the areas with most people as storefront zones\n",
        "        if storefront_regions:\n",
        "            main_storefront_regions = sorted(storefront_regions.items(),\n",
        "                                        key=lambda x: len(x[1]),\n",
        "                                        reverse=True)[:2]  # Top 2 regions\n",
        "\n",
        "            for idx, (region, objs) in enumerate(main_storefront_regions):\n",
        "                zones[f\"commercial_zone_{idx+1}\"] = {\n",
        "                    \"region\": region,\n",
        "                    \"objects\": [obj[\"class_name\"] for obj in objs],\n",
        "                    \"description\": f\"Asian commercial storefront with pedestrian activity\"\n",
        "                }\n",
        "\n",
        "        # Identify pedestrian pathway - enhanced to better detect linear pathways\n",
        "        pathway_items = []\n",
        "        pathway_regions = {}\n",
        "\n",
        "        # Extract people for pathway analysis\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "\n",
        "        # Analyze if people form a line (typical of shopping streets)\n",
        "        people_positions = [obj[\"normalized_center\"] for obj in people_objs]\n",
        "\n",
        "        structured_path = False\n",
        "        if len(people_positions) >= 3:\n",
        "            # Check if people are arranged along a similar y-coordinate (horizontal path)\n",
        "            y_coords = [pos[1] for pos in people_positions]\n",
        "            y_mean = sum(y_coords) / len(y_coords)\n",
        "            y_variance = sum((y - y_mean)**2 for y in y_coords) / len(y_coords)\n",
        "\n",
        "            horizontal_path = y_variance < 0.05  # Low variance indicates horizontal alignment\n",
        "\n",
        "            # Check if people are arranged along a similar x-coordinate (vertical path)\n",
        "            x_coords = [pos[0] for pos in people_positions]\n",
        "            x_mean = sum(x_coords) / len(x_coords)\n",
        "            x_variance = sum((x - x_mean)**2 for x in x_coords) / len(x_coords)\n",
        "\n",
        "            vertical_path = x_variance < 0.05  # Low variance indicates vertical alignment\n",
        "\n",
        "            structured_path = horizontal_path or vertical_path\n",
        "            path_direction = \"horizontal\" if horizontal_path else \"vertical\" if vertical_path else \"meandering\"\n",
        "\n",
        "        # Collect pathway objects (people, bicycles, motorcycles in middle area)\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [0, 1, 3]:  # Person, bicycle, motorcycle\n",
        "                y_pos = obj[\"normalized_center\"][1]\n",
        "                # Group by vertical position (middle of image likely pathway)\n",
        "                if 0.25 <= y_pos <= 0.75:\n",
        "                    region = obj[\"region\"]\n",
        "                    if region not in pathway_regions:\n",
        "                        pathway_regions[region] = []\n",
        "                    pathway_regions[region].append(obj)\n",
        "                    pathway_items.append(obj[\"class_name\"])\n",
        "\n",
        "        if pathway_items:\n",
        "            path_desc = \"Pedestrian walkway with people moving through the commercial area\"\n",
        "            if structured_path:\n",
        "                path_desc = f\"{path_direction.capitalize()} pedestrian walkway with organized foot traffic\"\n",
        "\n",
        "            zones[\"pedestrian_pathway\"] = {\n",
        "                \"region\": \"middle_center\",  # Assumption: pathway often in middle\n",
        "                \"objects\": list(set(pathway_items)),\n",
        "                \"description\": path_desc\n",
        "            }\n",
        "\n",
        "        # Identify vendor zone (small stalls/shops - inferred from context)\n",
        "        has_small_objects = any(obj[\"class_id\"] in [24, 26, 39, 41] for obj in detected_objects)  # bags, bottles, cups\n",
        "        has_people = any(obj[\"class_id\"] == 0 for obj in detected_objects)\n",
        "\n",
        "        if has_small_objects and has_people:\n",
        "            # Likely vendor areas are where people and small objects cluster\n",
        "            small_obj_regions = {}\n",
        "\n",
        "            for obj in detected_objects:\n",
        "                if obj[\"class_id\"] in [24, 26, 39, 41, 67]:  # bags, bottles, cups, phones\n",
        "                    region = obj[\"region\"]\n",
        "                    if region not in small_obj_regions:\n",
        "                        small_obj_regions[region] = []\n",
        "                    small_obj_regions[region].append(obj)\n",
        "\n",
        "            if small_obj_regions:\n",
        "                main_vendor_region = max(small_obj_regions.items(),\n",
        "                                    key=lambda x: len(x[1]),\n",
        "                                    default=(None, []))\n",
        "\n",
        "                if main_vendor_region[0] is not None:\n",
        "                    vendor_items = [obj[\"class_name\"] for obj in main_vendor_region[1]]\n",
        "                    zones[\"vendor_zone\"] = {\n",
        "                        \"region\": main_vendor_region[0],\n",
        "                        \"objects\": list(set(vendor_items)),\n",
        "                        \"description\": \"Vendor or market stall area with small merchandise\"\n",
        "                    }\n",
        "\n",
        "        # For night markets, identify illuminated zones\n",
        "        if scene_type == \"asian_night_market\":\n",
        "            # Night markets typically have bright spots for food stalls\n",
        "            # This would be enhanced with lighting analysis integration\n",
        "            zones[\"food_stall_zone\"] = {\n",
        "                \"region\": \"middle_center\",\n",
        "                \"objects\": [\"inferred food stalls\"],\n",
        "                \"description\": \"Food stall area typical of Asian night markets\"\n",
        "            }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_upscale_dining_zones(self, category_regions: Dict, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for upscale dining settings.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            Dict: Upscale dining functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Identify dining table zone\n",
        "        dining_items = []\n",
        "        dining_regions = {}\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [40, 41, 42, 43, 44, 45, 60]:  # Wine glass, cup, fork, knife, spoon, bowl, table\n",
        "                region = obj[\"region\"]\n",
        "                if region not in dining_regions:\n",
        "                    dining_regions[region] = []\n",
        "                dining_regions[region].append(obj)\n",
        "                dining_items.append(obj[\"class_name\"])\n",
        "\n",
        "        if dining_items:\n",
        "            main_dining_region = max(dining_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_dining_region[0] is not None:\n",
        "                zones[\"formal_dining_zone\"] = {\n",
        "                    \"region\": main_dining_region[0],\n",
        "                    \"objects\": list(set(dining_items)),\n",
        "                    \"description\": f\"Formal dining area with {', '.join(list(set(dining_items))[:3])}\"\n",
        "                }\n",
        "\n",
        "        # Identify decorative zone with enhanced detection\n",
        "        decor_items = []\n",
        "        decor_regions = {}\n",
        "\n",
        "        # Look for decorative elements (vases, wine glasses, unused dishes)\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [75, 40]:  # Vase, wine glass\n",
        "                region = obj[\"region\"]\n",
        "                if region not in decor_regions:\n",
        "                    decor_regions[region] = []\n",
        "                decor_regions[region].append(obj)\n",
        "                decor_items.append(obj[\"class_name\"])\n",
        "\n",
        "        if decor_items:\n",
        "            main_decor_region = max(decor_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_decor_region[0] is not None:\n",
        "                zones[\"decorative_zone\"] = {\n",
        "                    \"region\": main_decor_region[0],\n",
        "                    \"objects\": list(set(decor_items)),\n",
        "                    \"description\": f\"Decorative area with {', '.join(list(set(decor_items)))}\"\n",
        "                }\n",
        "\n",
        "        # Identify seating arrangement zone\n",
        "        chairs = [obj for obj in detected_objects if obj[\"class_id\"] == 56]  # chairs\n",
        "        if len(chairs) >= 2:\n",
        "            chair_regions = {}\n",
        "            for obj in chairs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in chair_regions:\n",
        "                    chair_regions[region] = []\n",
        "                chair_regions[region].append(obj)\n",
        "\n",
        "            if chair_regions:\n",
        "                main_seating_region = max(chair_regions.items(),\n",
        "                                    key=lambda x: len(x[1]),\n",
        "                                    default=(None, []))\n",
        "\n",
        "                if main_seating_region[0] is not None:\n",
        "                    zones[\"dining_seating_zone\"] = {\n",
        "                        \"region\": main_seating_region[0],\n",
        "                        \"objects\": [\"chair\"] * len(main_seating_region[1]),\n",
        "                        \"description\": f\"Formal dining seating arrangement with {len(main_seating_region[1])} chairs\"\n",
        "                    }\n",
        "\n",
        "        # Identify serving area (if different from dining area)\n",
        "        serving_items = []\n",
        "        serving_regions = {}\n",
        "\n",
        "        # Serving areas might have bottles, bowls, containers\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [39, 45]:  # Bottle, bowl\n",
        "                # Check if it's in a different region from the main dining table\n",
        "                if \"formal_dining_zone\" in zones and obj[\"region\"] != zones[\"formal_dining_zone\"][\"region\"]:\n",
        "                    region = obj[\"region\"]\n",
        "                    if region not in serving_regions:\n",
        "                        serving_regions[region] = []\n",
        "                    serving_regions[region].append(obj)\n",
        "                    serving_items.append(obj[\"class_name\"])\n",
        "\n",
        "        if serving_items:\n",
        "            main_serving_region = max(serving_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_serving_region[0] is not None:\n",
        "                zones[\"serving_zone\"] = {\n",
        "                    \"region\": main_serving_region[0],\n",
        "                    \"objects\": list(set(serving_items)),\n",
        "                    \"description\": f\"Serving or sideboard area with {', '.join(list(set(serving_items)))}\"\n",
        "                }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_financial_district_zones(self, category_regions: Dict, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for financial district scenes.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            Dict: Financial district functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Identify traffic zone\n",
        "        traffic_items = []\n",
        "        traffic_regions = {}\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [1, 2, 3, 5, 6, 7, 9]:  # Various vehicles and traffic lights\n",
        "                region = obj[\"region\"]\n",
        "                if region not in traffic_regions:\n",
        "                    traffic_regions[region] = []\n",
        "                traffic_regions[region].append(obj)\n",
        "                traffic_items.append(obj[\"class_name\"])\n",
        "\n",
        "        if traffic_items:\n",
        "            main_traffic_region = max(traffic_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "            if main_traffic_region[0] is not None:\n",
        "                zones[\"traffic_zone\"] = {\n",
        "                    \"region\": main_traffic_region[0],\n",
        "                    \"objects\": list(set(traffic_items)),\n",
        "                    \"description\": f\"Urban traffic area with {', '.join(list(set(traffic_items))[:3])}\"\n",
        "                }\n",
        "\n",
        "        # Building zones on the sides (inferred from scene context)\n",
        "        # Enhanced to check if there are actual regions that might contain buildings\n",
        "        # Check for regions without vehicles or pedestrians - likely building areas\n",
        "        left_side_regions = [\"top_left\", \"middle_left\", \"bottom_left\"]\n",
        "        right_side_regions = [\"top_right\", \"middle_right\", \"bottom_right\"]\n",
        "\n",
        "        # Check left side\n",
        "        left_building_evidence = True\n",
        "        for region in left_side_regions:\n",
        "            # If many vehicles or people in this region, less likely to be buildings\n",
        "            vehicle_in_region = any(obj[\"region\"] == region and obj[\"class_id\"] in [1, 2, 3, 5, 7]\n",
        "                                for obj in detected_objects)\n",
        "            people_in_region = any(obj[\"region\"] == region and obj[\"class_id\"] == 0\n",
        "                                for obj in detected_objects)\n",
        "\n",
        "            if vehicle_in_region or people_in_region:\n",
        "                left_building_evidence = False\n",
        "                break\n",
        "\n",
        "        # Check right side\n",
        "        right_building_evidence = True\n",
        "        for region in right_side_regions:\n",
        "            # If many vehicles or people in this region, less likely to be buildings\n",
        "            vehicle_in_region = any(obj[\"region\"] == region and obj[\"class_id\"] in [1, 2, 3, 5, 7]\n",
        "                                for obj in detected_objects)\n",
        "            people_in_region = any(obj[\"region\"] == region and obj[\"class_id\"] == 0\n",
        "                                for obj in detected_objects)\n",
        "\n",
        "            if vehicle_in_region or people_in_region:\n",
        "                right_building_evidence = False\n",
        "                break\n",
        "\n",
        "        # Add building zones if evidence supports them\n",
        "        if left_building_evidence:\n",
        "            zones[\"building_zone_left\"] = {\n",
        "                \"region\": \"middle_left\",\n",
        "                \"objects\": [\"building\"],  # Inferred\n",
        "                \"description\": \"Tall buildings line the left side of the street\"\n",
        "            }\n",
        "\n",
        "        if right_building_evidence:\n",
        "            zones[\"building_zone_right\"] = {\n",
        "                \"region\": \"middle_right\",\n",
        "                \"objects\": [\"building\"],  # Inferred\n",
        "                \"description\": \"Tall buildings line the right side of the street\"\n",
        "            }\n",
        "\n",
        "        # Identify pedestrian zone if people are present\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "        if people_objs:\n",
        "            people_regions = {}\n",
        "            for obj in people_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in people_regions:\n",
        "                    people_regions[region] = []\n",
        "                people_regions[region].append(obj)\n",
        "\n",
        "            if people_regions:\n",
        "                main_pedestrian_region = max(people_regions.items(),\n",
        "                                        key=lambda x: len(x[1]),\n",
        "                                        default=(None, []))\n",
        "\n",
        "                if main_pedestrian_region[0] is not None:\n",
        "                    zones[\"pedestrian_zone\"] = {\n",
        "                        \"region\": main_pedestrian_region[0],\n",
        "                        \"objects\": [\"person\"] * len(main_pedestrian_region[1]),\n",
        "                        \"description\": f\"Pedestrian area with {len(main_pedestrian_region[1])} people navigating the financial district\"\n",
        "                    }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_aerial_view_zones(self, category_regions: Dict, detected_objects: List[Dict], scene_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for scenes viewed from an aerial perspective.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Specific scene type\n",
        "\n",
        "        Returns:\n",
        "            Dict: Aerial view functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # For aerial views, we focus on patterns and flows rather than specific zones\n",
        "\n",
        "        # Identify pedestrian patterns\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "        if people_objs:\n",
        "            # Convert positions to arrays for pattern analysis\n",
        "            positions = np.array([obj[\"normalized_center\"] for obj in people_objs])\n",
        "\n",
        "            if len(positions) >= 3:\n",
        "                # Calculate distribution metrics\n",
        "                x_coords = positions[:, 0]\n",
        "                y_coords = positions[:, 1]\n",
        "\n",
        "                x_mean = np.mean(x_coords)\n",
        "                y_mean = np.mean(y_coords)\n",
        "                x_std = np.std(x_coords)\n",
        "                y_std = np.std(y_coords)\n",
        "\n",
        "                # Determine if people are organized in a linear pattern\n",
        "                if x_std < 0.1 or y_std < 0.1:\n",
        "                    # Linear distribution along one axis\n",
        "                    pattern_direction = \"vertical\" if x_std < y_std else \"horizontal\"\n",
        "\n",
        "                    zones[\"pedestrian_pattern\"] = {\n",
        "                        \"region\": \"central\",\n",
        "                        \"objects\": [\"person\"] * len(people_objs),\n",
        "                        \"description\": f\"Aerial view shows a {pattern_direction} pedestrian movement pattern\"\n",
        "                    }\n",
        "                else:\n",
        "                    # More dispersed pattern\n",
        "                    zones[\"pedestrian_distribution\"] = {\n",
        "                        \"region\": \"wide\",\n",
        "                        \"objects\": [\"person\"] * len(people_objs),\n",
        "                        \"description\": f\"Aerial view shows pedestrians distributed across the area\"\n",
        "                    }\n",
        "\n",
        "        # Identify vehicle patterns for traffic analysis\n",
        "        vehicle_objs = [obj for obj in detected_objects if obj[\"class_id\"] in [1, 2, 3, 5, 6, 7]]\n",
        "        if vehicle_objs:\n",
        "            # Convert positions to arrays for pattern analysis\n",
        "            positions = np.array([obj[\"normalized_center\"] for obj in vehicle_objs])\n",
        "\n",
        "            if len(positions) >= 2:\n",
        "                # Calculate distribution metrics\n",
        "                x_coords = positions[:, 0]\n",
        "                y_coords = positions[:, 1]\n",
        "\n",
        "                x_mean = np.mean(x_coords)\n",
        "                y_mean = np.mean(y_coords)\n",
        "                x_std = np.std(x_coords)\n",
        "                y_std = np.std(y_coords)\n",
        "\n",
        "                # Determine if vehicles are organized in lanes\n",
        "                if x_std < y_std * 0.5:\n",
        "                    # Vehicles aligned vertically - indicates north-south traffic\n",
        "                    zones[\"vertical_traffic_flow\"] = {\n",
        "                        \"region\": \"central_vertical\",\n",
        "                        \"objects\": [obj[\"class_name\"] for obj in vehicle_objs[:5]],\n",
        "                        \"description\": \"North-south traffic flow visible from aerial view\"\n",
        "                    }\n",
        "                elif y_std < x_std * 0.5:\n",
        "                    # Vehicles aligned horizontally - indicates east-west traffic\n",
        "                    zones[\"horizontal_traffic_flow\"] = {\n",
        "                        \"region\": \"central_horizontal\",\n",
        "                        \"objects\": [obj[\"class_name\"] for obj in vehicle_objs[:5]],\n",
        "                        \"description\": \"East-west traffic flow visible from aerial view\"\n",
        "                    }\n",
        "                else:\n",
        "                    # Vehicles in multiple directions - indicates intersection\n",
        "                    zones[\"intersection_traffic\"] = {\n",
        "                        \"region\": \"central\",\n",
        "                        \"objects\": [obj[\"class_name\"] for obj in vehicle_objs[:5]],\n",
        "                        \"description\": \"Multi-directional traffic at intersection visible from aerial view\"\n",
        "                    }\n",
        "\n",
        "        # For intersection specific aerial views, identify crossing patterns\n",
        "        if \"intersection\" in scene_type:\n",
        "            # Check for traffic signals\n",
        "            traffic_light_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 9]\n",
        "            if traffic_light_objs:\n",
        "                zones[\"traffic_control_pattern\"] = {\n",
        "                    \"region\": \"intersection\",\n",
        "                    \"objects\": [\"traffic light\"] * len(traffic_light_objs),\n",
        "                    \"description\": f\"Intersection traffic control with {len(traffic_light_objs)} signals visible from above\"\n",
        "                }\n",
        "\n",
        "            # Crosswalks are inferred from context in aerial views\n",
        "            zones[\"crossing_pattern\"] = {\n",
        "                \"region\": \"central\",\n",
        "                \"objects\": [\"inferred crosswalk\"],\n",
        "                \"description\": \"Crossing pattern visible from aerial perspective\"\n",
        "            }\n",
        "\n",
        "        # For plaza aerial views, identify gathering patterns\n",
        "        if \"plaza\" in scene_type:\n",
        "            # Plazas typically have central open area with people\n",
        "            if people_objs:\n",
        "                # Check if people are clustered in central region\n",
        "                central_people = [obj for obj in people_objs\n",
        "                                if \"middle\" in obj[\"region\"]]\n",
        "\n",
        "                if central_people:\n",
        "                    zones[\"central_gathering\"] = {\n",
        "                        \"region\": \"middle_center\",\n",
        "                        \"objects\": [\"person\"] * len(central_people),\n",
        "                        \"description\": f\"Central plaza gathering area with {len(central_people)} people viewed from above\"\n",
        "                    }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_outdoor_general_zones(self, category_regions: Dict, detected_objects: List[Dict], scene_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify functional zones for general outdoor scenes.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Specific outdoor scene type\n",
        "\n",
        "        Returns:\n",
        "            Dict: Outdoor functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Identify pedestrian zones\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "        if people_objs:\n",
        "            people_regions = {}\n",
        "            for obj in people_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in people_regions:\n",
        "                    people_regions[region] = []\n",
        "                people_regions[region].append(obj)\n",
        "\n",
        "            if people_regions:\n",
        "                # Find main pedestrian areas\n",
        "                main_people_regions = sorted(people_regions.items(),\n",
        "                                        key=lambda x: len(x[1]),\n",
        "                                        reverse=True)[:2]  # Top 2 regions\n",
        "\n",
        "                for idx, (region, objs) in enumerate(main_people_regions):\n",
        "                    if len(objs) > 0:\n",
        "                        zones[f\"pedestrian_zone_{idx+1}\"] = {\n",
        "                            \"region\": region,\n",
        "                            \"objects\": [\"person\"] * len(objs),\n",
        "                            \"description\": f\"Pedestrian area with {len(objs)} {'people' if len(objs) > 1 else 'person'}\"\n",
        "                        }\n",
        "\n",
        "        # Identify vehicle zones for streets and parking lots\n",
        "        vehicle_objs = [obj for obj in detected_objects if obj[\"class_id\"] in [1, 2, 3, 5, 6, 7]]\n",
        "        if vehicle_objs:\n",
        "            vehicle_regions = {}\n",
        "            for obj in vehicle_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in vehicle_regions:\n",
        "                    vehicle_regions[region] = []\n",
        "                vehicle_regions[region].append(obj)\n",
        "\n",
        "            if vehicle_regions:\n",
        "                main_vehicle_region = max(vehicle_regions.items(),\n",
        "                                    key=lambda x: len(x[1]),\n",
        "                                    default=(None, []))\n",
        "\n",
        "                if main_vehicle_region[0] is not None:\n",
        "                    vehicle_types = [obj[\"class_name\"] for obj in main_vehicle_region[1]]\n",
        "                    zones[\"vehicle_zone\"] = {\n",
        "                        \"region\": main_vehicle_region[0],\n",
        "                        \"objects\": vehicle_types,\n",
        "                        \"description\": f\"Traffic area with {', '.join(list(set(vehicle_types))[:3])}\"\n",
        "                    }\n",
        "\n",
        "        # For park areas, identify recreational zones\n",
        "        if scene_type == \"park_area\":\n",
        "            # Look for recreational objects (sports balls, kites, etc.)\n",
        "            rec_items = []\n",
        "            rec_regions = {}\n",
        "\n",
        "            for obj in detected_objects:\n",
        "                if obj[\"class_id\"] in [32, 33, 34, 35, 38]:  # sports ball, kite, baseball bat, glove, tennis racket\n",
        "                    region = obj[\"region\"]\n",
        "                    if region not in rec_regions:\n",
        "                        rec_regions[region] = []\n",
        "                    rec_regions[region].append(obj)\n",
        "                    rec_items.append(obj[\"class_name\"])\n",
        "\n",
        "            if rec_items:\n",
        "                main_rec_region = max(rec_regions.items(),\n",
        "                                key=lambda x: len(x[1]),\n",
        "                                default=(None, []))\n",
        "\n",
        "                if main_rec_region[0] is not None:\n",
        "                    zones[\"recreational_zone\"] = {\n",
        "                        \"region\": main_rec_region[0],\n",
        "                        \"objects\": list(set(rec_items)),\n",
        "                        \"description\": f\"Recreational area with {', '.join(list(set(rec_items)))}\"\n",
        "                    }\n",
        "\n",
        "        # For parking lots, identify parking zones\n",
        "        if scene_type == \"parking_lot\":\n",
        "            # Look for parked cars with consistent spacing\n",
        "            car_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 2]  # cars\n",
        "\n",
        "            if len(car_objs) >= 3:\n",
        "                # Check if cars are arranged in patterns (simplified)\n",
        "                car_positions = [obj[\"normalized_center\"] for obj in car_objs]\n",
        "\n",
        "                # Check for row patterns by analyzing vertical positions\n",
        "                y_coords = [pos[1] for pos in car_positions]\n",
        "                y_clusters = {}\n",
        "\n",
        "                # Simplified clustering - group cars by similar y-coordinates\n",
        "                for i, y in enumerate(y_coords):\n",
        "                    assigned = False\n",
        "                    for cluster_y in y_clusters.keys():\n",
        "                        if abs(y - cluster_y) < 0.1:  # Within 10% of image height\n",
        "                            y_clusters[cluster_y].append(i)\n",
        "                            assigned = True\n",
        "                            break\n",
        "\n",
        "                    if not assigned:\n",
        "                        y_clusters[y] = [i]\n",
        "\n",
        "                # If we have row patterns\n",
        "                if max(len(indices) for indices in y_clusters.values()) >= 2:\n",
        "                    zones[\"parking_row\"] = {\n",
        "                        \"region\": \"central\",\n",
        "                        \"objects\": [\"car\"] * len(car_objs),\n",
        "                        \"description\": f\"Organized parking area with vehicles arranged in rows\"\n",
        "                    }\n",
        "                else:\n",
        "                    zones[\"parking_area\"] = {\n",
        "                        \"region\": \"wide\",\n",
        "                        \"objects\": [\"car\"] * len(car_objs),\n",
        "                        \"description\": f\"Parking area with {len(car_objs)} vehicles\"\n",
        "                    }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _identify_default_zones(self, category_regions: Dict, detected_objects: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Identify general functional zones when no specific scene type is matched.\n",
        "\n",
        "        Args:\n",
        "            category_regions: Objects grouped by category and region\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            Dict: Default functional zones\n",
        "        \"\"\"\n",
        "        zones = {}\n",
        "\n",
        "        # Group objects by category and find main concentrations\n",
        "        for category, regions in category_regions.items():\n",
        "            if not regions:\n",
        "                continue\n",
        "\n",
        "            # Find region with most objects in this category\n",
        "            main_region = max(regions.items(),\n",
        "                        key=lambda x: len(x[1]),\n",
        "                        default=(None, []))\n",
        "\n",
        "            if main_region[0] is None or len(main_region[1]) < 2:\n",
        "                continue\n",
        "\n",
        "            # Create zone based on object category\n",
        "            zone_objects = [obj[\"class_name\"] for obj in main_region[1]]\n",
        "\n",
        "            # Skip if too few objects\n",
        "            if len(zone_objects) < 2:\n",
        "                continue\n",
        "\n",
        "            # Create appropriate zone name and description based on category\n",
        "            if category == \"furniture\":\n",
        "                zones[\"furniture_zone\"] = {\n",
        "                    \"region\": main_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Area with furniture including {', '.join(zone_objects[:3])}\"\n",
        "                }\n",
        "            elif category == \"electronics\":\n",
        "                zones[\"electronics_zone\"] = {\n",
        "                    \"region\": main_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Area with electronic devices including {', '.join(zone_objects[:3])}\"\n",
        "                }\n",
        "            elif category == \"kitchen_items\":\n",
        "                zones[\"dining_zone\"] = {\n",
        "                    \"region\": main_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Dining or food area with {', '.join(zone_objects[:3])}\"\n",
        "                }\n",
        "            elif category == \"vehicles\":\n",
        "                zones[\"vehicle_zone\"] = {\n",
        "                    \"region\": main_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Area with vehicles including {', '.join(zone_objects[:3])}\"\n",
        "                }\n",
        "            elif category == \"personal_items\":\n",
        "                zones[\"personal_items_zone\"] = {\n",
        "                    \"region\": main_region[0],\n",
        "                    \"objects\": zone_objects,\n",
        "                    \"description\": f\"Area with personal items including {', '.join(zone_objects[:3])}\"\n",
        "                }\n",
        "\n",
        "        # Check for people groups\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "        if len(people_objs) >= 2:\n",
        "            people_regions = {}\n",
        "            for obj in people_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in people_regions:\n",
        "                    people_regions[region] = []\n",
        "                people_regions[region].append(obj)\n",
        "\n",
        "            if people_regions:\n",
        "                main_people_region = max(people_regions.items(),\n",
        "                                    key=lambda x: len(x[1]),\n",
        "                                    default=(None, []))\n",
        "\n",
        "                if main_people_region[0] is not None:\n",
        "                    zones[\"people_zone\"] = {\n",
        "                        \"region\": main_people_region[0],\n",
        "                        \"objects\": [\"person\"] * len(main_people_region[1]),\n",
        "                        \"description\": f\"Area with {len(main_people_region[1])} people\"\n",
        "                    }\n",
        "\n",
        "        return zones\n",
        "\n",
        "    def _find_main_region(self, region_objects_dict: Dict) -> str:\n",
        "        \"\"\"Find the main region with the most objects\"\"\"\n",
        "        if not region_objects_dict:\n",
        "            return \"unknown\"\n",
        "\n",
        "        return max(region_objects_dict.items(),\n",
        "                key=lambda x: len(x[1]),\n",
        "                default=(\"unknown\", []))[0]\n",
        "\n",
        "    def _find_main_region(self, region_objects_dict: Dict) -> str:\n",
        "        \"\"\"Find the main region with the most objects\"\"\"\n",
        "        if not region_objects_dict:\n",
        "            return \"unknown\"\n",
        "\n",
        "        return max(region_objects_dict.items(),\n",
        "                 key=lambda x: len(x[1]),\n",
        "                 default=(\"unknown\", []))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atHycbc0-3_6"
      },
      "outputs": [],
      "source": [
        "# %%writefile enhance_scene_describer.py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "# from scene_type import SCENE_TYPES\n",
        "# from scene_detail_templates import SCENE_DETAIL_TEMPLATES\n",
        "# from object_template_fillers import OBJECT_TEMPLATE_FILLERS\n",
        "# from lighting_conditions import LIGHTING_CONDITIONS\n",
        "# from viewpoint_templates import VIEWPOINT_TEMPLATES\n",
        "# from cultural_templates import CULTURAL_TEMPLATES\n",
        "# from confifence_templates import CONFIDENCE_TEMPLATES\n",
        "\n",
        "class EnhancedSceneDescriber:\n",
        "    \"\"\"\n",
        "    Enhanced scene description generator with improved template handling,\n",
        "    viewpoint awareness, and cultural context recognition.\n",
        "    Provides detailed natural language descriptions of scenes based on\n",
        "    detection results and scene classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, templates_db: Optional[Dict] = None, scene_types: Optional[Dict] = None):\n",
        "        \"\"\"\n",
        "        Initialize the enhanced scene describer.\n",
        "\n",
        "        Args:\n",
        "            templates_db: Optional custom templates database\n",
        "            scene_types: Dictionary of scene type definitions\n",
        "        \"\"\"\n",
        "        # Load or use provided scene types\n",
        "        self.scene_types = scene_types or self._load_default_scene_types()\n",
        "\n",
        "        # Load templates database\n",
        "        self.templates = templates_db or self._load_templates()\n",
        "\n",
        "        # Initialize viewpoint detection parameters\n",
        "        self._initialize_viewpoint_parameters()\n",
        "\n",
        "    def _load_default_scene_types(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Load default scene types.\n",
        "\n",
        "        Returns:\n",
        "            Dict: Scene type definitions\n",
        "        \"\"\"\n",
        "\n",
        "        return SCENE_TYPES\n",
        "\n",
        "    def _load_templates(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Load description templates from imported Python modules.\n",
        "\n",
        "        Returns:\n",
        "            Dict: Template collections for different description components\n",
        "        \"\"\"\n",
        "        templates = {}\n",
        "\n",
        "        # 直接從導入的 Python 模組中獲取模板\n",
        "        templates[\"scene_detail_templates\"] = SCENE_DETAIL_TEMPLATES\n",
        "        templates[\"object_template_fillers\"] = OBJECT_TEMPLATE_FILLERS\n",
        "        templates[\"viewpoint_templates\"] = VIEWPOINT_TEMPLATES\n",
        "        templates[\"cultural_templates\"] = CULTURAL_TEMPLATES\n",
        "\n",
        "        # 從 LIGHTING_CONDITIONS 獲取照明模板\n",
        "        templates[\"lighting_templates\"] = {\n",
        "            key: data[\"general\"] for key, data in LIGHTING_CONDITIONS.get(\"time_descriptions\", {}).items()\n",
        "        }\n",
        "\n",
        "        # 設置默認的置信度模板\n",
        "        templates[\"confidence_templates\"] = {\n",
        "            \"high\": \"{description} {details}\",\n",
        "            \"medium\": \"This appears to be {description} {details}\",\n",
        "            \"low\": \"This might be {description}, but the confidence is low. {details}\"\n",
        "        }\n",
        "\n",
        "        # 初始化其他必要的模板（現在這個函數簡化了很多）\n",
        "        self._initialize_default_templates(templates)\n",
        "\n",
        "        return templates\n",
        "\n",
        "    def _initialize_default_templates(self, templates: Dict):\n",
        "        \"\"\"\n",
        "        檢查模板字典並填充任何缺失的默認模板。\n",
        "\n",
        "        在將模板移至專門的模組後，此方法主要作為安全機制，\n",
        "        確保即使導入失敗或某些模板未在外部定義，系統仍能正常運行。\n",
        "\n",
        "        Args:\n",
        "            templates: 要檢查和更新的模板字典\n",
        "        \"\"\"\n",
        "        # 檢查關鍵模板類型是否存在，如果不存在則添加默認值\n",
        "\n",
        "        # 置信度模板 - 用於控制描述的語氣\n",
        "        if \"confidence_templates\" not in templates:\n",
        "            templates[\"confidence_templates\"] = {\n",
        "                \"high\": \"{description} {details}\",\n",
        "                \"medium\": \"This appears to be {description} {details}\",\n",
        "                \"low\": \"This might be {description}, but the confidence is low. {details}\"\n",
        "            }\n",
        "\n",
        "        # 場景細節模板 - 如果未從外部導入\n",
        "        if \"scene_detail_templates\" not in templates:\n",
        "            templates[\"scene_detail_templates\"] = {\n",
        "                \"default\": [\"A space with various objects.\"]\n",
        "            }\n",
        "\n",
        "        # 物體填充模板 - 用於生成物體描述\n",
        "        if \"object_template_fillers\" not in templates:\n",
        "            templates[\"object_template_fillers\"] = {\n",
        "                \"default\": [\"various items\"]\n",
        "            }\n",
        "\n",
        "        # 視角模板 - 雖然我們現在從專門模組導入，但作為備份\n",
        "        if \"viewpoint_templates\" not in templates:\n",
        "            # 使用簡化版的默認視角模板\n",
        "            templates[\"viewpoint_templates\"] = {\n",
        "                \"eye_level\": {\n",
        "                    \"prefix\": \"From eye level, \",\n",
        "                    \"observation\": \"the scene is viewed straight on.\"\n",
        "                },\n",
        "                \"aerial\": {\n",
        "                    \"prefix\": \"From above, \",\n",
        "                    \"observation\": \"the scene is viewed from a bird's-eye perspective.\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # 文化模板\n",
        "        if \"cultural_templates\" not in templates:\n",
        "            templates[\"cultural_templates\"] = {\n",
        "                \"asian\": {\n",
        "                    \"elements\": [\"cultural elements\"],\n",
        "                    \"description\": \"The scene has Asian characteristics.\"\n",
        "                },\n",
        "                \"european\": {\n",
        "                    \"elements\": [\"architectural features\"],\n",
        "                    \"description\": \"The scene has European characteristics.\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # 照明模板 - 用於描述光照條件\n",
        "        if \"lighting_templates\" not in templates:\n",
        "            templates[\"lighting_templates\"] = {\n",
        "                \"day_clear\": \"The scene is captured during daylight.\",\n",
        "                \"night\": \"The scene is captured at night.\",\n",
        "                \"unknown\": \"The lighting conditions are not easily determined.\"\n",
        "            }\n",
        "\n",
        "    def _initialize_viewpoint_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters used for viewpoint detection.\n",
        "        \"\"\"\n",
        "        self.viewpoint_params = {\n",
        "            # Parameters for detecting aerial views\n",
        "            \"aerial_threshold\": 0.7,  # High object density viewed from top\n",
        "            \"aerial_size_variance_threshold\": 0.15,  # Low size variance in aerial views\n",
        "\n",
        "            # Parameters for detecting low angle views\n",
        "            \"low_angle_threshold\": 0.3,  # Bottom-heavy object distribution\n",
        "            \"vertical_size_ratio_threshold\": 1.8,  # Vertical objects appear taller\n",
        "\n",
        "            # Parameters for detecting elevated views\n",
        "            \"elevated_threshold\": 0.6,  # Objects mostly in middle/bottom\n",
        "            \"elevated_top_threshold\": 0.3  # Few objects at top of frame\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_description(self,\n",
        "                        scene_type: str,\n",
        "                        detected_objects: List[Dict],\n",
        "                        confidence: float,\n",
        "                        lighting_info: Optional[Dict] = None,\n",
        "                        functional_zones: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate enhanced scene description based on detection results, scene type,\n",
        "        and additional contextual information.\n",
        "\n",
        "        This is the main entry point that replaces the original _generate_scene_description.\n",
        "\n",
        "        Args:\n",
        "            scene_type: Identified scene type\n",
        "            detected_objects: List of detected objects\n",
        "            confidence: Scene classification confidence\n",
        "            lighting_info: Optional lighting condition information\n",
        "            functional_zones: Optional identified functional zones\n",
        "\n",
        "        Returns:\n",
        "            str: Natural language description of the scene\n",
        "        \"\"\"\n",
        "        # Handle unknown scene type or very low confidence\n",
        "        if scene_type == \"unknown\" or confidence < 0.4:\n",
        "            return self._format_final_description(self._generate_generic_description(detected_objects, lighting_info))\n",
        "\n",
        "        # Detect viewpoint\n",
        "        viewpoint = self._detect_viewpoint(detected_objects)\n",
        "\n",
        "        # Process aerial viewpoint scene types\n",
        "        if viewpoint == \"aerial\":\n",
        "            if \"intersection\" in scene_type or self._is_intersection(detected_objects):\n",
        "                scene_type = \"aerial_view_intersection\"\n",
        "            elif any(keyword in scene_type for keyword in [\"commercial\", \"shopping\", \"retail\"]):\n",
        "                scene_type = \"aerial_view_commercial_area\"\n",
        "            elif any(keyword in scene_type for keyword in [\"plaza\", \"square\"]):\n",
        "                scene_type = \"aerial_view_plaza\"\n",
        "            else:\n",
        "                scene_type = \"aerial_view_intersection\"\n",
        "\n",
        "        # Detect cultural context - only for non-aerial viewpoints\n",
        "        cultural_context = None\n",
        "        if viewpoint != \"aerial\":\n",
        "            cultural_context = self._detect_cultural_context(scene_type, detected_objects)\n",
        "\n",
        "        # Select appropriate template based on confidence\n",
        "        if confidence > 0.75:\n",
        "            confidence_level = \"high\"\n",
        "        elif confidence > 0.5:\n",
        "            confidence_level = \"medium\"\n",
        "        else:\n",
        "            confidence_level = \"low\"\n",
        "\n",
        "        # Get base description for the scene type\n",
        "        if viewpoint == \"aerial\":\n",
        "            if 'base_description' not in locals():\n",
        "                base_description = \"An aerial view showing the layout and movement patterns from above\"\n",
        "        elif scene_type in self.scene_types:\n",
        "            base_description = self.scene_types[scene_type].get(\"description\", \"A scene\")\n",
        "        else:\n",
        "            base_description = \"A scene\"\n",
        "\n",
        "        # Generate detailed scene information\n",
        "        scene_details = self._generate_scene_details(\n",
        "            scene_type,\n",
        "            detected_objects,\n",
        "            lighting_info,\n",
        "            viewpoint\n",
        "        )\n",
        "\n",
        "        # Start with the base description\n",
        "        description = base_description\n",
        "\n",
        "        # If there's a secondary description from the scene type template, append it properly\n",
        "        if scene_type in self.scene_types and \"secondary_description\" in self.scene_types[scene_type]:\n",
        "            secondary_desc = self.scene_types[scene_type][\"secondary_description\"]\n",
        "            if secondary_desc:\n",
        "                description = self._smart_append(description, secondary_desc)\n",
        "\n",
        "        # Improve description based on people count\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]  # Person class\n",
        "        if people_objs:\n",
        "            people_count = len(people_objs)\n",
        "            if people_count > 5:\n",
        "                people_phrase = f\"numerous people ({people_count})\"\n",
        "            else:\n",
        "                people_phrase = f\"{people_count} {'people' if people_count > 1 else 'person'}\"\n",
        "\n",
        "            # Add people information to the scene details if not already mentioned\n",
        "            if \"people\" not in description.lower() and \"pedestrian\" not in description.lower():\n",
        "                description = self._smart_append(description, f\"The scene includes {people_phrase}\")\n",
        "\n",
        "        # Apply cultural context if detected (only for non-aerial viewpoints)\n",
        "        if cultural_context and viewpoint != \"aerial\":\n",
        "            cultural_elements = self._generate_cultural_elements(cultural_context)\n",
        "            if cultural_elements:\n",
        "                description = self._smart_append(description, cultural_elements)\n",
        "\n",
        "        # Now append the detailed scene information if available\n",
        "        if scene_details:\n",
        "            # Use smart_append to ensure proper formatting between base description and details\n",
        "            description = self._smart_append(description, scene_details)\n",
        "\n",
        "        # Include lighting information if available\n",
        "        lighting_description = \"\"\n",
        "        if lighting_info and \"time_of_day\" in lighting_info:\n",
        "            lighting_type = lighting_info[\"time_of_day\"]\n",
        "            if lighting_type in self.templates.get(\"lighting_templates\", {}):\n",
        "                lighting_description = self.templates[\"lighting_templates\"][lighting_type]\n",
        "\n",
        "        # Add lighting description if available\n",
        "        if lighting_description and lighting_description not in description:\n",
        "            description = self._smart_append(description, lighting_description)\n",
        "\n",
        "        # Process viewpoint information\n",
        "        if viewpoint != \"eye_level\" and viewpoint in self.templates.get(\"viewpoint_templates\", {}):\n",
        "            viewpoint_template = self.templates[\"viewpoint_templates\"][viewpoint]\n",
        "\n",
        "            # Special handling for viewpoint prefix\n",
        "            prefix = viewpoint_template.get('prefix', '')\n",
        "            if prefix and not description.startswith(prefix):\n",
        "                # Prefix is a phrase like \"From above, \" that should precede the description\n",
        "                if description and description[0].isupper():\n",
        "                    # Maintain the flow by lowercasing the first letter after the prefix\n",
        "                    description = prefix + description[0].lower() + description[1:]\n",
        "                else:\n",
        "                    description = prefix + description\n",
        "\n",
        "            # Get appropriate scene elements description based on viewpoint\n",
        "            if viewpoint == \"aerial\":\n",
        "                scene_elements = \"the crossing patterns and pedestrian movement\"\n",
        "            else:\n",
        "                scene_elements = \"objects and layout\"\n",
        "\n",
        "            viewpoint_desc = viewpoint_template.get(\"observation\", \"\").format(\n",
        "                scene_elements=scene_elements\n",
        "            )\n",
        "\n",
        "            # Add viewpoint observation if not already included\n",
        "            if viewpoint_desc and viewpoint_desc not in description:\n",
        "                description = self._smart_append(description, viewpoint_desc)\n",
        "\n",
        "        # Add information about functional zones if available\n",
        "        if functional_zones and len(functional_zones) > 0:\n",
        "            zones_desc = self._describe_functional_zones(functional_zones)\n",
        "            if zones_desc:\n",
        "                description = self._smart_append(description, zones_desc)\n",
        "\n",
        "        # Calculate actual people count\n",
        "        people_count = len([obj for obj in detected_objects if obj[\"class_id\"] == 0])\n",
        "\n",
        "        # Check for inconsistencies in people count descriptions\n",
        "        if people_count > 5:\n",
        "            # Identify fragments that might contain smaller people counts\n",
        "            small_people_patterns = [\n",
        "                r\"Area with \\d+ people\\.\",\n",
        "                r\"Area with \\d+ person\\.\",\n",
        "                r\"with \\d+ people\",\n",
        "                r\"with \\d+ person\"\n",
        "            ]\n",
        "\n",
        "            # Check and remove each pattern\n",
        "            filtered_description = description\n",
        "            for pattern in small_people_patterns:\n",
        "                matches = re.findall(pattern, filtered_description)\n",
        "                for match in matches:\n",
        "                    # Extract the number from the match\n",
        "                    number_match = re.search(r'\\d+', match)\n",
        "                    if number_match:\n",
        "                        try:\n",
        "                            people_mentioned = int(number_match.group())\n",
        "                            # If the mentioned count is less than total, remove the entire sentence\n",
        "                            if people_mentioned < people_count:\n",
        "                                # Split description into sentences\n",
        "                                sentences = re.split(r'(?<=[.!?])\\s+', filtered_description)\n",
        "                                # Remove sentences containing the match\n",
        "                                filtered_sentences = []\n",
        "                                for sentence in sentences:\n",
        "                                    if match not in sentence:\n",
        "                                        filtered_sentences.append(sentence)\n",
        "                                # Recombine the description\n",
        "                                filtered_description = \" \".join(filtered_sentences)\n",
        "                        except ValueError:\n",
        "                            # Failed number conversion, continue processing\n",
        "                            continue\n",
        "\n",
        "            # Use the filtered description\n",
        "            description = filtered_description\n",
        "\n",
        "        # Final formatting to ensure correct punctuation and capitalization\n",
        "        description = self._format_final_description(description)\n",
        "\n",
        "        description_lines = description.split('\\n')\n",
        "        clean_description = []\n",
        "        skip_block = False  # 添加這個變數的定義\n",
        "\n",
        "        for line in description_lines:\n",
        "            # 檢查是否需要跳過這行\n",
        "            if line.strip().startswith(':param') or line.strip().startswith('\"\"\"'):\n",
        "                continue\n",
        "            if line.strip().startswith(\"Exercise\") or \"class SceneDescriptionSystem\" in line:\n",
        "                skip_block = True\n",
        "                continue\n",
        "            if ('def generate_scene_description' in line or\n",
        "                'def enhance_scene_descriptions' in line or\n",
        "                'def __init__' in line):\n",
        "                skip_block = True\n",
        "                continue\n",
        "            if line.strip().startswith('#TEST'):\n",
        "                skip_block = True\n",
        "                continue\n",
        "\n",
        "            # 空行結束跳過模式\n",
        "            if skip_block and line.strip() == \"\":\n",
        "                skip_block = False\n",
        "\n",
        "            # 如果不需要跳過，添加這行到結果\n",
        "            if not skip_block:\n",
        "                clean_description.append(line)\n",
        "\n",
        "        # 如果過濾後的描述為空，返回原始描述\n",
        "        if not clean_description:\n",
        "            return description\n",
        "        else:\n",
        "            return '\\n'.join(clean_description)\n",
        "\n",
        "    def _smart_append(self, current_text: str, new_fragment: str) -> str:\n",
        "        \"\"\"\n",
        "        Intelligently append a new text fragment to the current text,\n",
        "        handling punctuation and capitalization correctly.\n",
        "\n",
        "        Args:\n",
        "            current_text: The existing text to append to\n",
        "            new_fragment: The new text fragment to append\n",
        "\n",
        "        Returns:\n",
        "            str: The combined text with proper formatting\n",
        "        \"\"\"\n",
        "        # Handle empty cases\n",
        "        if not new_fragment:\n",
        "            return current_text\n",
        "\n",
        "        if not current_text:\n",
        "            # Ensure first character is uppercase for the first fragment\n",
        "            return new_fragment[0].upper() + new_fragment[1:] if new_fragment else \"\"\n",
        "\n",
        "        # Clean up existing text\n",
        "        current_text = current_text.rstrip()\n",
        "\n",
        "        # Check for ending punctuation\n",
        "        ends_with_sentence = current_text.endswith(('.', '!', '?'))\n",
        "        ends_with_comma = current_text.endswith(',')\n",
        "\n",
        "        # Specifically handle the \"A xxx A yyy\" pattern that's causing issues\n",
        "        if (current_text.startswith(\"A \") or current_text.startswith(\"An \")) and \\\n",
        "        (new_fragment.startswith(\"A \") or new_fragment.startswith(\"An \")):\n",
        "            return current_text + \". \" + new_fragment\n",
        "\n",
        "        # Decide how to join the texts\n",
        "        if ends_with_sentence:\n",
        "            # After a sentence, start with uppercase and add proper spacing\n",
        "            joined_text = current_text + \" \" + (new_fragment[0].upper() + new_fragment[1:])\n",
        "        elif ends_with_comma:\n",
        "            # After a comma, maintain flow with lowercase unless it's a proper noun or special case\n",
        "            if new_fragment.startswith(('I ', 'I\\'', 'A ', 'An ', 'The ')) or new_fragment[0].isupper():\n",
        "                joined_text = current_text + \" \" + new_fragment\n",
        "            else:\n",
        "                joined_text = current_text + \" \" + new_fragment[0].lower() + new_fragment[1:]\n",
        "        elif \"scene is\" in new_fragment.lower() or \"scene includes\" in new_fragment.lower():\n",
        "            # When adding a new sentence about the scene, use a period\n",
        "            joined_text = current_text + \". \" + new_fragment\n",
        "        else:\n",
        "            # For other cases, decide based on the content\n",
        "            if self._is_related_phrases(current_text, new_fragment):\n",
        "                if new_fragment.startswith(('I ', 'I\\'', 'A ', 'An ', 'The ')) or new_fragment[0].isupper():\n",
        "                    joined_text = current_text + \", \" + new_fragment\n",
        "                else:\n",
        "                    joined_text = current_text + \", \" + new_fragment[0].lower() + new_fragment[1:]\n",
        "            else:\n",
        "                # Use period for unrelated phrases\n",
        "                joined_text = current_text + \". \" + (new_fragment[0].upper() + new_fragment[1:])\n",
        "\n",
        "        return joined_text\n",
        "\n",
        "    def _is_related_phrases(self, text1: str, text2: str) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if two phrases are related and should be connected with a comma\n",
        "        rather than separated with a period.\n",
        "\n",
        "        Args:\n",
        "            text1: The first text fragment\n",
        "            text2: The second text fragment to be appended\n",
        "\n",
        "        Returns:\n",
        "            bool: Whether the phrases appear to be related\n",
        "        \"\"\"\n",
        "        # Check if either phrase starts with \"A\" or \"An\" - these are likely separate descriptions\n",
        "        if (text1.startswith(\"A \") or text1.startswith(\"An \")) and \\\n",
        "        (text2.startswith(\"A \") or text2.startswith(\"An \")):\n",
        "            return False  # These are separate descriptions, not related phrases\n",
        "\n",
        "        # Check if the second phrase starts with a connecting word\n",
        "        connecting_words = [\"which\", \"where\", \"who\", \"whom\", \"whose\", \"with\", \"without\",\n",
        "                        \"this\", \"these\", \"that\", \"those\", \"and\", \"or\", \"but\"]\n",
        "\n",
        "        first_word = text2.split()[0].lower() if text2 else \"\"\n",
        "        if first_word in connecting_words:\n",
        "            return True\n",
        "\n",
        "        # Check if the first phrase ends with something that suggests continuity\n",
        "        ending_patterns = [\"such as\", \"including\", \"like\", \"especially\", \"particularly\",\n",
        "                        \"for example\", \"for instance\", \"namely\", \"specifically\"]\n",
        "\n",
        "        for pattern in ending_patterns:\n",
        "            if text1.lower().endswith(pattern):\n",
        "                return True\n",
        "\n",
        "        # Check if both phrases are about the scene\n",
        "        if \"scene\" in text1.lower() and \"scene\" in text2.lower():\n",
        "            return False  # Separate statements about the scene should be separate sentences\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _format_final_description(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Format the final description text to ensure correct punctuation,\n",
        "        capitalization, and spacing.\n",
        "\n",
        "        Args:\n",
        "            text: The text to format\n",
        "\n",
        "        Returns:\n",
        "            str: The properly formatted text\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # 1. 特別處理連續以\"A\"開頭的片段 (這是一個常見問題)\n",
        "        text = re.sub(r'(A\\s[^.!?]+?)\\s+(A\\s)', r'\\1. \\2', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(An\\s[^.!?]+?)\\s+(An?\\s)', r'\\1. \\2', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 2. 確保第一個字母大寫\n",
        "        text = text[0].upper() + text[1:] if text else \"\"\n",
        "\n",
        "        # 3. 修正詞之間的空格問題\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)  # 多個空格改為一個\n",
        "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # 小寫後大寫間加空格\n",
        "\n",
        "        # 4. 修正詞連接問題\n",
        "        text = re.sub(r'([a-zA-Z])and', r'\\1 and', text)  # \"xxx\"和\"and\"間加空格\n",
        "        text = re.sub(r'([a-zA-Z])with', r'\\1 with', text)  # \"xxx\"和\"with\"間加空格\n",
        "        text = re.sub(r'plants(and|with|or)', r'plants \\1', text)  # 修正\"plantsand\"這類問題\n",
        "\n",
        "        # 5. 修正標點符號後的大小寫問題\n",
        "        text = re.sub(r'\\.(\\s+)([a-z])', lambda m: f'.{m.group(1)}{m.group(2).upper()}', text)  # 句號後大寫\n",
        "\n",
        "        # 6. 修正逗號後接大寫單詞的問題\n",
        "        def fix_capitalization_after_comma(match):\n",
        "            word = match.group(2)\n",
        "            # 例外情況：保留專有名詞、人稱代詞等的大寫\n",
        "            if word in [\"I\", \"I'm\", \"I've\", \"I'd\", \"I'll\"]:\n",
        "                return match.group(0)  # 保持原樣\n",
        "\n",
        "            # 保留月份、星期、地名等專有名詞的大寫\n",
        "            proper_nouns = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\",\n",
        "                            \"August\", \"September\", \"October\", \"November\", \"December\",\n",
        "                            \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "            if word in proper_nouns:\n",
        "                return match.group(0)  # 保持原樣\n",
        "\n",
        "            # 其他情況：將首字母改為小寫\n",
        "            return match.group(1) + word[0].lower() + word[1:]\n",
        "\n",
        "        # 匹配逗號後接空格再接大寫單詞的模式\n",
        "        text = re.sub(r'(,\\s+)([A-Z][a-zA-Z]*)', fix_capitalization_after_comma, text)\n",
        "\n",
        "\n",
        "        common_phrases = [\n",
        "            (r'Social or seating area', r'social or seating area'),\n",
        "            (r'Sleeping area', r'sleeping area'),\n",
        "            (r'Dining area', r'dining area'),\n",
        "            (r'Living space', r'living space')\n",
        "        ]\n",
        "\n",
        "        for phrase, replacement in common_phrases:\n",
        "            # 只修改句中的術語，保留句首的大寫\n",
        "            text = re.sub(r'(?<=[.!?]\\s)' + phrase, replacement, text)\n",
        "            # 修改句中的術語，但保留句首的大寫\n",
        "            text = re.sub(r'(?<=,\\s)' + phrase, replacement, text)\n",
        "\n",
        "        # 7. 確保標點符號後有空格\n",
        "        text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)  # 標點符號前不要空格\n",
        "        text = re.sub(r'([.,;:!?])([a-zA-Z0-9])', r'\\1 \\2', text)  # 標點符號後要有空格\n",
        "\n",
        "        # 8. 修正重複標點符號\n",
        "        text = re.sub(r'\\.{2,}', '.', text)  # 多個句號變一個\n",
        "        text = re.sub(r',{2,}', ',', text)  # 多個逗號變一個\n",
        "\n",
        "        # 9. 確保文本以標點結束\n",
        "        if text and not text[-1] in '.!?':\n",
        "            text += '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _is_intersection(self, detected_objects: List[Dict]) -> bool:\n",
        "        \"\"\"\n",
        "        通過分析物體分佈來判斷場景是否為十字路口\n",
        "        \"\"\"\n",
        "        # 檢查行人分佈模式\n",
        "        pedestrians = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "\n",
        "        if len(pedestrians) >= 8:  # 需要足夠的行人來形成十字路口\n",
        "            # 抓取行人位置\n",
        "            positions = [obj.get(\"normalized_center\", (0, 0)) for obj in pedestrians]\n",
        "\n",
        "            # 分析 x 和 y 坐標分佈\n",
        "            x_coords = [pos[0] for pos in positions]\n",
        "            y_coords = [pos[1] for pos in positions]\n",
        "\n",
        "            # 計算 x 和 y 坐標的變異數\n",
        "            x_variance = np.var(x_coords) if len(x_coords) > 1 else 0\n",
        "            y_variance = np.var(y_coords) if len(y_coords) > 1 else 0\n",
        "\n",
        "            # 計算範圍\n",
        "            x_range = max(x_coords) - min(x_coords)\n",
        "            y_range = max(y_coords) - min(y_coords)\n",
        "\n",
        "            # 如果 x 和 y 方向都有較大範圍且範圍相似，那就有可能是十字路口\n",
        "            if x_range > 0.5 and y_range > 0.5 and 0.7 < (x_range / y_range) < 1.3:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _generate_generic_description(self, detected_objects: List[Dict], lighting_info: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a generic description when scene type is unknown or confidence is very low.\n",
        "\n",
        "        Args:\n",
        "            detected_objects: List of detected objects\n",
        "            lighting_info: Optional lighting condition information\n",
        "\n",
        "        Returns:\n",
        "            str: Generic description based on detected objects\n",
        "        \"\"\"\n",
        "        # Count object occurrences\n",
        "        obj_counts = {}\n",
        "        for obj in detected_objects:\n",
        "            class_name = obj[\"class_name\"]\n",
        "            if class_name not in obj_counts:\n",
        "                obj_counts[class_name] = 0\n",
        "            obj_counts[class_name] += 1\n",
        "\n",
        "        # Get top objects by count\n",
        "        top_objects = sorted(obj_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "        if not top_objects:\n",
        "            base_desc = \"No clearly identifiable objects are visible in this scene.\"\n",
        "        else:\n",
        "            # Format object list\n",
        "            objects_text = []\n",
        "            for name, count in top_objects:\n",
        "                if count > 1:\n",
        "                    objects_text.append(f\"{count} {name}s\")\n",
        "                else:\n",
        "                    objects_text.append(name)\n",
        "\n",
        "            if len(objects_text) == 1:\n",
        "                objects_list = objects_text[0]\n",
        "            elif len(objects_text) == 2:\n",
        "                objects_list = f\"{objects_text[0]} and {objects_text[1]}\"\n",
        "            else:\n",
        "                objects_list = \", \".join(objects_text[:-1]) + f\", and {objects_text[-1]}\"\n",
        "\n",
        "            base_desc = f\"This scene contains {objects_list}.\"\n",
        "\n",
        "        # Add lighting information if available\n",
        "        if lighting_info and \"time_of_day\" in lighting_info:\n",
        "            lighting_type = lighting_info[\"time_of_day\"]\n",
        "            if lighting_type in self.templates.get(\"lighting_templates\", {}):\n",
        "                lighting_desc = self.templates[\"lighting_templates\"][lighting_type]\n",
        "                base_desc += f\" {lighting_desc}\"\n",
        "\n",
        "        return base_desc\n",
        "\n",
        "    def _generate_scene_details(self,\n",
        "                              scene_type: str,\n",
        "                              detected_objects: List[Dict],\n",
        "                              lighting_info: Optional[Dict] = None,\n",
        "                              viewpoint: str = \"eye_level\") -> str:\n",
        "        \"\"\"\n",
        "        Generate detailed description based on scene type and detected objects.\n",
        "\n",
        "        Args:\n",
        "            scene_type: Identified scene type\n",
        "            detected_objects: List of detected objects\n",
        "            lighting_info: Optional lighting condition information\n",
        "            viewpoint: Detected viewpoint (aerial, eye_level, etc.)\n",
        "\n",
        "        Returns:\n",
        "            str: Detailed scene description\n",
        "        \"\"\"\n",
        "        # Get scene-specific templates\n",
        "        scene_details = \"\"\n",
        "        scene_templates = self.templates.get(\"scene_detail_templates\", {})\n",
        "\n",
        "        # Handle specific scene types\n",
        "        if scene_type in scene_templates:\n",
        "            # Select a template appropriate for the viewpoint if available\n",
        "            viewpoint_key = f\"{scene_type}_{viewpoint}\"\n",
        "\n",
        "            if viewpoint_key in scene_templates:\n",
        "                # We have a viewpoint-specific template\n",
        "                templates_list = scene_templates[viewpoint_key]\n",
        "            else:\n",
        "                # Fall back to general templates for this scene type\n",
        "                templates_list = scene_templates[scene_type]\n",
        "\n",
        "            # Select a random template from the list\n",
        "            if templates_list:\n",
        "                detail_template = random.choice(templates_list)\n",
        "\n",
        "                # Fill the template with object information\n",
        "                scene_details = self._fill_detail_template(\n",
        "                    detail_template,\n",
        "                    detected_objects,\n",
        "                    scene_type\n",
        "                )\n",
        "        else:\n",
        "            # Use default templates if specific ones aren't available\n",
        "            if \"default\" in scene_templates:\n",
        "                detail_template = random.choice(scene_templates[\"default\"])\n",
        "                scene_details = self._fill_detail_template(\n",
        "                    detail_template,\n",
        "                    detected_objects,\n",
        "                    \"default\"\n",
        "                )\n",
        "            else:\n",
        "                # Fall back to basic description if no templates are available\n",
        "                scene_details = self._generate_basic_details(scene_type, detected_objects)\n",
        "\n",
        "        return scene_details\n",
        "\n",
        "    def _fill_detail_template(self, template: str, detected_objects: List[Dict], scene_type: str) -> str:\n",
        "        \"\"\"\n",
        "        Fill a template with specific details based on detected objects.\n",
        "\n",
        "        Args:\n",
        "            template: Template string with placeholders\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Identified scene type\n",
        "\n",
        "        Returns:\n",
        "            str: Filled template\n",
        "        \"\"\"\n",
        "        # Find placeholders in the template using simple {placeholder} syntax\n",
        "        import re\n",
        "        placeholders = re.findall(r'\\{([^}]+)\\}', template)\n",
        "\n",
        "        filled_template = template\n",
        "\n",
        "        # Get object template fillers\n",
        "        fillers = self.templates.get(\"object_template_fillers\", {})\n",
        "\n",
        "        # 為所有可能的變數設置默認值\n",
        "        default_replacements = {\n",
        "            # 室內相關\n",
        "            \"furniture\": \"various furniture pieces\",\n",
        "            \"seating\": \"comfortable seating\",\n",
        "            \"electronics\": \"entertainment devices\",\n",
        "            \"bed_type\": \"a bed\",\n",
        "            \"bed_location\": \"room\",\n",
        "            \"bed_description\": \"sleeping arrangements\",\n",
        "            \"extras\": \"personal items\",\n",
        "            \"table_setup\": \"a dining table and chairs\",\n",
        "            \"table_description\": \"a dining surface\",\n",
        "            \"dining_items\": \"dining furniture and tableware\",\n",
        "            \"appliances\": \"kitchen appliances\",\n",
        "            \"kitchen_items\": \"cooking utensils and dishware\",\n",
        "            \"cooking_equipment\": \"cooking equipment\",\n",
        "            \"office_equipment\": \"work-related furniture and devices\",\n",
        "            \"desk_setup\": \"a desk and chair\",\n",
        "            \"computer_equipment\": \"electronic devices\",\n",
        "\n",
        "            # 室外/城市相關\n",
        "            \"traffic_description\": \"vehicles and pedestrians\",\n",
        "            \"people_and_vehicles\": \"people and various vehicles\",\n",
        "            \"street_elements\": \"urban infrastructure\",\n",
        "            \"park_features\": \"benches and greenery\",\n",
        "            \"outdoor_elements\": \"natural features\",\n",
        "            \"park_description\": \"outdoor amenities\",\n",
        "            \"store_elements\": \"merchandise displays\",\n",
        "            \"shopping_activity\": \"customers browse and shop\",\n",
        "            \"store_items\": \"products for sale\",\n",
        "\n",
        "            # 高級餐廳相關\n",
        "            \"design_elements\": \"elegant decor\",\n",
        "            \"lighting\": \"stylish lighting fixtures\",\n",
        "\n",
        "            # 亞洲商業街相關\n",
        "            \"storefront_features\": \"compact shops\",\n",
        "            \"pedestrian_flow\": \"people walking\",\n",
        "            \"asian_elements\": \"distinctive cultural elements\",\n",
        "            \"cultural_elements\": \"traditional design features\",\n",
        "            \"signage\": \"colorful signs\",\n",
        "            \"street_activities\": \"busy urban activity\",\n",
        "\n",
        "            # 金融區相關\n",
        "            \"buildings\": \"tall buildings\",\n",
        "            \"traffic_elements\": \"vehicles\",\n",
        "            \"skyscrapers\": \"high-rise buildings\",\n",
        "            \"road_features\": \"wide streets\",\n",
        "            \"architectural_elements\": \"modern architecture\",\n",
        "            \"city_landmarks\": \"prominent structures\",\n",
        "\n",
        "            # 十字路口相關\n",
        "            \"crossing_pattern\": \"marked pedestrian crossings\",\n",
        "            \"pedestrian_behavior\": \"careful walking\",\n",
        "            \"pedestrian_density\": \"groups of pedestrians\",\n",
        "            \"traffic_pattern\": \"regulated traffic flow\",\n",
        "\n",
        "            # 交通樞紐相關\n",
        "            \"transit_vehicles\": \"public transportation vehicles\",\n",
        "            \"passenger_activity\": \"commuter movement\",\n",
        "            \"transportation_modes\": \"various transit options\",\n",
        "            \"passenger_needs\": \"waiting areas\",\n",
        "            \"transit_infrastructure\": \"transit facilities\",\n",
        "            \"passenger_movement\": \"commuter flow\",\n",
        "\n",
        "            # 購物區相關\n",
        "            \"retail_elements\": \"shops and displays\",\n",
        "            \"store_types\": \"various retail establishments\",\n",
        "            \"walkway_features\": \"pedestrian pathways\",\n",
        "            \"commercial_signage\": \"store signs\",\n",
        "            \"consumer_behavior\": \"shopping activities\",\n",
        "\n",
        "            # 空中視角相關\n",
        "            \"commercial_layout\": \"organized retail areas\",\n",
        "            \"pedestrian_pattern\": \"people movement patterns\",\n",
        "            \"gathering_features\": \"public gathering spaces\",\n",
        "            \"movement_pattern\": \"crowd flow patterns\",\n",
        "            \"urban_elements\": \"city infrastructure\",\n",
        "            \"public_activity\": \"social interaction\",\n",
        "\n",
        "            # 文化特定元素\n",
        "            \"stall_elements\": \"vendor booths\",\n",
        "            \"lighting_features\": \"decorative lights\",\n",
        "            \"food_elements\": \"food offerings\",\n",
        "            \"vendor_stalls\": \"market stalls\",\n",
        "            \"nighttime_activity\": \"evening commerce\",\n",
        "            \"cultural_lighting\": \"traditional lighting\",\n",
        "            \"night_market_sounds\": \"lively market sounds\",\n",
        "            \"evening_crowd_behavior\": \"nighttime social activity\",\n",
        "            \"architectural_elements\": \"cultural buildings\",\n",
        "            \"religious_structures\": \"sacred buildings\",\n",
        "            \"decorative_features\": \"ornamental designs\",\n",
        "            \"cultural_practices\": \"traditional activities\",\n",
        "            \"temple_architecture\": \"religious structures\",\n",
        "            \"sensory_elements\": \"atmospheric elements\",\n",
        "            \"visitor_activities\": \"cultural experiences\",\n",
        "            \"ritual_activities\": \"ceremonial practices\",\n",
        "            \"cultural_symbols\": \"meaningful symbols\",\n",
        "            \"architectural_style\": \"historical buildings\",\n",
        "            \"historic_elements\": \"traditional architecture\",\n",
        "            \"urban_design\": \"city planning elements\",\n",
        "            \"social_behaviors\": \"public interactions\",\n",
        "            \"european_features\": \"European architectural details\",\n",
        "            \"tourist_activities\": \"visitor activities\",\n",
        "            \"local_customs\": \"regional practices\",\n",
        "\n",
        "            # 時間特定元素\n",
        "            \"lighting_effects\": \"artificial lighting\",\n",
        "            \"shadow_patterns\": \"light and shadow\",\n",
        "            \"urban_features\": \"city elements\",\n",
        "            \"illuminated_elements\": \"lit structures\",\n",
        "            \"evening_activities\": \"nighttime activities\",\n",
        "            \"light_sources\": \"lighting points\",\n",
        "            \"lit_areas\": \"illuminated spaces\",\n",
        "            \"shadowed_zones\": \"darker areas\",\n",
        "            \"illuminated_signage\": \"bright signs\",\n",
        "            \"colorful_lighting\": \"multicolored lights\",\n",
        "            \"neon_elements\": \"neon signs\",\n",
        "            \"night_crowd_behavior\": \"evening social patterns\",\n",
        "            \"light_displays\": \"lighting installations\",\n",
        "            \"building_features\": \"architectural elements\",\n",
        "            \"nightlife_activities\": \"evening entertainment\",\n",
        "            \"lighting_modifier\": \"bright\",\n",
        "\n",
        "            # 混合環境元素\n",
        "            \"transitional_elements\": \"connecting features\",\n",
        "            \"indoor_features\": \"interior elements\",\n",
        "            \"outdoor_setting\": \"exterior spaces\",\n",
        "            \"interior_amenities\": \"inside comforts\",\n",
        "            \"exterior_features\": \"outside elements\",\n",
        "            \"inside_elements\": \"interior design\",\n",
        "            \"outside_spaces\": \"outdoor areas\",\n",
        "            \"dual_environment_benefits\": \"combined settings\",\n",
        "            \"passenger_activities\": \"waiting behaviors\",\n",
        "            \"transportation_types\": \"transit vehicles\",\n",
        "            \"sheltered_elements\": \"covered areas\",\n",
        "            \"exposed_areas\": \"open sections\",\n",
        "            \"waiting_behaviors\": \"passenger activities\",\n",
        "            \"indoor_facilities\": \"inside services\",\n",
        "            \"platform_features\": \"transit platform elements\",\n",
        "            \"transit_routines\": \"transportation procedures\",\n",
        "\n",
        "            # 專門場所元素\n",
        "            \"seating_arrangement\": \"spectator seating\",\n",
        "            \"playing_surface\": \"athletic field\",\n",
        "            \"sporting_activities\": \"sports events\",\n",
        "            \"spectator_facilities\": \"viewer accommodations\",\n",
        "            \"competition_space\": \"sports arena\",\n",
        "            \"sports_events\": \"athletic competitions\",\n",
        "            \"viewing_areas\": \"audience sections\",\n",
        "            \"field_elements\": \"field markings and equipment\",\n",
        "            \"game_activities\": \"competitive play\",\n",
        "            \"construction_equipment\": \"building machinery\",\n",
        "            \"building_materials\": \"construction supplies\",\n",
        "            \"construction_activities\": \"building work\",\n",
        "            \"work_elements\": \"construction tools\",\n",
        "            \"structural_components\": \"building structures\",\n",
        "            \"site_equipment\": \"construction gear\",\n",
        "            \"raw_materials\": \"building supplies\",\n",
        "            \"construction_process\": \"building phases\",\n",
        "            \"medical_elements\": \"healthcare equipment\",\n",
        "            \"clinical_activities\": \"medical procedures\",\n",
        "            \"facility_design\": \"healthcare layout\",\n",
        "            \"healthcare_features\": \"medical facilities\",\n",
        "            \"patient_interactions\": \"care activities\",\n",
        "            \"equipment_types\": \"medical devices\",\n",
        "            \"care_procedures\": \"health services\",\n",
        "            \"treatment_spaces\": \"clinical areas\",\n",
        "            \"educational_furniture\": \"learning furniture\",\n",
        "            \"learning_activities\": \"educational practices\",\n",
        "            \"instructional_design\": \"teaching layout\",\n",
        "            \"classroom_elements\": \"school equipment\",\n",
        "            \"teaching_methods\": \"educational approaches\",\n",
        "            \"student_engagement\": \"learning participation\",\n",
        "            \"learning_spaces\": \"educational areas\",\n",
        "            \"educational_tools\": \"teaching resources\",\n",
        "            \"knowledge_transfer\": \"learning exchanges\"\n",
        "        }\n",
        "\n",
        "        # For each placeholder, try to fill with appropriate content\n",
        "        for placeholder in placeholders:\n",
        "            if placeholder in fillers:\n",
        "                # Get random filler for this placeholder\n",
        "                options = fillers[placeholder]\n",
        "                if options:\n",
        "                    # Select 1-3 items from the options list\n",
        "                    num_items = min(len(options), random.randint(1, 3))\n",
        "                    selected_items = random.sample(options, num_items)\n",
        "\n",
        "                    # Create a formatted list\n",
        "                    if len(selected_items) == 1:\n",
        "                        replacement = selected_items[0]\n",
        "                    elif len(selected_items) == 2:\n",
        "                        replacement = f\"{selected_items[0]} and {selected_items[1]}\"\n",
        "                    else:\n",
        "                        replacement = \", \".join(selected_items[:-1]) + f\", and {selected_items[-1]}\"\n",
        "\n",
        "                    # Replace the placeholder\n",
        "                    filled_template = filled_template.replace(f\"{{{placeholder}}}\", replacement)\n",
        "            else:\n",
        "                # Try to fill with scene-specific logic\n",
        "                replacement = self._generate_placeholder_content(placeholder, detected_objects, scene_type)\n",
        "                if replacement:\n",
        "                    filled_template = filled_template.replace(f\"{{{placeholder}}}\", replacement)\n",
        "                elif placeholder in default_replacements:\n",
        "                    # Use default replacement if available\n",
        "                    filled_template = filled_template.replace(f\"{{{placeholder}}}\", default_replacements[placeholder])\n",
        "                else:\n",
        "                    # Last resort default\n",
        "                    filled_template = filled_template.replace(f\"{{{placeholder}}}\", \"various items\")\n",
        "\n",
        "        return filled_template\n",
        "\n",
        "    def _generate_placeholder_content(self, placeholder: str, detected_objects: List[Dict], scene_type: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate content for a template placeholder based on scene-specific logic.\n",
        "\n",
        "        Args:\n",
        "            placeholder: Template placeholder\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Identified scene type\n",
        "\n",
        "        Returns:\n",
        "            str: Content for the placeholder\n",
        "        \"\"\"\n",
        "        # Handle different types of placeholders with custom logic\n",
        "        if placeholder == \"furniture\":\n",
        "            # Extract furniture items\n",
        "            furniture_ids = [56, 57, 58, 59, 60, 61]  # Example furniture IDs\n",
        "            furniture_objects = [obj for obj in detected_objects if obj[\"class_id\"] in furniture_ids]\n",
        "\n",
        "            if furniture_objects:\n",
        "                furniture_names = [obj[\"class_name\"] for obj in furniture_objects[:3]]\n",
        "                return \", \".join(set(furniture_names))\n",
        "            return \"various furniture items\"\n",
        "\n",
        "        elif placeholder == \"electronics\":\n",
        "            # Extract electronic items\n",
        "            electronics_ids = [62, 63, 64, 65, 66, 67, 68, 69, 70]  # Example electronics IDs\n",
        "            electronics_objects = [obj for obj in detected_objects if obj[\"class_id\"] in electronics_ids]\n",
        "\n",
        "            if electronics_objects:\n",
        "                electronics_names = [obj[\"class_name\"] for obj in electronics_objects[:3]]\n",
        "                return \", \".join(set(electronics_names))\n",
        "            return \"electronic devices\"\n",
        "\n",
        "        elif placeholder == \"people_count\":\n",
        "            # Count people\n",
        "            people_count = len([obj for obj in detected_objects if obj[\"class_id\"] == 0])\n",
        "\n",
        "            if people_count == 0:\n",
        "                return \"no people\"\n",
        "            elif people_count == 1:\n",
        "                return \"one person\"\n",
        "            elif people_count < 5:\n",
        "                return f\"{people_count} people\"\n",
        "            else:\n",
        "                return \"several people\"\n",
        "\n",
        "        elif placeholder == \"seating\":\n",
        "            # Extract seating items\n",
        "            seating_ids = [56, 57]  # chair, sofa\n",
        "            seating_objects = [obj for obj in detected_objects if obj[\"class_id\"] in seating_ids]\n",
        "\n",
        "            if seating_objects:\n",
        "                seating_names = [obj[\"class_name\"] for obj in seating_objects[:2]]\n",
        "                return \", \".join(set(seating_names))\n",
        "            return \"seating arrangements\"\n",
        "\n",
        "        # Default case - empty string\n",
        "        return \"\"\n",
        "\n",
        "    def _generate_basic_details(self, scene_type: str, detected_objects: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Generate basic details when templates aren't available.\n",
        "\n",
        "        Args:\n",
        "            scene_type: Identified scene type\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            str: Basic scene details\n",
        "        \"\"\"\n",
        "        # Handle specific scene types with custom logic\n",
        "        if scene_type == \"living_room\":\n",
        "            tv_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 62]  # TV\n",
        "            sofa_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 57]  # Sofa\n",
        "\n",
        "            if tv_objs and sofa_objs:\n",
        "                tv_region = tv_objs[0][\"region\"]\n",
        "                sofa_region = sofa_objs[0][\"region\"]\n",
        "\n",
        "                arrangement = f\"The TV is in the {tv_region.replace('_', ' ')} of the image, \"\n",
        "                arrangement += f\"while the sofa is in the {sofa_region.replace('_', ' ')}. \"\n",
        "\n",
        "                return f\"{arrangement}This appears to be a space designed for relaxation and entertainment.\"\n",
        "\n",
        "        elif scene_type == \"bedroom\":\n",
        "            bed_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 59]  # Bed\n",
        "\n",
        "            if bed_objs:\n",
        "                bed_region = bed_objs[0][\"region\"]\n",
        "                extra_items = []\n",
        "\n",
        "                for obj in detected_objects:\n",
        "                    if obj[\"class_id\"] == 74:  # Clock\n",
        "                        extra_items.append(\"clock\")\n",
        "                    elif obj[\"class_id\"] == 73:  # Book\n",
        "                        extra_items.append(\"book\")\n",
        "\n",
        "                extras = \"\"\n",
        "                if extra_items:\n",
        "                    extras = f\" There is also a {' and a '.join(extra_items)} visible.\"\n",
        "\n",
        "                return f\"The bed is located in the {bed_region.replace('_', ' ')} of the image.{extras}\"\n",
        "\n",
        "        elif scene_type in [\"dining_area\", \"kitchen\"]:\n",
        "            # Count food and dining-related items\n",
        "            food_items = []\n",
        "            for obj in detected_objects:\n",
        "                if obj[\"class_id\"] in [39, 41, 42, 43, 44, 45]:  # Kitchen items\n",
        "                    food_items.append(obj[\"class_name\"])\n",
        "\n",
        "            food_str = \"\"\n",
        "            if food_items:\n",
        "                unique_items = list(set(food_items))\n",
        "                if len(unique_items) <= 3:\n",
        "                    food_str = f\" with {', '.join(unique_items)}\"\n",
        "                else:\n",
        "                    food_str = f\" with {', '.join(unique_items[:3])} and other items\"\n",
        "\n",
        "            return f\"{food_str}.\"\n",
        "\n",
        "        elif scene_type == \"city_street\":\n",
        "            # Count people and vehicles\n",
        "            people_count = len([obj for obj in detected_objects if obj[\"class_id\"] == 0])\n",
        "            vehicle_count = len([obj for obj in detected_objects\n",
        "                               if obj[\"class_id\"] in [1, 2, 3, 5, 7]])  # Bicycle, car, motorbike, bus, truck\n",
        "\n",
        "            traffic_desc = \"\"\n",
        "            if people_count > 0 and vehicle_count > 0:\n",
        "                traffic_desc = f\" with {people_count} {'people' if people_count > 1 else 'person'} and \"\n",
        "                traffic_desc += f\"{vehicle_count} {'vehicles' if vehicle_count > 1 else 'vehicle'}\"\n",
        "            elif people_count > 0:\n",
        "                traffic_desc = f\" with {people_count} {'people' if people_count > 1 else 'person'}\"\n",
        "            elif vehicle_count > 0:\n",
        "                traffic_desc = f\" with {vehicle_count} {'vehicles' if vehicle_count > 1 else 'vehicle'}\"\n",
        "\n",
        "            return f\"{traffic_desc}.\"\n",
        "\n",
        "        # Handle more specialized scenes\n",
        "        elif scene_type == \"asian_commercial_street\":\n",
        "            # Look for key urban elements\n",
        "            people_count = len([obj for obj in detected_objects if obj[\"class_id\"] == 0])\n",
        "            vehicle_count = len([obj for obj in detected_objects if obj[\"class_id\"] in [1, 2, 3]])\n",
        "\n",
        "            # Analyze pedestrian distribution\n",
        "            people_positions = []\n",
        "            for obj in detected_objects:\n",
        "                if obj[\"class_id\"] == 0:  # Person\n",
        "                    people_positions.append(obj[\"normalized_center\"])\n",
        "\n",
        "            # Check if people are distributed along a line (indicating a walking path)\n",
        "            structured_path = False\n",
        "            if len(people_positions) >= 3:\n",
        "                # Simplified check - see if y-coordinates are similar for multiple people\n",
        "                y_coords = [pos[1] for pos in people_positions]\n",
        "                y_mean = sum(y_coords) / len(y_coords)\n",
        "                y_variance = sum((y - y_mean)**2 for y in y_coords) / len(y_coords)\n",
        "                if y_variance < 0.05:  # Low variance indicates linear arrangement\n",
        "                    structured_path = True\n",
        "\n",
        "            street_desc = \"A commercial street with \"\n",
        "            if people_count > 0:\n",
        "                street_desc += f\"{people_count} {'pedestrians' if people_count > 1 else 'pedestrian'}\"\n",
        "                if vehicle_count > 0:\n",
        "                    street_desc += f\" and {vehicle_count} {'vehicles' if vehicle_count > 1 else 'vehicle'}\"\n",
        "            elif vehicle_count > 0:\n",
        "                street_desc += f\"{vehicle_count} {'vehicles' if vehicle_count > 1 else 'vehicle'}\"\n",
        "            else:\n",
        "                street_desc += \"various commercial elements\"\n",
        "\n",
        "            if structured_path:\n",
        "                street_desc += \". The pedestrians appear to be following a defined walking path\"\n",
        "\n",
        "            # Add cultural elements\n",
        "            street_desc += \". The signage and architectural elements suggest an Asian urban setting.\"\n",
        "\n",
        "            return street_desc\n",
        "\n",
        "        # Default general description\n",
        "        return \"The scene contains various elements characteristic of this environment.\"\n",
        "\n",
        "    def _detect_viewpoint(self, detected_objects: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        改進視角檢測，特別加強對空中俯視視角的識別。\n",
        "\n",
        "        Args:\n",
        "            detected_objects: 檢測到的物體列表\n",
        "\n",
        "        Returns:\n",
        "            str: 檢測到的視角類型\n",
        "        \"\"\"\n",
        "        if not detected_objects:\n",
        "            return \"eye_level\"  # default\n",
        "\n",
        "        # 提取物體位置和大小\n",
        "        top_region_count = 0\n",
        "        bottom_region_count = 0\n",
        "        total_objects = len(detected_objects)\n",
        "\n",
        "        # 追蹤大小分布以檢測空中視角\n",
        "        sizes = []\n",
        "\n",
        "        # 垂直大小比例用於低角度檢測\n",
        "        height_width_ratios = []\n",
        "\n",
        "        # 用於檢測規則圖案的變數\n",
        "        people_positions = []\n",
        "        crosswalk_pattern_detected = False\n",
        "\n",
        "        for obj in detected_objects:\n",
        "            # 計算頂部/底部區域中的物體\n",
        "            region = obj[\"region\"]\n",
        "            if \"top\" in region:\n",
        "                top_region_count += 1\n",
        "            elif \"bottom\" in region:\n",
        "                bottom_region_count += 1\n",
        "\n",
        "            # 計算標準化大小（面積）\n",
        "            if \"normalized_area\" in obj:\n",
        "                sizes.append(obj[\"normalized_area\"])\n",
        "\n",
        "            # 計算高度/寬度比例\n",
        "            if \"normalized_size\" in obj:\n",
        "                width, height = obj[\"normalized_size\"]\n",
        "                if width > 0:\n",
        "                    height_width_ratios.append(height / width)\n",
        "\n",
        "            # 收集人的位置用於圖案檢測\n",
        "            if obj[\"class_id\"] == 0:  # 人\n",
        "                if \"normalized_center\" in obj:\n",
        "                    people_positions.append(obj[\"normalized_center\"])\n",
        "\n",
        "        # 專門為斑馬線十字路口添加檢測邏輯\n",
        "        # 檢查是否有明顯的垂直和水平行人分布\n",
        "        people_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]  # 人\n",
        "\n",
        "        if len(people_objs) >= 8:  # 需要足夠多的人才能形成十字路口模式\n",
        "            # 檢查是否有斑馬線模式 - 新增功能\n",
        "            if len(people_positions) >= 4:\n",
        "                # 對位置進行聚類分析，尋找線性分布\n",
        "                x_coords = [pos[0] for pos in people_positions]\n",
        "                y_coords = [pos[1] for pos in people_positions]\n",
        "\n",
        "                # 計算 x 和 y 坐標的變異數和範圍\n",
        "                x_variance = np.var(x_coords) if len(x_coords) > 1 else 0\n",
        "                y_variance = np.var(y_coords) if len(y_coords) > 1 else 0\n",
        "\n",
        "                x_range = max(x_coords) - min(x_coords)\n",
        "                y_range = max(y_coords) - min(y_coords)\n",
        "\n",
        "                # 嘗試檢測十字形分布\n",
        "                # 如果 x 和 y 方向都有較大範圍，且範圍相似，可能是十字路口\n",
        "                if x_range > 0.5 and y_range > 0.5 and 0.7 < (x_range / y_range) < 1.3:\n",
        "\n",
        "                    # 計算到中心點的距離\n",
        "                    center_x = np.mean(x_coords)\n",
        "                    center_y = np.mean(y_coords)\n",
        "\n",
        "                    # 將點映射到十字架的軸上（水平和垂直）\n",
        "                    x_axis_distance = [abs(x - center_x) for x in x_coords]\n",
        "                    y_axis_distance = [abs(y - center_y) for y in y_coords]\n",
        "\n",
        "                    # 點應該接近軸線（水平或垂直）\n",
        "                    # 對於每個點，檢查它是否接近水平或垂直軸線\n",
        "                    close_to_axis_count = 0\n",
        "                    for i in range(len(x_coords)):\n",
        "                        if x_axis_distance[i] < 0.1 or y_axis_distance[i] < 0.1:\n",
        "                            close_to_axis_count += 1\n",
        "\n",
        "                    # 如果足夠多的點接近軸線，認為是十字路口\n",
        "                    if close_to_axis_count >= len(x_coords) * 0.6:\n",
        "                        crosswalk_pattern_detected = True\n",
        "\n",
        "                # 如果沒有檢測到十字形，嘗試檢測線性聚類分布\n",
        "                if not crosswalk_pattern_detected:\n",
        "                    # 檢查 x 和 y 方向的聚類\n",
        "                    x_clusters = self._detect_linear_clusters(x_coords)\n",
        "                    y_clusters = self._detect_linear_clusters(y_coords)\n",
        "\n",
        "                    # 如果在 x 和 y 方向上都有多個聚類，可能是交叉的斑馬線\n",
        "                    if len(x_clusters) >= 2 and len(y_clusters) >= 2:\n",
        "                        crosswalk_pattern_detected = True\n",
        "\n",
        "        # 檢測斑馬線模式 - 優先判斷\n",
        "        if crosswalk_pattern_detected:\n",
        "            return \"aerial\"\n",
        "\n",
        "        # 檢測行人分布情況\n",
        "        if len(people_objs) >= 10:\n",
        "            people_region_counts = {}\n",
        "            for obj in people_objs:\n",
        "                region = obj[\"region\"]\n",
        "                if region not in people_region_counts:\n",
        "                    people_region_counts[region] = 0\n",
        "                people_region_counts[region] += 1\n",
        "\n",
        "            # 計算不同區域中的行人數量\n",
        "            region_count = len([r for r, c in people_region_counts.items() if c >= 2])\n",
        "\n",
        "            # 如果行人分布在多個區域中，可能是空中視角\n",
        "            if region_count >= 4:\n",
        "                # 檢查行人分布的模式\n",
        "                # 特別是檢查不同區域中行人數量的差異\n",
        "                region_counts = list(people_region_counts.values())\n",
        "                region_counts_variance = np.var(region_counts) if len(region_counts) > 1 else 0\n",
        "                region_counts_mean = np.mean(region_counts) if region_counts else 0\n",
        "\n",
        "                # 如果行人分布較為均勻（變異係數小），可能是空中視角\n",
        "                if region_counts_mean > 0:\n",
        "                    variation_coefficient = region_counts_variance / region_counts_mean\n",
        "                    if variation_coefficient < 0.5:\n",
        "                        return \"aerial\"\n",
        "\n",
        "        # 計算指標\n",
        "        top_ratio = top_region_count / total_objects if total_objects > 0 else 0\n",
        "        bottom_ratio = bottom_region_count / total_objects if total_objects > 0 else 0\n",
        "\n",
        "        # 大小變異數（標準化）\n",
        "        size_variance = 0\n",
        "        if sizes:\n",
        "            mean_size = sum(sizes) / len(sizes)\n",
        "            size_variance = sum((s - mean_size) ** 2 for s in sizes) / len(sizes)\n",
        "            size_variance = size_variance / (mean_size ** 2)  # 標準化\n",
        "\n",
        "        # 平均高度/寬度比例\n",
        "        avg_height_width_ratio = sum(height_width_ratios) / len(height_width_ratios) if height_width_ratios else 1.0\n",
        "\n",
        "        # 空中視角：低大小差異，物體均勻分布，底部很少或沒有物體\n",
        "        if (size_variance < self.viewpoint_params[\"aerial_size_variance_threshold\"] and\n",
        "            bottom_ratio < 0.3 and top_ratio > self.viewpoint_params[\"aerial_threshold\"]):\n",
        "            return \"aerial\"\n",
        "\n",
        "        # 低角度視角：物體傾向於比寬高，頂部較多物體\n",
        "        elif (avg_height_width_ratio > self.viewpoint_params[\"vertical_size_ratio_threshold\"] and\n",
        "            top_ratio > self.viewpoint_params[\"low_angle_threshold\"]):\n",
        "            return \"low_angle\"\n",
        "\n",
        "        # 高視角：底部較多物體，頂部較少\n",
        "        elif (bottom_ratio > self.viewpoint_params[\"elevated_threshold\"] and\n",
        "            top_ratio < self.viewpoint_params[\"elevated_top_threshold\"]):\n",
        "            return \"elevated\"\n",
        "\n",
        "        # 默認：平視角\n",
        "        return \"eye_level\"\n",
        "\n",
        "    def _detect_linear_clusters(self, coords, threshold=0.05):\n",
        "        \"\"\"\n",
        "        檢測坐標中的線性聚類\n",
        "\n",
        "        Args:\n",
        "            coords: 一維坐標列表\n",
        "            threshold: 聚類閾值\n",
        "\n",
        "        Returns:\n",
        "            list: 聚類列表\n",
        "        \"\"\"\n",
        "        if not coords:\n",
        "            return []\n",
        "\n",
        "        # 排序坐標\n",
        "        sorted_coords = sorted(coords)\n",
        "\n",
        "        clusters = []\n",
        "        current_cluster = [sorted_coords[0]]\n",
        "\n",
        "        for i in range(1, len(sorted_coords)):\n",
        "            # 如果當前坐標與前一個接近，添加到當前聚類\n",
        "            if sorted_coords[i] - sorted_coords[i-1] < threshold:\n",
        "                current_cluster.append(sorted_coords[i])\n",
        "            else:\n",
        "                # 否則開始新的聚類\n",
        "                if len(current_cluster) >= 2:  # 至少需要2個點形成聚類\n",
        "                    clusters.append(current_cluster)\n",
        "                current_cluster = [sorted_coords[i]]\n",
        "\n",
        "        # 添加最後一個cluster\n",
        "        if len(current_cluster) >= 2:\n",
        "            clusters.append(current_cluster)\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _detect_cultural_context(self, scene_type: str, detected_objects: List[Dict]) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Detect the likely cultural context of the scene.\n",
        "\n",
        "        Args:\n",
        "            scene_type: Identified scene type\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            Optional[str]: Detected cultural context (asian, european, etc.) or None\n",
        "        \"\"\"\n",
        "        # Scene types with explicit cultural contexts\n",
        "        cultural_scene_mapping = {\n",
        "            \"asian_commercial_street\": \"asian\",\n",
        "            \"asian_night_market\": \"asian\",\n",
        "            \"asian_temple_area\": \"asian\",\n",
        "            \"european_plaza\": \"european\"\n",
        "        }\n",
        "\n",
        "        # Check if scene type directly indicates cultural context\n",
        "        if scene_type in cultural_scene_mapping:\n",
        "            return cultural_scene_mapping[scene_type]\n",
        "\n",
        "        # No specific cultural context detected\n",
        "        return None\n",
        "\n",
        "    def _generate_cultural_elements(self, cultural_context: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate description of cultural elements for the detected context.\n",
        "\n",
        "        Args:\n",
        "            cultural_context: Detected cultural context\n",
        "\n",
        "        Returns:\n",
        "            str: Description of cultural elements\n",
        "        \"\"\"\n",
        "        # Get template for this cultural context\n",
        "        cultural_templates = self.templates.get(\"cultural_templates\", {})\n",
        "\n",
        "        if cultural_context in cultural_templates:\n",
        "            template = cultural_templates[cultural_context]\n",
        "            elements = template.get(\"elements\", [])\n",
        "\n",
        "            if elements:\n",
        "                # Select 1-2 random elements\n",
        "                num_elements = min(len(elements), random.randint(1, 2))\n",
        "                selected_elements = random.sample(elements, num_elements)\n",
        "\n",
        "                # Format elements list\n",
        "                elements_text = \" and \".join(selected_elements) if num_elements == 2 else selected_elements[0]\n",
        "\n",
        "                # Fill template\n",
        "                return template.get(\"description\", \"\").format(elements=elements_text)\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _optimize_object_description(self, description: str) -> str:\n",
        "        \"\"\"\n",
        "        優化物品描述，避免重複列舉相同物品\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        # 處理床鋪重複描述\n",
        "        if \"bed in the room\" in description:\n",
        "            description = description.replace(\"a bed in the room\", \"a bed\")\n",
        "\n",
        "        # 處理重複的物品列表\n",
        "        # 尋找格式如 \"item, item, item\" 的模式\n",
        "        object_lists = re.findall(r'with ([^\\.]+?)(?:\\.|\\band\\b)', description)\n",
        "\n",
        "        for obj_list in object_lists:\n",
        "            # 計算每個物品出現次數\n",
        "            items = re.findall(r'([a-zA-Z\\s]+)(?:,|\\band\\b|$)', obj_list)\n",
        "            item_counts = {}\n",
        "\n",
        "            for item in items:\n",
        "                item = item.strip()\n",
        "                if item and item not in [\"and\", \"with\"]:\n",
        "                    if item not in item_counts:\n",
        "                        item_counts[item] = 0\n",
        "                    item_counts[item] += 1\n",
        "\n",
        "            # 生成優化後的物品列表\n",
        "            if item_counts:\n",
        "                new_items = []\n",
        "                for item, count in item_counts.items():\n",
        "                    if count > 1:\n",
        "                        new_items.append(f\"{count} {item}s\")\n",
        "                    else:\n",
        "                        new_items.append(item)\n",
        "\n",
        "                # 格式化新列表\n",
        "                if len(new_items) == 1:\n",
        "                    new_list = new_items[0]\n",
        "                elif len(new_items) == 2:\n",
        "                    new_list = f\"{new_items[0]} and {new_items[1]}\"\n",
        "                else:\n",
        "                    new_list = \", \".join(new_items[:-1]) + f\", and {new_items[-1]}\"\n",
        "\n",
        "                # 替換原始列表\n",
        "                description = description.replace(obj_list, new_list)\n",
        "\n",
        "        return description\n",
        "\n",
        "    def _describe_functional_zones(self, functional_zones: Dict) -> str:\n",
        "        \"\"\"\n",
        "        生成場景功能區域的描述，優化處理行人區域、人數統計和物品重複問題。\n",
        "\n",
        "        Args:\n",
        "            functional_zones: 識別出的功能區域字典\n",
        "\n",
        "        Returns:\n",
        "            str: 功能區域描述\n",
        "        \"\"\"\n",
        "        if not functional_zones:\n",
        "            return \"\"\n",
        "\n",
        "        # 計算場景中的總人數\n",
        "        total_people_count = 0\n",
        "        people_by_zone = {}\n",
        "\n",
        "        # 計算每個區域的人數並累計總人數\n",
        "        for zone_name, zone_info in functional_zones.items():\n",
        "            if \"objects\" in zone_info:\n",
        "                zone_people_count = zone_info[\"objects\"].count(\"person\")\n",
        "                people_by_zone[zone_name] = zone_people_count\n",
        "                total_people_count += zone_people_count\n",
        "\n",
        "        # 分類區域為行人區域和其他區域\n",
        "        pedestrian_zones = []\n",
        "        other_zones = []\n",
        "\n",
        "        for zone_name, zone_info in functional_zones.items():\n",
        "            # 檢查是否是行人相關區域\n",
        "            if any(keyword in zone_name.lower() for keyword in [\"pedestrian\", \"crossing\", \"people\"]):\n",
        "                pedestrian_zones.append((zone_name, zone_info))\n",
        "            else:\n",
        "                other_zones.append((zone_name, zone_info))\n",
        "\n",
        "        # 獲取最重要的行人區域和其他區域\n",
        "        main_pedestrian_zones = sorted(pedestrian_zones,\n",
        "                                    key=lambda z: people_by_zone.get(z[0], 0),\n",
        "                                    reverse=True)[:1]  # 最多1個主要行人區域\n",
        "\n",
        "        top_other_zones = sorted(other_zones,\n",
        "                            key=lambda z: len(z[1].get(\"objects\", [])),\n",
        "                            reverse=True)[:2]  # 最多2個其他區域\n",
        "\n",
        "        # 合併區域\n",
        "        top_zones = main_pedestrian_zones + top_other_zones\n",
        "\n",
        "        if not top_zones:\n",
        "            return \"\"\n",
        "\n",
        "        # 生成匯總描述\n",
        "        summary = \"\"\n",
        "        max_mentioned_people = 0  # 跟踪已經提到的最大人數\n",
        "\n",
        "        # 如果總人數顯著且還沒在主描述中提到，添加總人數描述\n",
        "        if total_people_count > 5:\n",
        "            summary = f\"The scene contains a significant number of pedestrians ({total_people_count} people). \"\n",
        "            max_mentioned_people = total_people_count  # 更新已提到的最大人數\n",
        "\n",
        "        # 處理每個區域的描述，確保人數信息的一致性\n",
        "        processed_zones = []\n",
        "\n",
        "        for zone_name, zone_info in top_zones:\n",
        "            zone_desc = zone_info.get(\"description\", \"a functional zone\")\n",
        "            zone_people_count = people_by_zone.get(zone_name, 0)\n",
        "\n",
        "            # 檢查描述中是否包含人數信息\n",
        "            contains_people_info = \"with\" in zone_desc and (\"person\" in zone_desc.lower() or \"people\" in zone_desc.lower())\n",
        "\n",
        "            # 如果描述包含人數信息，且人數較小（小於已提到的最大人數），則修改描述\n",
        "            if contains_people_info and zone_people_count < max_mentioned_people:\n",
        "                parts = zone_desc.split(\"with\")\n",
        "                if len(parts) > 1:\n",
        "                    # 移除人數部分\n",
        "                    zone_desc = parts[0].strip() + \" area\"\n",
        "\n",
        "            processed_zones.append((zone_name, {\"description\": zone_desc}))\n",
        "\n",
        "        # 根據處理後的區域數量生成最終描述\n",
        "        final_desc = \"\"\n",
        "\n",
        "        if len(processed_zones) == 1:\n",
        "            _, zone_info = processed_zones[0]\n",
        "            zone_desc = zone_info[\"description\"]\n",
        "            final_desc = summary + f\"The scene includes {zone_desc}.\"\n",
        "        elif len(processed_zones) == 2:\n",
        "            _, zone1_info = processed_zones[0]\n",
        "            _, zone2_info = processed_zones[1]\n",
        "            zone1_desc = zone1_info[\"description\"]\n",
        "            zone2_desc = zone2_info[\"description\"]\n",
        "            final_desc = summary + f\"The scene is divided into two main areas: {zone1_desc} and {zone2_desc}.\"\n",
        "        else:\n",
        "            zones_desc = [\"The scene contains multiple functional areas including\"]\n",
        "            zone_descriptions = [z[1][\"description\"] for z in processed_zones]\n",
        "\n",
        "            # 格式化最終的多區域描述\n",
        "            if len(zone_descriptions) == 3:\n",
        "                formatted_desc = f\"{zone_descriptions[0]}, {zone_descriptions[1]}, and {zone_descriptions[2]}\"\n",
        "            else:\n",
        "                formatted_desc = \", \".join(zone_descriptions[:-1]) + f\", and {zone_descriptions[-1]}\"\n",
        "\n",
        "            final_desc = summary + f\"{zones_desc[0]} {formatted_desc}.\"\n",
        "\n",
        "        return self._optimize_object_description(final_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWMzq6Va77Y8"
      },
      "outputs": [],
      "source": [
        "# %%writefile lighting_analyzer.py\n",
        "import numpy as np\n",
        "import cv2\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "class LightingAnalyzer:\n",
        "    \"\"\"\n",
        "    分析圖像的光照條件，提供增強的室內or室外判斷和光照類型分類，並專注於光照分析。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        初始化光照分析器。\n",
        "\n",
        "        Args:\n",
        "            config: 可選的配置字典，用於自定義分析參數\n",
        "        \"\"\"\n",
        "        self.config = config or self._get_default_config()\n",
        "\n",
        "    def analyze(self, image):\n",
        "        \"\"\"\n",
        "        分析圖像的光照條件。\n",
        "\n",
        "        主要分析入口點，計算基本特徵，判斷室內/室外，確定光照條件。\n",
        "\n",
        "        Args:\n",
        "            image: 輸入圖像 (numpy array 或 PIL Image)\n",
        "\n",
        "        Returns:\n",
        "            Dict: 包含光照分析結果的字典\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 轉換圖像格式\n",
        "            if not isinstance(image, np.ndarray):\n",
        "                image_np = np.array(image)\n",
        "            else:\n",
        "                image_np = image.copy()\n",
        "\n",
        "            # 確保 RGB 格式\n",
        "            if image_np.shape[2] == 3 and isinstance(image_np, np.ndarray):\n",
        "                image_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
        "            else:\n",
        "                image_rgb = image_np\n",
        "\n",
        "            # 計算基本特徵\n",
        "            features = self._compute_basic_features(image_rgb)\n",
        "\n",
        "            # 分析室內or室外\n",
        "            indoor_result = self._analyze_indoor_outdoor(features)\n",
        "            is_indoor = indoor_result[\"is_indoor\"]\n",
        "            indoor_probability = indoor_result[\"indoor_probability\"]\n",
        "\n",
        "            # 確定光照條件\n",
        "            lighting_conditions = self._determine_lighting_conditions(features, is_indoor)\n",
        "\n",
        "            # 整合結果\n",
        "            result = {\n",
        "                \"time_of_day\": lighting_conditions[\"time_of_day\"],\n",
        "                \"confidence\": float(lighting_conditions[\"confidence\"]),\n",
        "                \"is_indoor\": is_indoor,\n",
        "                \"indoor_probability\": float(indoor_probability),\n",
        "                \"brightness\": {\n",
        "                    \"average\": float(features[\"avg_brightness\"]),\n",
        "                    \"std_dev\": float(features[\"brightness_std\"]),\n",
        "                    \"dark_ratio\": float(features[\"dark_pixel_ratio\"])\n",
        "                },\n",
        "                \"color_info\": {\n",
        "                    \"blue_ratio\": float(features[\"blue_ratio\"]),\n",
        "                    \"yellow_orange_ratio\": float(features[\"yellow_orange_ratio\"]),\n",
        "                    \"gray_ratio\": float(features[\"gray_ratio\"]),\n",
        "                    \"avg_saturation\": float(features[\"avg_saturation\"]),\n",
        "                    \"sky_brightness\": float(features[\"sky_brightness\"]),\n",
        "                    \"color_atmosphere\": features[\"color_atmosphere\"],\n",
        "                    \"warm_ratio\": float(features[\"warm_ratio\"]),\n",
        "                    \"cool_ratio\": float(features[\"cool_ratio\"])\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # 添加診斷信息\n",
        "            if self.config[\"include_diagnostics\"]:\n",
        "                result[\"diagnostics\"] = {\n",
        "                    \"feature_contributions\": indoor_result.get(\"feature_contributions\", {}),\n",
        "                    \"lighting_diagnostics\": lighting_conditions.get(\"diagnostics\", {})\n",
        "                }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in lighting analysis: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"time_of_day\": \"unknown\",\n",
        "                \"confidence\": 0,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def _compute_basic_features(self, image_rgb):\n",
        "        \"\"\"\n",
        "        計算圖像的基本光照特徵（徹底優化版本）。\n",
        "\n",
        "        Args:\n",
        "            image_rgb: RGB 格式的圖像 (numpy array)\n",
        "\n",
        "        Returns:\n",
        "            Dict: 包含計算出的特徵值\n",
        "        \"\"\"\n",
        "        # 獲取圖像尺寸\n",
        "        height, width = image_rgb.shape[:2]\n",
        "\n",
        "        # 根據圖像大小自適應縮放因子\n",
        "        base_scale = 4\n",
        "        scale_factor = base_scale + min(8, max(0, int((height * width) / (1000 * 1000))))\n",
        "\n",
        "        # 創建縮小的圖像以加速處理\n",
        "        small_rgb = cv2.resize(image_rgb, (width//scale_factor, height//scale_factor))\n",
        "\n",
        "        # 一次性轉換所有顏色空間，避免重複計算\n",
        "        hsv_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
        "        gray_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
        "        small_gray = cv2.resize(gray_img, (width//scale_factor, height//scale_factor))\n",
        "\n",
        "        # 分離HSV通道\n",
        "        h_channel = hsv_img[:,:,0]\n",
        "        s_channel = hsv_img[:,:,1]\n",
        "        v_channel = hsv_img[:,:,2]\n",
        "\n",
        "        # 基本亮度特徵\n",
        "        avg_brightness = np.mean(v_channel)\n",
        "        brightness_std = np.std(v_channel)\n",
        "        dark_pixel_ratio = np.sum(v_channel < 50) / (height * width)\n",
        "\n",
        "        # 顏色特徵\n",
        "        yellow_orange_mask = ((h_channel >= 15) & (h_channel <= 40))\n",
        "        yellow_orange_ratio = np.sum(yellow_orange_mask) / (height * width)\n",
        "\n",
        "        blue_mask = ((h_channel >= 90) & (h_channel <= 130))\n",
        "        blue_ratio = np.sum(blue_mask) / (height * width)\n",
        "\n",
        "        # 特別檢查圖像上部區域，尋找藍天特徵\n",
        "        upper_region_h = h_channel[:height//4, :]\n",
        "        upper_region_s = s_channel[:height//4, :]\n",
        "        upper_region_v = v_channel[:height//4, :]\n",
        "\n",
        "        # 藍天通常具有高飽和度的藍色\n",
        "        sky_blue_mask = ((upper_region_h >= 90) & (upper_region_h <= 130) &\n",
        "                        (upper_region_s > 70) & (upper_region_v > 150))\n",
        "        sky_blue_ratio = np.sum(sky_blue_mask) / max(1, upper_region_h.size)\n",
        "\n",
        "        gray_mask = (s_channel < 50) & (v_channel > 100)\n",
        "        gray_ratio = np.sum(gray_mask) / (height * width)\n",
        "\n",
        "        avg_saturation = np.mean(s_channel)\n",
        "\n",
        "        # 天空亮度\n",
        "        upper_half = v_channel[:height//2, :]\n",
        "        sky_brightness = np.mean(upper_half)\n",
        "\n",
        "        # 色調分析\n",
        "        warm_colors = ((h_channel >= 0) & (h_channel <= 60)) | (h_channel >= 300)\n",
        "        warm_ratio = np.sum(warm_colors) / (height * width)\n",
        "\n",
        "        cool_colors = (h_channel >= 180) & (h_channel <= 270)\n",
        "        cool_ratio = np.sum(cool_colors) / (height * width)\n",
        "\n",
        "        # 確定色彩氛圍\n",
        "        if warm_ratio > 0.4:\n",
        "            color_atmosphere = \"warm\"\n",
        "        elif cool_ratio > 0.4:\n",
        "            color_atmosphere = \"cool\"\n",
        "        else:\n",
        "            color_atmosphere = \"neutral\"\n",
        "\n",
        "        # 只在縮小的圖像上計算梯度，大幅提高效能\n",
        "        gx = cv2.Sobel(small_gray, cv2.CV_32F, 1, 0, ksize=3)\n",
        "        gy = cv2.Sobel(small_gray, cv2.CV_32F, 0, 1, ksize=3)\n",
        "\n",
        "        vertical_strength = np.mean(np.abs(gy))\n",
        "        horizontal_strength = np.mean(np.abs(gx))\n",
        "        gradient_ratio = vertical_strength / max(horizontal_strength, 1e-5)\n",
        "\n",
        "        # -- 亮度均勻性 --\n",
        "        brightness_uniformity = 1 - min(1, brightness_std / max(avg_brightness, 1e-5))\n",
        "\n",
        "        # -- 高效的天花板分析 --\n",
        "        # 使用更大的下採樣率分析頂部區域\n",
        "        top_scale = scale_factor * 2  # 更積極的下採樣\n",
        "        top_region = v_channel[:height//4:top_scale, ::top_scale]\n",
        "        top_region_std = np.std(top_region)\n",
        "        ceiling_uniformity = 1.0 - min(1, top_region_std / max(np.mean(top_region), 1e-5))\n",
        "\n",
        "        # 使用更簡單的方法檢測上部水平線\n",
        "        top_gradients = np.abs(gy[:small_gray.shape[0]//4, :])\n",
        "        horizontal_lines_strength = np.mean(top_gradients)\n",
        "        # 標準化\n",
        "        horizontal_line_ratio = min(1, horizontal_lines_strength / 40)\n",
        "\n",
        "        # 極簡的亮點檢測\n",
        "        sampled_v = v_channel[::scale_factor*2, ::scale_factor*2]\n",
        "        light_threshold = min(220, avg_brightness + 2*brightness_std)\n",
        "        is_bright = sampled_v > light_threshold\n",
        "        bright_spot_count = np.sum(is_bright)\n",
        "\n",
        "        # 圓形光源分析的簡化替代方法\n",
        "        circular_light_score = 0\n",
        "        indoor_light_score = 0\n",
        "        light_distribution_uniformity = 0.5\n",
        "\n",
        "        # 只有當檢測到亮點，且不是大量亮點時（可能是室外光反射）才進行光源分析\n",
        "        if 1 < bright_spot_count < 20:\n",
        "            # 簡單統計亮點分布\n",
        "            bright_y, bright_x = np.where(is_bright)\n",
        "            if len(bright_y) > 1:\n",
        "                # 檢查亮點是否成組出現 - 室內照明常見模式\n",
        "                mean_x = np.mean(bright_x)\n",
        "                mean_y = np.mean(bright_y)\n",
        "                dist_from_center = np.sqrt((bright_x - mean_x)**2 + (bright_y - mean_y)**2)\n",
        "\n",
        "                # 如果亮點分布較集中，可能是燈具\n",
        "                if np.std(dist_from_center) < np.mean(dist_from_center):\n",
        "                    circular_light_score = min(3, len(bright_y) // 2)\n",
        "                    light_distribution_uniformity = 0.7\n",
        "\n",
        "                # 評估亮點是否位於上部區域，常見於室內頂燈\n",
        "                if np.mean(bright_y) < sampled_v.shape[0] / 2:\n",
        "                    indoor_light_score = 0.6\n",
        "                else:\n",
        "                    indoor_light_score = 0.3\n",
        "\n",
        "        # 使用邊緣區域梯度來快速估計邊界\n",
        "        edge_scale = scale_factor * 2\n",
        "\n",
        "        # 只採樣圖像邊緣部分進行分析\n",
        "        left_edge = small_gray[:, :small_gray.shape[1]//6]\n",
        "        right_edge = small_gray[:, 5*small_gray.shape[1]//6:]\n",
        "        top_edge = small_gray[:small_gray.shape[0]//6, :]\n",
        "\n",
        "        # 計算每個邊緣區域的梯度強度\n",
        "        left_gradient = np.mean(np.abs(cv2.Sobel(left_edge, cv2.CV_32F, 1, 0, ksize=3)))\n",
        "        right_gradient = np.mean(np.abs(cv2.Sobel(right_edge, cv2.CV_32F, 1, 0, ksize=3)))\n",
        "        top_gradient = np.mean(np.abs(cv2.Sobel(top_edge, cv2.CV_32F, 0, 1, ksize=3)))\n",
        "\n",
        "        # 標準化\n",
        "        left_edge_density = min(1, left_gradient / 50)\n",
        "        right_edge_density = min(1, right_gradient / 50)\n",
        "        top_edge_density = min(1, top_gradient / 50)\n",
        "\n",
        "        # 封閉環境通常在圖像邊緣有較強的梯度\n",
        "        boundary_edge_score = (left_edge_density + right_edge_density + top_edge_density) / 3\n",
        "\n",
        "        # 簡單估計整體邊緣密度\n",
        "        edges_density = min(1, (np.mean(np.abs(gx)) + np.mean(np.abs(gy))) / 100)\n",
        "\n",
        "        street_line_score = 0\n",
        "\n",
        "        # 檢查下半部分是否有強烈的垂直線條\n",
        "        bottom_half = small_gray[small_gray.shape[0]//2:, :]\n",
        "        bottom_vert_gradient = cv2.Sobel(bottom_half, cv2.CV_32F, 0, 1, ksize=3)\n",
        "        strong_vert_lines = np.abs(bottom_vert_gradient) > 50\n",
        "        if np.sum(strong_vert_lines) > (bottom_half.size * 0.05):  # 如果超過5%的像素是強垂直線\n",
        "            street_line_score = 0.7\n",
        "\n",
        "        # 整合所有特徵\n",
        "        features = {\n",
        "            # 基本亮度和顏色特徵\n",
        "            \"avg_brightness\": avg_brightness,\n",
        "            \"brightness_std\": brightness_std,\n",
        "            \"dark_pixel_ratio\": dark_pixel_ratio,\n",
        "            \"yellow_orange_ratio\": yellow_orange_ratio,\n",
        "            \"blue_ratio\": blue_ratio,\n",
        "            \"sky_blue_ratio\": sky_blue_ratio,\n",
        "            \"gray_ratio\": gray_ratio,\n",
        "            \"avg_saturation\": avg_saturation,\n",
        "            \"sky_brightness\": sky_brightness,\n",
        "            \"color_atmosphere\": color_atmosphere,\n",
        "            \"warm_ratio\": warm_ratio,\n",
        "            \"cool_ratio\": cool_ratio,\n",
        "\n",
        "            # 結構特徵\n",
        "            \"gradient_ratio\": gradient_ratio,\n",
        "            \"brightness_uniformity\": brightness_uniformity,\n",
        "            \"bright_spot_count\": bright_spot_count,\n",
        "            \"vertical_strength\": vertical_strength,\n",
        "            \"horizontal_strength\": horizontal_strength,\n",
        "\n",
        "            # 室內/室外判斷特徵\n",
        "            \"ceiling_uniformity\": ceiling_uniformity,\n",
        "            \"horizontal_line_ratio\": horizontal_line_ratio,\n",
        "            \"indoor_light_score\": indoor_light_score,\n",
        "            \"circular_light_count\": circular_light_score,\n",
        "            \"light_distribution_uniformity\": light_distribution_uniformity,\n",
        "            \"boundary_edge_score\": boundary_edge_score,\n",
        "            \"top_region_std\": top_region_std,\n",
        "            \"edges_density\": edges_density,\n",
        "\n",
        "            # 室外特定特徵\n",
        "            \"street_line_score\": street_line_score\n",
        "        }\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _analyze_indoor_outdoor(self, features):\n",
        "        \"\"\"\n",
        "        使用多特徵融合進行室內/室外判斷\n",
        "\n",
        "        Args:\n",
        "            features: 特徵字典\n",
        "\n",
        "        Returns:\n",
        "            Dict: 室內/室外判斷結果\n",
        "        \"\"\"\n",
        "        # 獲取配置中的特徵權重\n",
        "        weights = self.config[\"indoor_outdoor_weights\"]\n",
        "\n",
        "        # 初始概率值 - 開始時中性評估\n",
        "        indoor_score = 0\n",
        "        feature_contributions = {}\n",
        "        diagnostics = {}\n",
        "\n",
        "        # 1. 藍色區域（天空）特徵 - 藍色區域多通常表示室外\n",
        "        if features.get(\"blue_ratio\", 0) > 0.2:\n",
        "            # 檢查是否有室內指標，如果有明顯的室內特徵，則減少藍色的負面影響\n",
        "            if (features.get(\"ceiling_uniformity\", 0) > 0.5 or\n",
        "                features.get(\"boundary_edge_score\", 0) > 0.3 or\n",
        "                features.get(\"indoor_light_score\", 0) > 0.2 or\n",
        "                features.get(\"bright_spot_count\", 0) > 0):\n",
        "                blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 8\n",
        "            else:\n",
        "                blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 15\n",
        "        else:\n",
        "            blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 15\n",
        "\n",
        "        indoor_score += blue_score\n",
        "        feature_contributions[\"blue_ratio\"] = blue_score\n",
        "\n",
        "        # 判斷視角 - 如果上部有藍天而上下亮度差異大，可能是仰視室外建築\n",
        "        if (features.get(\"sky_blue_ratio\", 0) > 0.01 and\n",
        "            features[\"sky_brightness\"] > features[\"avg_brightness\"] * 1.1):\n",
        "            viewpoint_outdoor_score = -1.8  # 強烈的室外指標\n",
        "            indoor_score += viewpoint_outdoor_score\n",
        "            feature_contributions[\"outdoor_viewpoint\"] = viewpoint_outdoor_score\n",
        "\n",
        "        # 2. 亮度均勻性特徵 - 室內通常光照更均勻\n",
        "        uniformity_score = weights[\"brightness_uniformity\"] * features[\"brightness_uniformity\"]\n",
        "        indoor_score += uniformity_score\n",
        "        feature_contributions[\"brightness_uniformity\"] = uniformity_score\n",
        "\n",
        "        # 3. 天花板特徵 - 強化天花板檢測的權重\n",
        "        ceiling_contribution = 0\n",
        "        if \"ceiling_uniformity\" in features:\n",
        "            ceiling_uniformity = features[\"ceiling_uniformity\"]\n",
        "            horizontal_line_ratio = features.get(\"horizontal_line_ratio\", 0)\n",
        "\n",
        "            # 增強天花板檢測的影響\n",
        "            if ceiling_uniformity > 0.5:\n",
        "                ceiling_weight = 3\n",
        "                ceiling_contribution = weights.get(\"ceiling_features\", 1.5) * ceiling_weight\n",
        "                if horizontal_line_ratio > 0.2:  # 如果有水平線條，進一步增強\n",
        "                    ceiling_contribution *= 1.5\n",
        "            elif ceiling_uniformity > 0.4:\n",
        "                ceiling_contribution = weights.get(\"ceiling_features\", 1.5) * 1.2\n",
        "\n",
        "            indoor_score += ceiling_contribution\n",
        "            feature_contributions[\"ceiling_features\"] = ceiling_contribution\n",
        "\n",
        "        # 4. 強化吊燈的檢測\n",
        "        light_contribution = 0\n",
        "        if \"indoor_light_score\" in features:\n",
        "            indoor_light_score = features[\"indoor_light_score\"]\n",
        "            circular_light_count = features.get(\"circular_light_count\", 0)\n",
        "\n",
        "            # 加強對特定類型光源的檢測\n",
        "            if circular_light_count >= 1:  # 即便只有一個圓形光源也很可能是室內\n",
        "                light_contribution = weights.get(\"light_features\", 1.2) * 2.0\n",
        "            elif indoor_light_score > 0.3:\n",
        "                light_contribution = weights.get(\"light_features\", 1.2) * 1.0\n",
        "\n",
        "            indoor_score += light_contribution\n",
        "            feature_contributions[\"light_features\"] = light_contribution\n",
        "\n",
        "        # 5. 環境封閉度特徵\n",
        "        boundary_contribution = 0\n",
        "        if \"boundary_edge_score\" in features:\n",
        "            boundary_edge_score = features[\"boundary_edge_score\"]\n",
        "            edges_density = features.get(\"edges_density\", 0)\n",
        "\n",
        "            # 高邊界評分暗示封閉環境（室內）\n",
        "            if boundary_edge_score > 0.3:\n",
        "                boundary_contribution = weights.get(\"boundary_features\", 1.2) * 2\n",
        "            elif boundary_edge_score > 0.2:\n",
        "                boundary_contribution = weights.get(\"boundary_features\", 1.2) * 1.2\n",
        "\n",
        "            indoor_score += boundary_contribution\n",
        "            feature_contributions[\"boundary_features\"] = boundary_contribution\n",
        "\n",
        "        if (features.get(\"edges_density\", 0) > 0.2 and\n",
        "            features.get(\"bright_spot_count\", 0) > 5 and\n",
        "            features.get(\"vertical_strength\", 0) > features.get(\"horizontal_strength\", 0) * 1.5):\n",
        "            # 商業街道特徵：高邊緣密度 + 多亮點 + 強垂直特徵\n",
        "            street_feature_score = -weights.get(\"street_features\", 1.2) * 1.5\n",
        "            indoor_score += street_feature_score\n",
        "            feature_contributions[\"street_features\"] = street_feature_score\n",
        "\n",
        "        # 添加對亞洲商業街道的專門檢測\n",
        "        if (features.get(\"edges_density\", 0) > 0.25 and  # 高邊緣密度\n",
        "            features.get(\"vertical_strength\", 0) > features.get(\"horizontal_strength\", 0) * 1.8 and  # 更強的垂直結構\n",
        "            features.get(\"brightness_uniformity\", 0) < 0.6):  # 較低的亮度均勻性（招牌、燈光等造成）\n",
        "            asian_street_score = -2.2  # 非常強的室外代表性特徵\n",
        "            indoor_score += asian_street_score\n",
        "            feature_contributions[\"asian_commercial_street\"] = asian_street_score\n",
        "\n",
        "\n",
        "        # 6. 垂直/水平梯度比率\n",
        "        gradient_contribution = 0\n",
        "        if features[\"gradient_ratio\"] > 2.0:\n",
        "            combined_uniformity = (features[\"brightness_uniformity\"] +\n",
        "                                features.get(\"ceiling_uniformity\", 0)) / 2\n",
        "\n",
        "            if combined_uniformity > 0.5:\n",
        "                gradient_contribution = weights[\"gradient_ratio\"] * 0.7\n",
        "            else:\n",
        "                gradient_contribution = -weights[\"gradient_ratio\"] * 0.3\n",
        "\n",
        "            indoor_score += gradient_contribution\n",
        "            feature_contributions[\"gradient_ratio\"] = gradient_contribution\n",
        "\n",
        "        # 7. 亮點檢測（光源）\n",
        "        bright_spot_contribution = 0\n",
        "        bright_spot_count = features[\"bright_spot_count\"]\n",
        "        circular_light_count = features.get(\"circular_light_count\", 0)\n",
        "\n",
        "        # 調整亮點分析邏輯\n",
        "        if circular_light_count >= 1:  # 即使只有一個圓形光源\n",
        "            bright_spot_contribution = weights[\"bright_spots\"] * 1.5\n",
        "        elif bright_spot_count < 5:  # 適當放寬閾值\n",
        "            bright_spot_contribution = weights[\"bright_spots\"] * 0.5\n",
        "        elif bright_spot_count > 15:  # 大量亮點比較有可能為室外\n",
        "            bright_spot_contribution = -weights[\"bright_spots\"] * 0.4\n",
        "\n",
        "        indoor_score += bright_spot_contribution\n",
        "        feature_contributions[\"bright_spots\"] = bright_spot_contribution\n",
        "\n",
        "        # 8. 色調分析\n",
        "        yellow_contribution = 0\n",
        "        if features[\"avg_brightness\"] < 150 and features[\"yellow_orange_ratio\"] > 0.15:\n",
        "            if features.get(\"indoor_light_score\", 0) > 0.2:\n",
        "                yellow_contribution = weights[\"color_tone\"] * 0.8\n",
        "            else:\n",
        "                yellow_contribution = weights[\"color_tone\"] * 0.5\n",
        "\n",
        "            indoor_score += yellow_contribution\n",
        "            feature_contributions[\"yellow_tone\"] = yellow_contribution\n",
        "\n",
        "        if features.get(\"blue_ratio\", 0) > 0.7:\n",
        "            # 檢查是否有室內指標，如果有明顯的室內特徵，則減少藍色的負面影響\n",
        "            if (features.get(\"ceiling_uniformity\", 0) > 0.6 or\n",
        "                features.get(\"boundary_edge_score\", 0) > 0.3 or\n",
        "                features.get(\"indoor_light_score\", 0) > 0):\n",
        "                blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 10\n",
        "            else:\n",
        "                blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 18\n",
        "        else:\n",
        "            blue_score = -weights[\"blue_ratio\"] * features[\"blue_ratio\"] * 18\n",
        "        # 9. 上半部與下半部亮度對比\n",
        "        sky_contribution = 0\n",
        "        if features[\"sky_brightness\"] > features[\"avg_brightness\"] * 1.3:\n",
        "            if features[\"blue_ratio\"] > 0.15:\n",
        "                sky_contribution = -weights[\"sky_brightness\"] * 0.9\n",
        "            else:\n",
        "                sky_contribution = -weights[\"sky_brightness\"] * 0.6\n",
        "\n",
        "            indoor_score += sky_contribution\n",
        "            feature_contributions[\"sky_brightness\"] = sky_contribution\n",
        "\n",
        "        # 加入額外的餐廳特徵檢測邏輯\n",
        "        dining_feature_contribution = 0\n",
        "\n",
        "        # 檢測中央懸掛式燈具，有懸掛燈代表有天花板，就代表是室內\n",
        "        if circular_light_count >= 1 and features.get(\"light_distribution_uniformity\", 0) > 0.4:\n",
        "            dining_feature_contribution = 1.5\n",
        "            indoor_score += dining_feature_contribution\n",
        "            feature_contributions[\"dining_features\"] = dining_feature_contribution\n",
        "\n",
        "        # 10. 增強的藍天的檢測，即便是小面積的藍天也是很強的室外指標\n",
        "        sky_contribution = 0\n",
        "        if \"sky_blue_ratio\" in features:\n",
        "            # 只有當藍色區域集中在上部且亮度高時，才認為是藍天\n",
        "            if features[\"sky_blue_ratio\"] > 0.01 and features[\"sky_brightness\"] > features.get(\"avg_brightness\", 0) * 1.2:\n",
        "                sky_outdoor_score = -2.5 * features[\"sky_blue_ratio\"] * weights.get(\"blue_ratio\", 1.2)\n",
        "                indoor_score += sky_outdoor_score\n",
        "                feature_contributions[\"sky_blue_detection\"] = sky_outdoor_score\n",
        "\n",
        "        asian_street_indicators = 0\n",
        "\n",
        "        # 1: 高垂直結構強度\n",
        "        vertical_ratio = features.get(\"vertical_strength\", 0) / max(features.get(\"horizontal_strength\", 1e-5), 1e-5)\n",
        "        if vertical_ratio > 1.8:\n",
        "            asian_street_indicators += 1\n",
        "\n",
        "        # 2: 高邊緣密度 + 路面標記特徵\n",
        "        if features.get(\"edges_density\", 0) > 0.25 and features.get(\"street_line_score\", 0) > 0.2:\n",
        "            asian_street_indicators += 2\n",
        "\n",
        "        # 3: 多個亮點 + 亮度不均勻\n",
        "        if features.get(\"bright_spot_count\", 0) > 5 and features.get(\"brightness_uniformity\", 0) < 0.6:\n",
        "            asian_street_indicators += 1\n",
        "\n",
        "        # 4: 藍色區域小（天空被高樓遮擋）但亮度高\n",
        "        if features.get(\"blue_ratio\", 0) < 0.1 and features.get(\"sky_brightness\", 0) > features.get(\"avg_brightness\", 0) * 1.1:\n",
        "            asian_street_indicators += 1\n",
        "\n",
        "        # 如果滿足至少 3 個指標，調整權重變成偏向室外的判斷\n",
        "        if asian_street_indicators >= 3:\n",
        "            # 記錄檢測到的模式\n",
        "            feature_contributions[\"asian_street_pattern\"] = -2.5\n",
        "            indoor_score += -2.5  # 明顯向室外傾斜\n",
        "\n",
        "            # 降低室內指標的權重\n",
        "            if \"boundary_features\" in feature_contributions:\n",
        "                adjusted_contribution = feature_contributions[\"boundary_features\"] * 0.4\n",
        "                indoor_score -= (feature_contributions[\"boundary_features\"] - adjusted_contribution)\n",
        "                feature_contributions[\"boundary_features\"] = adjusted_contribution\n",
        "\n",
        "            if \"ceiling_features\" in feature_contributions:\n",
        "                adjusted_contribution = feature_contributions[\"ceiling_features\"] * 0.3\n",
        "                indoor_score -= (feature_contributions[\"ceiling_features\"] - adjusted_contribution)\n",
        "                feature_contributions[\"ceiling_features\"] = adjusted_contribution\n",
        "\n",
        "            # 添加信息到診斷數據\n",
        "            diagnostics[\"asian_street_detected\"] = True\n",
        "            diagnostics[\"asian_street_indicators\"] = asian_street_indicators\n",
        "\n",
        "        bedroom_indicators = 0\n",
        "\n",
        "        # 1: 窗戶和牆壁形成的直角\n",
        "        if features.get(\"brightness_uniformity\", 0) > 0.6 and features.get(\"boundary_edge_score\", 0) > 0.3:\n",
        "            bedroom_indicators += 1.5  # 增加權重\n",
        "\n",
        "        # 2: 天花板和光源\n",
        "        if features.get(\"ceiling_uniformity\", 0) > 0.5 and features.get(\"bright_spot_count\", 0) > 0:\n",
        "            bedroom_indicators += 2.5\n",
        "\n",
        "        # 3: 良好對比度的牆壁顏色，適合臥房還有客廳\n",
        "        if features.get(\"brightness_uniformity\", 0) > 0.6 and features.get(\"avg_saturation\", 0) < 100:\n",
        "            bedroom_indicators += 1.5\n",
        "\n",
        "        # 特殊的檢測 4: 檢測窗戶\n",
        "        if features.get(\"boundary_edge_score\", 0) > 0.25 and features.get(\"brightness_std\", 0) > 40:\n",
        "            bedroom_indicators += 1.5\n",
        "\n",
        "        # 如果滿足足夠的家居指標，提高多點室內判斷分數\n",
        "        if bedroom_indicators >= 3:\n",
        "            # 增加家居環境評分\n",
        "            home_env_score = 3\n",
        "            indoor_score += home_env_score\n",
        "            feature_contributions[\"home_environment_pattern\"] = home_env_score\n",
        "        elif bedroom_indicators >= 2:\n",
        "            # 適度增加家居環境評分\n",
        "            home_env_score = 2\n",
        "            indoor_score += home_env_score\n",
        "            feature_contributions[\"home_environment_pattern\"] = home_env_score\n",
        "\n",
        "        # 根據總分轉換為概率（使用sigmoid函數）\n",
        "        indoor_probability = 1 / (1 + np.exp(-indoor_score * 0.22))\n",
        "\n",
        "        # 判斷結果\n",
        "        is_indoor = indoor_probability > 0.5\n",
        "\n",
        "        return {\n",
        "            \"is_indoor\": is_indoor,\n",
        "            \"indoor_probability\": indoor_probability,\n",
        "            \"indoor_score\": indoor_score,\n",
        "            \"feature_contributions\": feature_contributions,\n",
        "            \"diagnostics\": diagnostics\n",
        "        }\n",
        "\n",
        "    def _determine_lighting_conditions(self, features, is_indoor):\n",
        "        \"\"\"\n",
        "        基於特徵和室內/室外判斷確定光照條件。\n",
        "\n",
        "        Args:\n",
        "            features: 特徵字典\n",
        "            is_indoor: 是否是室內環境\n",
        "\n",
        "        Returns:\n",
        "            Dict: 光照條件分析結果\n",
        "        \"\"\"\n",
        "        # 初始化\n",
        "        time_of_day = \"unknown\"\n",
        "        confidence = 0.5\n",
        "        diagnostics = {}\n",
        "\n",
        "        avg_brightness = features[\"avg_brightness\"]\n",
        "        dark_pixel_ratio = features[\"dark_pixel_ratio\"]\n",
        "        yellow_orange_ratio = features[\"yellow_orange_ratio\"]\n",
        "        blue_ratio = features[\"blue_ratio\"]\n",
        "        gray_ratio = features[\"gray_ratio\"]\n",
        "\n",
        "        # 基於室內/室外分別判斷\n",
        "        if is_indoor:\n",
        "            # 計算室內住宅自然光指標\n",
        "            natural_window_light = 0\n",
        "\n",
        "            # 檢查窗戶特徵和光線特性\n",
        "            if (features.get(\"blue_ratio\", 0) > 0.1 and\n",
        "                features.get(\"sky_brightness\", 0) > avg_brightness * 1.1):\n",
        "                natural_window_light += 1\n",
        "\n",
        "            # 檢查均勻柔和的光線分布\n",
        "            if (features.get(\"brightness_uniformity\", 0) > 0.65 and\n",
        "                features.get(\"brightness_std\", 0) < 70):\n",
        "                natural_window_light += 1\n",
        "\n",
        "            # 檢查暖色調比例\n",
        "            if features.get(\"warm_ratio\", 0) > 0.2:\n",
        "                natural_window_light += 1\n",
        "\n",
        "            # 家居環境指標\n",
        "            home_env_score = features.get(\"home_environment_pattern\", 0)\n",
        "            if home_env_score > 1.5:\n",
        "                natural_window_light += 1\n",
        "\n",
        "            # 1. 室內明亮環境，可能有窗戶自然光\n",
        "            if avg_brightness > 130:\n",
        "                # 檢測自然光住宅空間 - 新增類型!\n",
        "                if natural_window_light >= 2 and home_env_score > 1.5:\n",
        "                    time_of_day = \"indoor_residential_natural\"  # 家裡的自然光類型\n",
        "                    confidence = 0.8\n",
        "                    diagnostics[\"reason\"] = \"Bright residential space with natural window lighting\"\n",
        "                # 檢查窗戶特徵 - 如果有明亮的窗戶且色調為藍\n",
        "                elif features.get(\"blue_ratio\", 0) > 0.1 and features.get(\"sky_brightness\", 0) > 150:\n",
        "                    time_of_day = \"indoor_bright\"\n",
        "                    confidence = 0.8\n",
        "                    diagnostics[\"reason\"] = \"Bright indoor scene with window light\"\n",
        "                else:\n",
        "                    time_of_day = \"indoor_bright\"\n",
        "                    confidence = 0.75\n",
        "                    diagnostics[\"reason\"] = \"High brightness in indoor environment\"\n",
        "            # 2. 室內中等亮度環境\n",
        "            elif avg_brightness > 100:\n",
        "                time_of_day = \"indoor_moderate\"\n",
        "                confidence = 0.7\n",
        "                diagnostics[\"reason\"] = \"Moderate brightness in indoor environment\"\n",
        "            # 3. 室內低光照環境\n",
        "            else:\n",
        "                time_of_day = \"indoor_dim\"\n",
        "                confidence = 0.65 + dark_pixel_ratio / 3\n",
        "                diagnostics[\"reason\"] = \"Low brightness in indoor environment\"\n",
        "\n",
        "            # 1. 檢測設計師風格住宅，可以偵測到比較多種類的狀況\n",
        "            designer_residential_score = 0\n",
        "            # 檢測特色燈具\n",
        "            if (features.get(\"circular_light_count\", 0) > 0 or features.get(\"bright_spot_count\", 0) > 2):\n",
        "                designer_residential_score += 1\n",
        "            # 檢測高品質均勻照明\n",
        "            if features.get(\"brightness_uniformity\", 0) > 0.7:\n",
        "                designer_residential_score += 1\n",
        "            # 檢測溫暖色調\n",
        "            if features.get(\"warm_ratio\", 0) > 0.3:\n",
        "                designer_residential_score += 1\n",
        "            # 檢測家居環境特徵\n",
        "            if home_env_score > 1.5:\n",
        "                designer_residential_score += 1\n",
        "\n",
        "            if designer_residential_score >= 3 and home_env_score > 1.5:\n",
        "                time_of_day = \"indoor_designer_residential\"\n",
        "                confidence = 0.85\n",
        "                diagnostics[\"special_case\"] = \"Designer residential lighting with decorative elements\"\n",
        "\n",
        "            # 2. 檢測餐廳/酒吧場景\n",
        "            elif avg_brightness < 150 and yellow_orange_ratio > 0.2:\n",
        "                if features[\"warm_ratio\"] > 0.4:\n",
        "                    time_of_day = \"indoor_restaurant\"\n",
        "                    confidence = 0.65 + yellow_orange_ratio / 4\n",
        "                    diagnostics[\"special_case\"] = \"Warm, yellow-orange lighting suggests restaurant/bar setting\"\n",
        "\n",
        "            # 3. 檢測商業照明空間\n",
        "            elif avg_brightness > 120 and features[\"bright_spot_count\"] > 4:\n",
        "                # 增加商業照明判別的精確度\n",
        "                commercial_score = 0\n",
        "                # 多個亮點\n",
        "                commercial_score += min(1.0, features[\"bright_spot_count\"] * 0.05)\n",
        "                # 不太可能是住宅的指標\n",
        "                if features.get(\"home_environment_pattern\", 0) < 1.5:\n",
        "                    commercial_score += 0.5\n",
        "                # 整體照明結構化布局\n",
        "                if features.get(\"light_distribution_uniformity\", 0) > 0.6:\n",
        "                    commercial_score += 0.5\n",
        "\n",
        "                if commercial_score > 0.6 and designer_residential_score < 3:\n",
        "                    time_of_day = \"indoor_commercial\"\n",
        "                    confidence = 0.7 + commercial_score / 5\n",
        "                    diagnostics[\"special_case\"] = \"Multiple structured light sources suggest commercial lighting\"\n",
        "        else:\n",
        "            # 室外場景判斷保持不變\n",
        "            if avg_brightness < 90:  # 降低夜間判斷的亮度閾值\n",
        "                # 檢測是否有車燈/街燈\n",
        "                has_lights = features[\"bright_spot_count\"] > 3\n",
        "\n",
        "                if has_lights:\n",
        "                    time_of_day = \"night\"\n",
        "                    confidence = 0.8 + dark_pixel_ratio / 5\n",
        "                    diagnostics[\"reason\"] = \"Low brightness with light sources detected\"\n",
        "\n",
        "                    # 檢查是否是霓虹燈場景\n",
        "                    if yellow_orange_ratio > 0.15 and features[\"bright_spot_count\"] > 5:\n",
        "                        time_of_day = \"neon_night\"\n",
        "                        confidence = 0.75 + yellow_orange_ratio / 3\n",
        "                        diagnostics[\"special_case\"] = \"Multiple colorful light sources suggest neon lighting\"\n",
        "                else:\n",
        "                    time_of_day = \"night\"\n",
        "                    confidence = 0.7 + dark_pixel_ratio / 3\n",
        "                    diagnostics[\"reason\"] = \"Low brightness outdoor scene\"\n",
        "            elif avg_brightness < 130 and yellow_orange_ratio > 0.2:\n",
        "                time_of_day = \"sunset/sunrise\"\n",
        "                confidence = 0.7 + yellow_orange_ratio / 3\n",
        "                diagnostics[\"reason\"] = \"Moderate brightness with yellow-orange tones\"\n",
        "            elif avg_brightness > 150 and blue_ratio > 0.15:\n",
        "                time_of_day = \"day_clear\"\n",
        "                confidence = 0.7 + blue_ratio / 3\n",
        "                diagnostics[\"reason\"] = \"High brightness with blue tones (likely sky)\"\n",
        "            elif avg_brightness > 130:\n",
        "                time_of_day = \"day_cloudy\"\n",
        "                confidence = 0.7 + gray_ratio / 3\n",
        "                diagnostics[\"reason\"] = \"Good brightness with higher gray tones\"\n",
        "            else:\n",
        "                # 默認判斷\n",
        "                if yellow_orange_ratio > gray_ratio:\n",
        "                    time_of_day = \"sunset/sunrise\"\n",
        "                    confidence = 0.6 + yellow_orange_ratio / 3\n",
        "                    diagnostics[\"reason\"] = \"Yellow-orange tones dominant\"\n",
        "                else:\n",
        "                    time_of_day = \"day_cloudy\"\n",
        "                    confidence = 0.6 + gray_ratio / 3\n",
        "                    diagnostics[\"reason\"] = \"Gray tones dominant\"\n",
        "\n",
        "            # 檢查是否是特殊室外場景（如體育場）\n",
        "            if avg_brightness > 120 and features[\"brightness_uniformity\"] > 0.8:\n",
        "                # 高亮度且非常均勻的光照可能是體育場燈光\n",
        "                time_of_day = \"stadium_lighting\"\n",
        "                confidence = 0.7\n",
        "                diagnostics[\"special_case\"] = \"Uniform bright lighting suggests stadium/sports lighting\"\n",
        "\n",
        "            # 檢查是否是混合光照（如室內/室外過渡區）\n",
        "            if 100 < avg_brightness < 150 and 0.1 < blue_ratio < 0.2:\n",
        "                if features[\"gradient_ratio\"] > 1.5:\n",
        "                    time_of_day = \"mixed_lighting\"\n",
        "                    confidence = 0.65\n",
        "                    diagnostics[\"special_case\"] = \"Features suggest indoor-outdoor transition area\"\n",
        "\n",
        "        # 確保信心值在 0-1 範圍內\n",
        "        confidence = min(0.95, max(0.5, confidence))\n",
        "\n",
        "        if time_of_day in [\"indoor_residential_natural\", \"indoor_designer_residential\"] and hasattr(self, \"config\"):\n",
        "            # 確保 LIGHTING_CONDITIONS 中有這些新類型的描述\n",
        "            if time_of_day == \"indoor_residential_natural\":\n",
        "                lightingType = {\n",
        "                    \"template_modifiers\": {\n",
        "                        \"indoor_residential_natural\": \"naturally-lit residential\"\n",
        "                    },\n",
        "                    \"time_descriptions\": {\n",
        "                        \"indoor_residential_natural\": {\n",
        "                            \"general\": \"The scene is captured in a residential space with ample natural light from windows.\",\n",
        "                            \"bright\": \"The residential space is brightly lit with natural daylight streaming through windows.\",\n",
        "                            \"medium\": \"The home environment has good natural lighting providing a warm, inviting atmosphere.\",\n",
        "                            \"dim\": \"The living space has soft natural light filtering through windows or openings.\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            elif time_of_day == \"indoor_designer_residential\":\n",
        "                lightingType = {\n",
        "                    \"template_modifiers\": {\n",
        "                        \"indoor_designer_residential\": \"designer-lit residential\"\n",
        "                    },\n",
        "                    \"time_descriptions\": {\n",
        "                        \"indoor_designer_residential\": {\n",
        "                            \"general\": \"The scene is captured in a residential space with carefully designed lighting elements.\",\n",
        "                            \"bright\": \"The home features professionally designed lighting with decorative fixtures creating a bright atmosphere.\",\n",
        "                            \"medium\": \"The residential interior showcases curated lighting design balancing form and function.\",\n",
        "                            \"dim\": \"The living space has thoughtfully placed designer lighting creating an intimate ambiance.\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            \"time_of_day\": time_of_day,\n",
        "            \"confidence\": confidence,\n",
        "            \"diagnostics\": diagnostics\n",
        "        }\n",
        "\n",
        "\n",
        "    def _get_default_config(self):\n",
        "        \"\"\"\n",
        "        返回優化版本的默認配置參數。\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"indoor_outdoor_weights\": {\n",
        "                \"blue_ratio\": 0.6,\n",
        "                \"brightness_uniformity\": 1.2,\n",
        "                \"gradient_ratio\": 0.7,\n",
        "                \"bright_spots\": 0.8,\n",
        "                \"color_tone\": 0.5,\n",
        "                \"sky_brightness\": 0.9,\n",
        "                \"brightness_variation\": 0.7,\n",
        "                \"ceiling_features\": 1.5,\n",
        "                \"light_features\": 1.1,\n",
        "                \"boundary_features\": 2.8,\n",
        "                \"street_features\": 2,\n",
        "                \"building_features\": 1.6\n",
        "            },\n",
        "            \"include_diagnostics\": True\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClX1q_9rW_TY"
      },
      "outputs": [],
      "source": [
        "# %%writefile scene_description.py\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "# from scene_type import SCENE_TYPES\n",
        "# from scene_detail_templates import SCENE_DETAIL_TEMPLATES\n",
        "# from object_template_fillers import OBJECT_TEMPLATE_FILLERS\n",
        "# from activity_templates import ACTIVITY_TEMPLATES\n",
        "# from safety_templates import SAFETY_TEMPLATES\n",
        "# from confifence_templates import CONFIDENCE_TEMPLATES\n",
        "\n",
        "class SceneDescriptor:\n",
        "    \"\"\"\n",
        "    Generates natural language descriptions of scenes.\n",
        "    Handles scene descriptions, activity inference, and safety concerns identification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scene_types=None, object_categories=None):\n",
        "        \"\"\"\n",
        "        Initialize the scene descriptor\n",
        "\n",
        "        Args:\n",
        "            scene_types: Dictionary of scene type definitions\n",
        "        \"\"\"\n",
        "        self.scene_types = scene_types or {}\n",
        "        self.SCENE_TYPES = scene_types or {}\n",
        "\n",
        "        if object_categories:\n",
        "            self.OBJECT_CATEGORIES = object_categories\n",
        "        else:\n",
        "            # 從 JSON 加載或使用默認值\n",
        "            self.OBJECT_CATEGORIES = self._load_json_data(\"object_categories\") or {\n",
        "                \"furniture\": [56, 57, 58, 59, 60, 61],\n",
        "                \"electronics\": [62, 63, 64, 65, 66, 67, 68, 69, 70],\n",
        "                \"kitchen_items\": [39, 40, 41, 42, 43, 44, 45],\n",
        "                \"food\": [46, 47, 48, 49, 50, 51, 52, 53, 54, 55],\n",
        "                \"vehicles\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "                \"personal_items\": [24, 25, 26, 27, 28, 73, 78, 79]\n",
        "            }\n",
        "\n",
        "        # 加載所有模板數據\n",
        "        self._load_templates()\n",
        "\n",
        "    def _load_templates(self):\n",
        "        \"\"\"Load all template data from script or fallback to imported defaults\"\"\"\n",
        "        self.confidence_templates = CONFIDENCE_TEMPLATES\n",
        "        self.scene_detail_templates = SCENE_DETAIL_TEMPLATES\n",
        "        self.object_template_fillers = OBJECT_TEMPLATE_FILLERS\n",
        "        self.safety_templates = SAFETY_TEMPLATES\n",
        "        self.activity_templates = ACTIVITY_TEMPLATES\n",
        "\n",
        "\n",
        "    def _initialize_fallback_templates(self):\n",
        "        \"\"\"Initialize fallback templates when no external data is available\"\"\"\n",
        "        # 只在無法從文件或導入加載時使用\n",
        "        self.confidence_templates = {\n",
        "            \"high\": \"{description} {details}\",\n",
        "            \"medium\": \"This appears to be {description} {details}\",\n",
        "            \"low\": \"This might be {description}, but the confidence is low. {details}\"\n",
        "        }\n",
        "\n",
        "        # 僅提供最基本的模板作為後備\n",
        "        self.scene_detail_templates = {\n",
        "            \"default\": [\"A space with various objects.\"]\n",
        "        }\n",
        "\n",
        "        self.object_template_fillers = {\n",
        "            \"default\": [\"various items\"]\n",
        "        }\n",
        "\n",
        "        self.safety_templates = {\n",
        "            \"general\": \"Pay attention to {safety_element}.\"\n",
        "        }\n",
        "\n",
        "        self.activity_templates = {\n",
        "            \"default\": [\"General activity\"]\n",
        "        }\n",
        "\n",
        "    def _get_alternative_scenes(self, scene_scores: Dict[str, float],\n",
        "                            threshold: float, top_k: int = 2) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get alternative scene interpretations with their scores.\n",
        "\n",
        "        Args:\n",
        "            scene_scores: Dictionary of scene type scores\n",
        "            threshold: Minimum confidence threshold\n",
        "            top_k: Number of alternatives to return\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with alternative scenes\n",
        "        \"\"\"\n",
        "        # Sort scenes by score in descending order\n",
        "        sorted_scenes = sorted(scene_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Skip the first one (best match) and take the next top_k\n",
        "        alternatives = []\n",
        "        for scene_type, score in sorted_scenes[1:1+top_k]:\n",
        "            if score >= threshold:\n",
        "                alternatives.append({\n",
        "                    \"type\": scene_type,\n",
        "                    \"name\": self.SCENE_TYPES.get(scene_type, {}).get(\"name\", \"Unknown\"),\n",
        "                    \"confidence\": score\n",
        "                })\n",
        "\n",
        "        return alternatives\n",
        "\n",
        "\n",
        "    def _infer_possible_activities(self, scene_type: str, detected_objects: List[Dict]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Infer possible activities based on scene type and detected objects.\n",
        "\n",
        "        Args:\n",
        "            scene_type: Identified scene type\n",
        "            detected_objects: List of detected objects\n",
        "\n",
        "        Returns:\n",
        "            List of possible activities\n",
        "        \"\"\"\n",
        "        activities = []\n",
        "\n",
        "        if scene_type.startswith(\"aerial_view_\"):\n",
        "            if scene_type == \"aerial_view_intersection\":\n",
        "                # 使用預定義的十字路口活動\n",
        "                activities.extend(self.activity_templates.get(\"aerial_view_intersection\", []))\n",
        "\n",
        "                # 添加與行人和車輛相關的特定活動\n",
        "                pedestrians = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "                vehicles = [obj for obj in detected_objects if obj[\"class_id\"] in [2, 5, 7]]  # Car, bus, truck\n",
        "\n",
        "                if pedestrians and vehicles:\n",
        "                    activities.append(\"Waiting for an opportunity to cross the street\")\n",
        "                    activities.append(\"Obeying traffic signals\")\n",
        "\n",
        "            elif scene_type == \"aerial_view_commercial_area\":\n",
        "                activities.extend(self.activity_templates.get(\"aerial_view_commercial_area\", []))\n",
        "\n",
        "            elif scene_type == \"aerial_view_plaza\":\n",
        "                activities.extend(self.activity_templates.get(\"aerial_view_plaza\", []))\n",
        "\n",
        "            else:\n",
        "                # 處理其他未明確定義的空中視角場景\n",
        "                aerial_activities = [\n",
        "                    \"Street crossing\",\n",
        "                    \"Waiting for signals\",\n",
        "                    \"Following traffic rules\",\n",
        "                    \"Pedestrian movement\"\n",
        "                ]\n",
        "                activities.extend(aerial_activities)\n",
        "\n",
        "        if scene_type in self.activity_templates:\n",
        "            activities.extend(self.activity_templates[scene_type])\n",
        "        elif \"default\" in self.activity_templates:\n",
        "            activities.extend(self.activity_templates[\"default\"])\n",
        "\n",
        "        detected_class_ids = [obj[\"class_id\"] for obj in detected_objects]\n",
        "\n",
        "        # Add activities based on specific object combinations\n",
        "        if 62 in detected_class_ids and 57 in detected_class_ids:  # TV and sofa\n",
        "            activities.append(\"Watching shows or movies\")\n",
        "\n",
        "        if 63 in detected_class_ids:  # laptop\n",
        "            activities.append(\"Using a computer/laptop\")\n",
        "\n",
        "        if 67 in detected_class_ids:  # cell phone\n",
        "            activities.append(\"Using a mobile phone\")\n",
        "\n",
        "        if 73 in detected_class_ids:  # book\n",
        "            activities.append(\"Reading\")\n",
        "\n",
        "        if any(food_id in detected_class_ids for food_id in [46, 47, 48, 49, 50, 51, 52, 53, 54, 55]):\n",
        "            activities.append(\"Eating or preparing food\")\n",
        "\n",
        "        # Person-specific activities\n",
        "        if 0 in detected_class_ids:  # Person\n",
        "            if any(vehicle in detected_class_ids for vehicle in [1, 2, 3, 5, 7]):  # Vehicles\n",
        "                activities.append(\"Commuting or traveling\")\n",
        "\n",
        "            if 16 in detected_class_ids:  # Dog\n",
        "                activities.append(\"Walking a dog\")\n",
        "\n",
        "            if 24 in detected_class_ids or 26 in detected_class_ids:  # Backpack or handbag\n",
        "                activities.append(\"Carrying personal items\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        return list(set(activities))\n",
        "\n",
        "    def _identify_safety_concerns(self, detected_objects: List[Dict], scene_type: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Identify potential safety concerns based on objects and scene type.\n",
        "\n",
        "        Args:\n",
        "            detected_objects: List of detected objects\n",
        "            scene_type: Identified scene type\n",
        "\n",
        "        Returns:\n",
        "            List of potential safety concerns\n",
        "        \"\"\"\n",
        "        concerns = []\n",
        "        detected_class_ids = [obj[\"class_id\"] for obj in detected_objects]\n",
        "\n",
        "        # ORIGINAL SAFETY CONCERNS LOGIC\n",
        "\n",
        "        # General safety concerns\n",
        "        if 42 in detected_class_ids or 43 in detected_class_ids:  # Fork or knife\n",
        "            concerns.append(\"Sharp utensils present\")\n",
        "\n",
        "        if 76 in detected_class_ids:  # Scissors\n",
        "            concerns.append(\"Cutting tools present\")\n",
        "\n",
        "        # Traffic-related concerns\n",
        "        if scene_type in [\"city_street\", \"parking_lot\"]:\n",
        "            if 0 in detected_class_ids:  # Person\n",
        "                if any(vehicle in detected_class_ids for vehicle in [2, 3, 5, 7, 8]):  # Vehicles\n",
        "                    concerns.append(\"Pedestrians near vehicles\")\n",
        "\n",
        "            if 9 in detected_class_ids:  # Traffic light\n",
        "                concerns.append(\"Monitor traffic signals\")\n",
        "\n",
        "        # Identify crowded scenes\n",
        "        person_count = detected_class_ids.count(0)\n",
        "        if person_count > 5:\n",
        "            concerns.append(f\"Crowded area with multiple people ({person_count})\")\n",
        "\n",
        "        # Scene-specific concerns\n",
        "        if scene_type == \"kitchen\":\n",
        "            if 68 in detected_class_ids or 69 in detected_class_ids:  # Microwave or oven\n",
        "                concerns.append(\"Hot cooking equipment\")\n",
        "\n",
        "        # Potentially unstable objects\n",
        "        for obj in detected_objects:\n",
        "            if obj[\"class_id\"] in [39, 40, 41, 45]:  # Bottle, wine glass, cup, bowl\n",
        "                if obj[\"region\"] in [\"top_left\", \"top_center\", \"top_right\"] and obj[\"normalized_area\"] > 0.05:\n",
        "                    concerns.append(f\"Elevated {obj['class_name']} might be unstable\")\n",
        "\n",
        "        # NEW SAFETY CONCERNS LOGIC FOR ADDITIONAL SCENE TYPES\n",
        "\n",
        "        # Upscale dining safety concerns\n",
        "        if scene_type == \"upscale_dining\":\n",
        "            # Check for fragile items\n",
        "            if 40 in detected_class_ids:  # Wine glass\n",
        "                concerns.append(\"Fragile glassware present\")\n",
        "\n",
        "            # Check for lit candles (can't directly detect but can infer from context)\n",
        "            # Look for small bright spots that might be candles\n",
        "            if any(obj[\"class_id\"] == 41 for obj in detected_objects):  # Cup (which might include candle holders)\n",
        "                # We can't reliably detect candles, but if the scene appears to be formal dining,\n",
        "                # we can suggest this as a possibility\n",
        "                concerns.append(\"Possible lit candles or decorative items requiring care\")\n",
        "\n",
        "            # Check for overcrowded table\n",
        "            table_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 60]  # Dining table\n",
        "            if table_objs:\n",
        "                table_region = table_objs[0][\"region\"]\n",
        "                items_on_table = 0\n",
        "\n",
        "                for obj in detected_objects:\n",
        "                    if obj[\"class_id\"] in [39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]:\n",
        "                        if obj[\"region\"] == table_region:\n",
        "                            items_on_table += 1\n",
        "\n",
        "                if items_on_table > 8:\n",
        "                    concerns.append(\"Dining table has multiple items which should be handled with care\")\n",
        "\n",
        "        # Asian commercial street safety concerns\n",
        "        elif scene_type == \"asian_commercial_street\":\n",
        "            # Check for crowded walkways\n",
        "            if 0 in detected_class_ids:  # Person\n",
        "                person_count = detected_class_ids.count(0)\n",
        "                if person_count > 3:\n",
        "                    # Calculate person density (simplified)\n",
        "                    person_positions = []\n",
        "                    for obj in detected_objects:\n",
        "                        if obj[\"class_id\"] == 0:\n",
        "                            person_positions.append(obj[\"normalized_center\"])\n",
        "\n",
        "                    if len(person_positions) >= 2:\n",
        "                        # Calculate average distance between people\n",
        "                        total_distance = 0\n",
        "                        count = 0\n",
        "                        for i in range(len(person_positions)):\n",
        "                            for j in range(i+1, len(person_positions)):\n",
        "                                p1 = person_positions[i]\n",
        "                                p2 = person_positions[j]\n",
        "                                distance = ((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)**0.5\n",
        "                                total_distance += distance\n",
        "                                count += 1\n",
        "\n",
        "                        if count > 0:\n",
        "                            avg_distance = total_distance / count\n",
        "                            if avg_distance < 0.1:  # Close proximity\n",
        "                                concerns.append(\"Crowded walkway with limited personal space\")\n",
        "\n",
        "            # Check for motorcycles/bicycles near pedestrians\n",
        "            if (1 in detected_class_ids or 3 in detected_class_ids) and 0 in detected_class_ids:  # Bicycle/motorcycle and person\n",
        "                concerns.append(\"Two-wheeled vehicles in pedestrian areas\")\n",
        "\n",
        "            # Check for potential trip hazards\n",
        "            # We can't directly detect this, but can infer from context\n",
        "            if scene_type == \"asian_commercial_street\" and \"bottom\" in \" \".join([obj[\"region\"] for obj in detected_objects if obj[\"class_id\"] == 0]):\n",
        "                # If people are in bottom regions, they might be walking on uneven surfaces\n",
        "                concerns.append(\"Potential uneven walking surfaces in commercial area\")\n",
        "\n",
        "        # Financial district safety concerns\n",
        "        elif scene_type == \"financial_district\":\n",
        "            # Check for heavy traffic conditions\n",
        "            vehicle_count = sum(1 for obj_id in detected_class_ids if obj_id in [2, 5, 7])  # Car, bus, truck\n",
        "            if vehicle_count > 5:\n",
        "                concerns.append(\"Heavy vehicle traffic in urban area\")\n",
        "\n",
        "            # Check for pedestrians crossing busy streets\n",
        "            if 0 in detected_class_ids:  # Person\n",
        "                person_count = detected_class_ids.count(0)\n",
        "                vehicle_nearby = any(vehicle in detected_class_ids for vehicle in [2, 3, 5, 7])\n",
        "\n",
        "                if person_count > 0 and vehicle_nearby:\n",
        "                    concerns.append(\"Pedestrians navigating busy urban traffic\")\n",
        "\n",
        "            # Check for traffic signals\n",
        "            if 9 in detected_class_ids:  # Traffic light\n",
        "                concerns.append(\"Observe traffic signals when navigating this area\")\n",
        "            else:\n",
        "                # If no traffic lights detected but it's a busy area, it's worth noting\n",
        "                if vehicle_count > 3:\n",
        "                    concerns.append(\"Busy traffic area potentially without visible traffic signals in view\")\n",
        "\n",
        "            # Time of day considerations\n",
        "            # We don't have direct time data, but can infer from vehicle lights\n",
        "            vehicle_objs = [obj for obj in detected_objects if obj[\"class_id\"] in [2, 5, 7]]\n",
        "            if vehicle_objs and any(\"lighting_conditions\" in obj for obj in detected_objects):\n",
        "                # If vehicles are present and it might be evening/night\n",
        "                concerns.append(\"Reduced visibility conditions during evening commute\")\n",
        "\n",
        "        # Urban intersection safety concerns\n",
        "        elif scene_type == \"urban_intersection\":\n",
        "            # Check for pedestrians in crosswalks\n",
        "            pedestrian_objs = [obj for obj in detected_objects if obj[\"class_id\"] == 0]\n",
        "            vehicle_objs = [obj for obj in detected_objects if obj[\"class_id\"] in [2, 3, 5, 7]]\n",
        "\n",
        "            if pedestrian_objs:\n",
        "                # Calculate distribution of pedestrians to see if they're crossing\n",
        "                pedestrian_positions = [obj[\"normalized_center\"] for obj in pedestrian_objs]\n",
        "\n",
        "                # Simplified check for pedestrians in crossing pattern\n",
        "                if len(pedestrian_positions) >= 3:\n",
        "                    # Check if pedestrians are distributed across different regions\n",
        "                    pedestrian_regions = set(obj[\"region\"] for obj in pedestrian_objs)\n",
        "                    if len(pedestrian_regions) >= 2:\n",
        "                        concerns.append(\"Multiple pedestrians crossing the intersection\")\n",
        "\n",
        "            # Check for traffic signal observation\n",
        "            if 9 in detected_class_ids:  # Traffic light\n",
        "                concerns.append(\"Observe traffic signals when crossing\")\n",
        "\n",
        "            # Check for busy intersection\n",
        "            if len(vehicle_objs) > 3:\n",
        "                concerns.append(\"Busy intersection with multiple vehicles\")\n",
        "\n",
        "            # Check for pedestrians potentially jay-walking\n",
        "            if pedestrian_objs and not 9 in detected_class_ids:  # People but no traffic lights\n",
        "                concerns.append(\"Pedestrians should use designated crosswalks\")\n",
        "\n",
        "            # Visibility concerns based on lighting\n",
        "            # This would be better with actual lighting data\n",
        "            pedestrian_count = len(pedestrian_objs)\n",
        "            if pedestrian_count > 5:\n",
        "                concerns.append(\"High pedestrian density at crossing points\")\n",
        "\n",
        "        # Transit hub safety concerns\n",
        "        elif scene_type == \"transit_hub\":\n",
        "            # These would be for transit areas like train stations or bus terminals\n",
        "            if 0 in detected_class_ids:  # Person\n",
        "                person_count = detected_class_ids.count(0)\n",
        "                if person_count > 8:\n",
        "                    concerns.append(\"Crowded transit area requiring careful navigation\")\n",
        "\n",
        "            # Check for luggage/bags that could be trip hazards\n",
        "            if 24 in detected_class_ids or 28 in detected_class_ids:  # Backpack or suitcase\n",
        "                concerns.append(\"Luggage and personal items may create obstacles\")\n",
        "\n",
        "            # Public transportation vehicles\n",
        "            if any(vehicle in detected_class_ids for vehicle in [5, 6, 7]):  # Bus, train, truck\n",
        "                concerns.append(\"Stay clear of arriving and departing transit vehicles\")\n",
        "\n",
        "        # Shopping district safety concerns\n",
        "        elif scene_type == \"shopping_district\":\n",
        "            # Check for crowded shopping areas\n",
        "            if 0 in detected_class_ids:  # Person\n",
        "                person_count = detected_class_ids.count(0)\n",
        "                if person_count > 5:\n",
        "                    concerns.append(\"Crowded shopping area with multiple people\")\n",
        "\n",
        "            # Check for shopping bags and personal items\n",
        "            if 24 in detected_class_ids or 26 in detected_class_ids:  # Backpack or handbag\n",
        "                concerns.append(\"Mind personal belongings in busy retail environment\")\n",
        "\n",
        "            # Check for store entrances/exits which might have automatic doors\n",
        "            # We can't directly detect this, but can infer from context\n",
        "            if scene_type == \"shopping_district\" and 0 in detected_class_ids:\n",
        "                concerns.append(\"Be aware of store entrances and exits with potential automatic doors\")\n",
        "\n",
        "        return concerns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile clip_prompts.py\n",
        "\n",
        "# 場景類型提示\n",
        "SCENE_TYPE_PROMPTS = {\n",
        "    # 基本室內場景\n",
        "    \"living_room\": \"A photo of a living room with furniture and entertainment systems.\",\n",
        "    \"bedroom\": \"A photo of a bedroom with a bed and personal items.\",\n",
        "    \"dining_area\": \"A photo of a dining area with a table and chairs for meals.\",\n",
        "    \"kitchen\": \"A photo of a kitchen with cooking appliances and food preparation areas.\",\n",
        "    \"office_workspace\": \"A photo of an office workspace with desk, computer and work equipment.\",\n",
        "    \"meeting_room\": \"A photo of a meeting room with a conference table and multiple chairs.\",\n",
        "\n",
        "    # 基本室外/城市場景\n",
        "    \"city_street\": \"A photo of a city street with traffic, pedestrians and urban buildings.\",\n",
        "    \"parking_lot\": \"A photo of a parking lot with multiple parked vehicles.\",\n",
        "    \"park_area\": \"A photo of a park or recreational area with greenery and outdoor facilities.\",\n",
        "    \"retail_store\": \"A photo of a retail store with merchandise displays and shopping areas.\",\n",
        "    \"supermarket\": \"A photo of a supermarket with food items, aisles and shopping carts.\",\n",
        "\n",
        "    # 特殊室內場景\n",
        "    \"upscale_dining\": \"A photo of an upscale dining area with elegant furniture and refined decor.\",\n",
        "    \"conference_room\": \"A photo of a professional conference room with presentation equipment and seating.\",\n",
        "    \"classroom\": \"A photo of a classroom with desks, chairs and educational equipment.\",\n",
        "    \"library\": \"A photo of a library with bookshelves, reading areas and study spaces.\",\n",
        "\n",
        "    # 亞洲特色場景\n",
        "    \"asian_commercial_street\": \"A photo of an Asian commercial street with dense signage, shops and pedestrians.\",\n",
        "    \"asian_night_market\": \"A photo of an Asian night market with food stalls, crowds and colorful lights.\",\n",
        "    \"asian_temple_area\": \"A photo of an Asian temple with traditional architecture and cultural elements.\",\n",
        "\n",
        "    # 交通相關場景\n",
        "    \"financial_district\": \"A photo of a financial district with tall office buildings and business activity.\",\n",
        "    \"urban_intersection\": \"A photo of an urban intersection with crosswalks, traffic lights and pedestrians crossing.\",\n",
        "    \"transit_hub\": \"A photo of a transportation hub with multiple modes of public transit and passengers.\",\n",
        "    \"bus_stop\": \"A photo of a bus stop with people waiting and buses arriving or departing.\",\n",
        "    \"bus_station\": \"A photo of a bus terminal with multiple buses and traveler facilities.\",\n",
        "    \"train_station\": \"A photo of a train station with platforms, trains and passenger activity.\",\n",
        "    \"airport\": \"A photo of an airport with planes, terminals and traveler activity.\",\n",
        "\n",
        "    # 商業場景\n",
        "    \"shopping_district\": \"A photo of a shopping district with multiple retail stores and consumer activity.\",\n",
        "    \"cafe\": \"A photo of a cafe with coffee service, seating and casual dining.\",\n",
        "    \"restaurant\": \"A photo of a restaurant with dining tables, food service and eating areas.\",\n",
        "\n",
        "    # 空中視角場景\n",
        "    \"aerial_view_intersection\": \"An aerial view of an intersection showing crosswalks and traffic patterns from above.\",\n",
        "    \"aerial_view_commercial_area\": \"An aerial view of a commercial area showing shopping districts from above.\",\n",
        "    \"aerial_view_plaza\": \"An aerial view of a public plaza or square showing patterns of people movement from above.\",\n",
        "\n",
        "    # 娛樂場景\n",
        "    \"zoo\": \"A photo of a zoo with animal enclosures, exhibits and visitors.\",\n",
        "    \"playground\": \"A photo of a playground with recreational equipment and children playing.\",\n",
        "    \"sports_field\": \"A photo of a sports field with playing surfaces and athletic equipment.\",\n",
        "    \"sports_stadium\": \"A photo of a sports stadium with spectator seating and athletic facilities.\",\n",
        "\n",
        "    # 水相關場景\n",
        "    \"harbor\": \"A photo of a harbor with boats, docks and waterfront activity.\",\n",
        "    \"beach_water_recreation\": \"A photo of a beach area with water activities, sand and recreational equipment like surfboards.\",\n",
        "\n",
        "    # 文化時間特定場景\n",
        "    \"nighttime_street\": \"A photo of a street at night with artificial lighting and evening activity.\",\n",
        "    \"nighttime_commercial_district\": \"A photo of a commercial district at night with illuminated signs and evening shopping.\",\n",
        "    \"european_plaza\": \"A photo of a European-style plaza with historic architecture and public gathering spaces.\",\n",
        "\n",
        "    # 混合環境場景\n",
        "    \"indoor_outdoor_cafe\": \"A photo of a cafe with both indoor seating and outdoor patio areas.\",\n",
        "    \"transit_station_platform\": \"A photo of a transit station platform with waiting areas and arriving vehicles.\",\n",
        "\n",
        "    # 工作場景\n",
        "    \"construction_site\": \"A photo of a construction site with building materials, equipment and workers.\",\n",
        "    \"medical_facility\": \"A photo of a medical facility with healthcare equipment and professional staff.\",\n",
        "    \"educational_setting\": \"A photo of an educational setting with learning spaces and academic resources.\",\n",
        "    \"professional_kitchen\": \"A photo of a professional commercial kitchen with industrial cooking equipment and food preparation stations.\"\n",
        "}\n",
        "\n",
        "# 文化特定場景提示\n",
        "CULTURAL_SCENE_PROMPTS = {\n",
        "    \"asian_commercial_street\": [\n",
        "        \"A busy Asian shopping street with neon signs and dense storefronts.\",\n",
        "        \"A commercial street in Asia with multi-level signage and narrow walkways.\",\n",
        "        \"A street scene in Taiwan or Hong Kong with vertical signage and compact shops.\",\n",
        "        \"A crowded commercial alley in an Asian city with signs in Chinese characters.\",\n",
        "        \"A narrow shopping street in Asia with small shops on both sides.\",\n",
        "        \"An outdoor shopping district in an East Asian city with electronic billboards.\",\n",
        "        \"A bustling commercial street in Taiwan with food vendors and retail shops.\",\n",
        "        \"A pedestrian shopping area with Korean or Chinese signs and storefronts.\",\n",
        "        \"A daytime shopping street in an Asian urban center with vertical development.\"\n",
        "    ],\n",
        "    \"asian_night_market\": [\n",
        "        \"A vibrant night market in Asia with food stalls and large crowds.\",\n",
        "        \"An evening street market in Taiwan with street food vendors and bright lights.\",\n",
        "        \"A busy night bazaar in Asia with illuminated stalls and local food.\",\n",
        "        \"A crowded night street food market in an Asian city with vendor carts.\",\n",
        "        \"An Asian night market with steam from cooking food and hanging lanterns.\",\n",
        "        \"A nocturnal food street in East Asia with vendor canopies and neon lights.\",\n",
        "        \"A bustling evening market with rows of food stalls and plastic stools.\",\n",
        "        \"A lively Asian street food scene at night with cooking stations and crowds.\"\n",
        "    ],\n",
        "    \"asian_temple_area\": [\n",
        "        \"A traditional Asian temple with ornate roof details and religious symbols.\",\n",
        "        \"A Buddhist temple complex in East Asia with multiple pavilions and prayer areas.\",\n",
        "        \"A sacred site in Asia with incense burners and ceremonial elements.\",\n",
        "        \"A temple courtyard with stone statues and traditional Asian architecture.\",\n",
        "        \"A spiritual center in East Asia with pagoda-style structures and visitors.\",\n",
        "        \"An ancient temple site with Asian architectural elements and cultural symbols.\",\n",
        "        \"A religious compound with characteristic Asian roof curves and decorative features.\"\n",
        "    ],\n",
        "    \"european_plaza\": [\n",
        "        \"A historic European city square with classical architecture and cafes.\",\n",
        "        \"An old-world plaza in Europe with cobblestone paving and historic buildings.\",\n",
        "        \"A public square in a European city with fountains and surrounding architecture.\",\n",
        "        \"A central plaza in Europe with outdoor seating areas and historic monuments.\",\n",
        "        \"A traditional European town square with surrounding shops and restaurants.\",\n",
        "        \"A historic gathering space in Europe with distinctive architecture and pedestrians.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 對比類別提示\n",
        "COMPARATIVE_PROMPTS = {\n",
        "    \"indoor_vs_outdoor\": [\n",
        "        \"An indoor shopping mall corridor with controlled lighting and storefronts.\",\n",
        "        \"An outdoor commercial street with natural lighting and urban storefronts.\",\n",
        "        \"An enclosed shopping gallery with artificial lighting and climate control.\",\n",
        "        \"An open-air market street with natural light and weather exposure.\"\n",
        "    ],\n",
        "    \"professional_vs_home\": [\n",
        "        \"A professional commercial kitchen with stainless steel equipment and workstations.\",\n",
        "        \"A home kitchen with residential appliances and family cooking space.\",\n",
        "        \"A restaurant kitchen with multiple cooking stations and chef activity.\",\n",
        "        \"A family kitchen with standard household equipment and personal touches.\"\n",
        "    ],\n",
        "    \"sports_venue_vs_park\": [\n",
        "        \"A professional sports stadium with designated playing areas and audience seating.\",\n",
        "        \"A public park with casual recreation space and community greenery.\",\n",
        "        \"An athletic venue with specialized sports equipment and competitive playing surfaces.\",\n",
        "        \"An outdoor community space with general purpose areas and natural elements.\"\n",
        "    ],\n",
        "    \"asian_vs_western_commercial\": [\n",
        "        \"An Asian shopping street with vertical signage and compact multi-level shops.\",\n",
        "        \"A Western commercial street with horizontal storefronts and wider sidewalks.\",\n",
        "        \"An East Asian retail area with dense signage in Asian scripts and narrow walkways.\",\n",
        "        \"A Western shopping district with uniform building heights and Latin alphabetic signs.\"\n",
        "    ],\n",
        "    \"daytime_vs_nighttime\": [\n",
        "        \"A daytime urban scene with natural sunlight illuminating streets and buildings.\",\n",
        "        \"A nighttime city scene with artificial lighting from stores, signs and streetlights.\",\n",
        "        \"A commercial district during daylight hours with natural shadows and visibility.\",\n",
        "        \"An evening urban setting with illuminated storefronts and light patterns on streets.\"\n",
        "    ],\n",
        "    \"aerial_vs_street_level\": [\n",
        "        \"An aerial view showing urban patterns and layouts from above.\",\n",
        "        \"A street-level view showing pedestrian perspective and immediate surroundings.\",\n",
        "        \"A bird's-eye view of city organization and movement patterns from high above.\",\n",
        "        \"An eye-level perspective showing direct human interaction with urban elements.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 環境條件文本提示\n",
        "LIGHTING_CONDITION_PROMPTS = {\n",
        "    \"day_clear\": \"A photo taken during daytime with clear skies and direct sunlight.\",\n",
        "    \"day_cloudy\": \"A photo taken during daytime with overcast conditions and diffused light.\",\n",
        "    \"sunset/sunrise\": \"A photo taken during sunset or sunrise with warm golden lighting and long shadows.\",\n",
        "    \"night\": \"A photo taken at night with minimal natural light and artificial illumination.\",\n",
        "    \"indoor_bright\": \"An indoor photo with bright, even artificial lighting throughout the space.\",\n",
        "    \"indoor_moderate\": \"An indoor photo with moderate lighting creating a balanced indoor atmosphere.\",\n",
        "    \"indoor_dim\": \"An indoor photo with low lighting levels creating a subdued environment.\",\n",
        "    \"neon_night\": \"A night scene with colorful neon lighting creating vibrant illumination patterns.\",\n",
        "    \"indoor_commercial\": \"An indoor retail environment with directed display lighting highlighting products.\",\n",
        "    \"indoor_restaurant\": \"An indoor dining space with ambient mood lighting for atmosphere.\",\n",
        "    \"stadium_lighting\": \"A sports venue with powerful floodlights creating intense, even illumination.\",\n",
        "    \"mixed_lighting\": \"A scene with combined natural and artificial light sources creating transition zones.\",\n",
        "    \"beach_daylight\": \"A photo taken at a beach with bright natural sunlight and reflections from water.\",\n",
        "    \"sports_arena_lighting\": \"A photo of a sports venue illuminated by powerful overhead lighting systems.\",\n",
        "    \"kitchen_task_lighting\": \"A photo of a kitchen with focused lighting concentrated on work surfaces.\"\n",
        "}\n",
        "\n",
        "# 針對新場景類型的特殊提示\n",
        "SPECIALIZED_SCENE_PROMPTS = {\n",
        "    \"beach_water_recreation\": [\n",
        "        \"A coastal beach scene with people surfing and sunbathing on sandy shores.\",\n",
        "        \"Active water sports participants at a beach with surfboards and swimming areas.\",\n",
        "        \"A sunny beach destination with recreational water equipment and beachgoers.\",\n",
        "        \"A shoreline recreation area with surf gear and coastal activities.\",\n",
        "        \"An oceanfront scene with people engaging in water sports and beach leisure.\",\n",
        "        \"A popular beach spot with swimming areas and surfing zones.\",\n",
        "        \"A coastal recreation setting with beach umbrellas and water activities.\"\n",
        "    ],\n",
        "    \"sports_venue\": [\n",
        "        \"An indoor sports arena with professional equipment and competition spaces.\",\n",
        "        \"A sports stadium with marked playing areas and spectator seating arrangement.\",\n",
        "        \"A specialized athletic venue with competition equipment and performance areas.\",\n",
        "        \"A professional sports facility with game-related apparatus and audience zones.\",\n",
        "        \"An organized sports center with competitive play areas and athletic equipment.\",\n",
        "        \"A competition venue with sport-specific markings and professional setup.\",\n",
        "        \"A formal athletic facility with standardized equipment and playing surfaces.\"\n",
        "    ],\n",
        "    \"professional_kitchen\": [\n",
        "        \"A commercial restaurant kitchen with multiple cooking stations and food prep areas.\",\n",
        "        \"A professional culinary workspace with industrial appliances and chef activity.\",\n",
        "        \"A busy restaurant back-of-house with stainless steel equipment and meal preparation.\",\n",
        "        \"A commercial food service kitchen with chef workstations and specialized zones.\",\n",
        "        \"An industrial kitchen facility with specialized cooking equipment and prep surfaces.\",\n",
        "        \"A high-volume food production kitchen with professional-grade appliances.\",\n",
        "        \"A restaurant kitchen with distinct cooking areas and culinary workflow design.\"\n",
        "    ],\n",
        "    \"urban_intersection\": [\n",
        "        \"A city intersection with crosswalks and traffic signals controlling movement.\",\n",
        "        \"A busy urban crossroad with pedestrian crossings and vehicle traffic.\",\n",
        "        \"A regulated street intersection with crosswalk markings and waiting pedestrians.\",\n",
        "        \"A metropolitan junction with traffic lights and pedestrian crossing zones.\",\n",
        "        \"A city street crossing with safety features for pedestrians and traffic flow.\",\n",
        "        \"A controlled urban intersection with movement patterns for vehicles and people.\",\n",
        "        \"A city center crossroad with traffic management features and pedestrian areas.\"\n",
        "    ],\n",
        "    \"financial_district\": [\n",
        "        \"A downtown business area with tall office buildings and commercial activity.\",\n",
        "        \"An urban financial center with skyscrapers and professional environment.\",\n",
        "        \"A city's business district with corporate headquarters and office towers.\",\n",
        "        \"A metropolitan financial zone with high-rise buildings and business traffic.\",\n",
        "        \"A corporate district in a city center with professional architecture.\",\n",
        "        \"An urban area dominated by office buildings and business establishments.\",\n",
        "        \"A city's economic center with banking institutions and corporate offices.\"\n",
        "    ],\n",
        "    \"aerial_view_intersection\": [\n",
        "        \"A bird's-eye view of a city intersection showing crossing patterns from above.\",\n",
        "        \"An overhead perspective of an urban crossroad showing traffic organization.\",\n",
        "        \"A top-down view of a street intersection revealing pedestrian crosswalks.\",\n",
        "        \"An aerial shot of a city junction showing the layout of roads and crossings.\",\n",
        "        \"A high-angle view of an intersection showing traffic and pedestrian flow patterns.\",\n",
        "        \"A drone perspective of urban crossing design viewed from directly above.\",\n",
        "        \"A vertical view of a street intersection showing crossing infrastructure.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "VIEWPOINT_PROMPTS = {\n",
        "    \"eye_level\": \"A photo taken from normal human eye level showing a direct frontal perspective.\",\n",
        "    \"aerial\": \"A photo taken from high above looking directly down at the scene below.\",\n",
        "    \"elevated\": \"A photo taken from a higher than normal position looking down at an angle.\",\n",
        "    \"low_angle\": \"A photo taken from a low position looking upward at the scene.\",\n",
        "    \"bird_eye\": \"A photo taken from very high above showing a complete overhead perspective.\",\n",
        "    \"street_level\": \"A photo taken from the perspective of someone standing on the street.\",\n",
        "    \"interior\": \"A photo taken from inside a building showing the internal environment.\",\n",
        "    \"vehicular\": \"A photo taken from inside or mounted on a moving vehicle.\"\n",
        "}\n",
        "\n",
        "OBJECT_COMBINATION_PROMPTS = {\n",
        "    \"dining_setting\": \"A scene with tables, chairs, plates, and eating utensils arranged for meals.\",\n",
        "    \"office_setup\": \"A scene with desks, chairs, computers, and office supplies for work.\",\n",
        "    \"living_space\": \"A scene with sofas, coffee tables, TVs, and comfortable seating arrangements.\",\n",
        "    \"transportation_hub\": \"A scene with vehicles, waiting areas, passengers, and transit information.\",\n",
        "    \"retail_environment\": \"A scene with merchandise displays, shoppers, and store fixtures.\",\n",
        "    \"crosswalk_scene\": \"A scene with street markings, pedestrians crossing, and traffic signals.\",\n",
        "    \"cooking_area\": \"A scene with stoves, prep surfaces, cooking utensils, and food items.\",\n",
        "    \"recreational_space\": \"A scene with sports equipment, play areas, and activity participants.\"\n",
        "}\n",
        "\n",
        "ACTIVITY_PROMPTS = {\n",
        "    \"shopping\": \"People looking at merchandise, carrying shopping bags, and browsing stores.\",\n",
        "    \"dining\": \"People eating food, sitting at tables, and using dining utensils.\",\n",
        "    \"commuting\": \"People waiting for transportation, boarding vehicles, and traveling.\",\n",
        "    \"working\": \"People using computers, attending meetings, and engaged in professional tasks.\",\n",
        "    \"exercising\": \"People engaged in physical activities, using sports equipment, and training.\",\n",
        "    \"cooking\": \"People preparing food, using kitchen equipment, and creating meals.\",\n",
        "    \"crossing_street\": \"People walking across designated crosswalks and navigating intersections.\",\n",
        "    \"recreational_activity\": \"People engaged in leisure activities, games, and social recreation.\"\n",
        "}"
      ],
      "metadata": {
        "id": "qOZv19OLEu6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile clip_analyzer.py\n",
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple, Any, Optional, Union\n",
        "\n",
        "# from clip_prompts import (\n",
        "#     SCENE_TYPE_PROMPTS,\n",
        "#     CULTURAL_SCENE_PROMPTS,\n",
        "#     COMPARATIVE_PROMPTS,\n",
        "#     LIGHTING_CONDITION_PROMPTS,\n",
        "#     SPECIALIZED_SCENE_PROMPTS,\n",
        "#     VIEWPOINT_PROMPTS,\n",
        "#     OBJECT_COMBINATION_PROMPTS,\n",
        "#     ACTIVITY_PROMPTS\n",
        "# )\n",
        "\n",
        "class CLIPAnalyzer:\n",
        "    \"\"\"\n",
        "    Use Clip to intergrate scene understanding function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"ViT-B/32\", device: str = None):\n",
        "        \"\"\"\n",
        "        初始化 CLIP 分析器。\n",
        "\n",
        "        Args:\n",
        "            model_name: CLIP Model name,  \"ViT-B/32\"、\"ViT-B/16\"、\"ViT-L/14\"\n",
        "            device: Use GPU if it can use\n",
        "        \"\"\"\n",
        "        # 自動選擇設備\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Loading CLIP model {model_name} on {self.device}...\")\n",
        "        try:\n",
        "            self.model, self.preprocess = clip.load(model_name, device=self.device)\n",
        "            print(f\"CLIP model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading CLIP model: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.scene_type_prompts = SCENE_TYPE_PROMPTS\n",
        "        self.cultural_scene_prompts = CULTURAL_SCENE_PROMPTS\n",
        "        self.comparative_prompts = COMPARATIVE_PROMPTS\n",
        "        self.lighting_condition_prompts = LIGHTING_CONDITION_PROMPTS\n",
        "        self.specialized_scene_prompts = SPECIALIZED_SCENE_PROMPTS\n",
        "        self.viewpoint_prompts = VIEWPOINT_PROMPTS\n",
        "        self.object_combination_prompts = OBJECT_COMBINATION_PROMPTS\n",
        "        self.activity_prompts = ACTIVITY_PROMPTS\n",
        "\n",
        "        # turn to CLIP format\n",
        "        self._prepare_text_prompts()\n",
        "\n",
        "    def _prepare_text_prompts(self):\n",
        "        \"\"\"準備所有文本提示的 CLIP 特徵\"\"\"\n",
        "        # base prompt\n",
        "        scene_texts = [self.scene_type_prompts[scene_type] for scene_type in self.scene_type_prompts]\n",
        "        self.scene_type_tokens = clip.tokenize(scene_texts).to(self.device)\n",
        "\n",
        "        # cultural\n",
        "        self.cultural_tokens_dict = {}\n",
        "        for scene_type, prompts in self.cultural_scene_prompts.items():\n",
        "            self.cultural_tokens_dict[scene_type] = clip.tokenize(prompts).to(self.device)\n",
        "\n",
        "        # Light\n",
        "        lighting_texts = [self.lighting_condition_prompts[cond] for cond in self.lighting_condition_prompts]\n",
        "        self.lighting_tokens = clip.tokenize(lighting_texts).to(self.device)\n",
        "\n",
        "        # specializes_status\n",
        "        self.specialized_tokens_dict = {}\n",
        "        for scene_type, prompts in self.specialized_scene_prompts.items():\n",
        "            self.specialized_tokens_dict[scene_type] = clip.tokenize(prompts).to(self.device)\n",
        "\n",
        "        # view point\n",
        "        viewpoint_texts = [self.viewpoint_prompts[viewpoint] for viewpoint in self.viewpoint_prompts]\n",
        "        self.viewpoint_tokens = clip.tokenize(viewpoint_texts).to(self.device)\n",
        "\n",
        "        # object combination\n",
        "        object_combination_texts = [self.object_combination_prompts[combo] for combo in self.object_combination_prompts]\n",
        "        self.object_combination_tokens = clip.tokenize(object_combination_texts).to(self.device)\n",
        "\n",
        "        # activicty prompt\n",
        "        activity_texts = [self.activity_prompts[activity] for activity in self.activity_prompts]\n",
        "        self.activity_tokens = clip.tokenize(activity_texts).to(self.device)\n",
        "\n",
        "    def analyze_image(self, image, include_cultural_analysis: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        分析圖像，預測場景類型和光照條件。\n",
        "\n",
        "        Args:\n",
        "            image: 輸入圖像 (PIL Image 或 numpy array)\n",
        "            include_cultural_analysis: 是否包含文化場景的詳細分析\n",
        "\n",
        "        Returns:\n",
        "            Dict: 包含場景類型預測和光照條件的分析結果\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 確保圖像是 PIL 格式\n",
        "            if not isinstance(image, Image.Image):\n",
        "                if isinstance(image, np.ndarray):\n",
        "                    image = Image.fromarray(image)\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported image format. Expected PIL Image or numpy array.\")\n",
        "\n",
        "            # 預處理圖像\n",
        "            image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # 獲取圖像特徵\n",
        "            with torch.no_grad():\n",
        "                image_features = self.model.encode_image(image_input)\n",
        "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 分析場景類型\n",
        "            scene_scores = self._analyze_scene_type(image_features)\n",
        "\n",
        "            # 分析光照條件\n",
        "            lighting_scores = self._analyze_lighting_condition(image_features)\n",
        "\n",
        "            # 文化場景的增強分析\n",
        "            cultural_analysis = {}\n",
        "            if include_cultural_analysis:\n",
        "                for scene_type in self.cultural_scene_prompts:\n",
        "                    if scene_type in scene_scores and scene_scores[scene_type] > 0.2:\n",
        "                        cultural_analysis[scene_type] = self._analyze_cultural_scene(\n",
        "                            image_features, scene_type\n",
        "                        )\n",
        "\n",
        "            specialized_analysis = {}\n",
        "            for scene_type in self.specialized_scene_prompts:\n",
        "                if scene_type in scene_scores and scene_scores[scene_type] > 0.2:\n",
        "                    specialized_analysis[scene_type] = self._analyze_specialized_scene(\n",
        "                        image_features, scene_type\n",
        "                    )\n",
        "\n",
        "            viewpoint_scores = self._analyze_viewpoint(image_features)\n",
        "\n",
        "            object_combination_scores = self._analyze_object_combinations(image_features)\n",
        "\n",
        "            activity_scores = self._analyze_activities(image_features)\n",
        "\n",
        "            # display results\n",
        "            result = {\n",
        "                \"scene_scores\": scene_scores,\n",
        "                \"top_scene\": max(scene_scores.items(), key=lambda x: x[1]),\n",
        "                \"lighting_condition\": max(lighting_scores.items(), key=lambda x: x[1]),\n",
        "                \"embedding\": image_features.cpu().numpy().tolist()[0] if self.device == \"cuda\" else image_features.numpy().tolist()[0],\n",
        "                \"viewpoint\": max(viewpoint_scores.items(), key=lambda x: x[1]),\n",
        "                \"object_combinations\": sorted(object_combination_scores.items(), key=lambda x: x[1], reverse=True)[:3],\n",
        "                \"activities\": sorted(activity_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            }\n",
        "\n",
        "            if cultural_analysis:\n",
        "                result[\"cultural_analysis\"] = cultural_analysis\n",
        "\n",
        "            if specialized_analysis:\n",
        "                result[\"specialized_analysis\"] = specialized_analysis\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing image with CLIP: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def _analyze_scene_type(self, image_features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"分析圖像特徵與各場景類型的相似度\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 計算場景類型文本特徵\n",
        "            text_features = self.model.encode_text(self.scene_type_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 建立場景分數字典\n",
        "            scene_scores = {}\n",
        "            for i, scene_type in enumerate(self.scene_type_prompts.keys()):\n",
        "                scene_scores[scene_type] = float(similarity[i])\n",
        "\n",
        "            return scene_scores\n",
        "\n",
        "    def _analyze_lighting_condition(self, image_features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"分析圖像的光照條件\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 計算光照條件文本特徵\n",
        "            text_features = self.model.encode_text(self.lighting_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 建立光照條件分數字典\n",
        "            lighting_scores = {}\n",
        "            for i, lighting_type in enumerate(self.lighting_condition_prompts.keys()):\n",
        "                lighting_scores[lighting_type] = float(similarity[i])\n",
        "\n",
        "            return lighting_scores\n",
        "\n",
        "    def _analyze_cultural_scene(self, image_features: torch.Tensor, scene_type: str) -> Dict[str, Any]:\n",
        "        \"\"\"針對特定文化場景進行深入分析\"\"\"\n",
        "        if scene_type not in self.cultural_tokens_dict:\n",
        "            return {\"error\": f\"No cultural analysis available for {scene_type}\"}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 獲取特定文化場景的文本特徵\n",
        "            cultural_tokens = self.cultural_tokens_dict[scene_type]\n",
        "            text_features = self.model.encode_text(cultural_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 找到最匹配的文化描述\n",
        "            prompts = self.cultural_scene_prompts[scene_type]\n",
        "            scores = [(prompts[i], float(similarity[i])) for i in range(len(prompts))]\n",
        "            scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return {\n",
        "                \"best_description\": scores[0][0],\n",
        "                \"confidence\": scores[0][1],\n",
        "                \"all_matches\": scores\n",
        "            }\n",
        "\n",
        "    def _analyze_specialized_scene(self, image_features: torch.Tensor, scene_type: str) -> Dict[str, Any]:\n",
        "        \"\"\"針對特定專門場景進行深入分析\"\"\"\n",
        "        if scene_type not in self.specialized_tokens_dict:\n",
        "            return {\"error\": f\"No specialized analysis available for {scene_type}\"}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 獲取特定專門場景的文本特徵\n",
        "            specialized_tokens = self.specialized_tokens_dict[scene_type]\n",
        "            text_features = self.model.encode_text(specialized_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 找到最匹配的專門描述\n",
        "            prompts = self.specialized_scene_prompts[scene_type]\n",
        "            scores = [(prompts[i], float(similarity[i])) for i in range(len(prompts))]\n",
        "            scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return {\n",
        "                \"best_description\": scores[0][0],\n",
        "                \"confidence\": scores[0][1],\n",
        "                \"all_matches\": scores\n",
        "            }\n",
        "\n",
        "    def _analyze_viewpoint(self, image_features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"分析圖像的拍攝視角\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 計算視角文本特徵\n",
        "            text_features = self.model.encode_text(self.viewpoint_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 建立視角分數字典\n",
        "            viewpoint_scores = {}\n",
        "            for i, viewpoint in enumerate(self.viewpoint_prompts.keys()):\n",
        "                viewpoint_scores[viewpoint] = float(similarity[i])\n",
        "\n",
        "            return viewpoint_scores\n",
        "\n",
        "    def _analyze_object_combinations(self, image_features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"分析圖像中的物體組合\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 計算物體組合文本特徵\n",
        "            text_features = self.model.encode_text(self.object_combination_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 建立物體組合分數字典\n",
        "            combination_scores = {}\n",
        "            for i, combination in enumerate(self.object_combination_prompts.keys()):\n",
        "                combination_scores[combination] = float(similarity[i])\n",
        "\n",
        "            return combination_scores\n",
        "\n",
        "    def _analyze_activities(self, image_features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"分析圖像中的活動\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 計算活動文本特徵\n",
        "            text_features = self.model.encode_text(self.activity_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 計算相似度分數\n",
        "            similarity = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "            # 建立活動分數字典\n",
        "            activity_scores = {}\n",
        "            for i, activity in enumerate(self.activity_prompts.keys()):\n",
        "                activity_scores[activity] = float(similarity[i])\n",
        "\n",
        "            return activity_scores\n",
        "\n",
        "    def get_image_embedding(self, image) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        獲取圖像的 CLIP 嵌入表示\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image 或 numpy array\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: 圖像的 CLIP 特徵向量\n",
        "        \"\"\"\n",
        "        # 確保圖像是 PIL 格式\n",
        "        if not isinstance(image, Image.Image):\n",
        "            if isinstance(image, np.ndarray):\n",
        "                image = Image.fromarray(image)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported image format. Expected PIL Image or numpy array.\")\n",
        "\n",
        "        # 預處理並編碼\n",
        "        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # 轉換為 numpy 並返回\n",
        "        return image_features.cpu().numpy()[0] if self.device == \"cuda\" else image_features.numpy()[0]\n",
        "\n",
        "    def text_to_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        將文本轉換為 CLIP 嵌入表示\n",
        "\n",
        "        Args:\n",
        "            text: 輸入文本\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: 文本的 CLIP 特徵向量\n",
        "        \"\"\"\n",
        "        text_token = clip.tokenize([text]).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = self.model.encode_text(text_token)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return text_features.cpu().numpy()[0] if self.device == \"cuda\" else text_features.numpy()[0]\n",
        "\n",
        "    def calculate_similarity(self, image, text_queries: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        計算圖像與多個文本查詢的相似度\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image 或 numpy array\n",
        "            text_queries: 文本查詢列表\n",
        "\n",
        "        Returns:\n",
        "            Dict: 每個查詢的相似度分數\n",
        "        \"\"\"\n",
        "        # 獲取圖像嵌入\n",
        "        if isinstance(image, np.ndarray) and len(image.shape) == 1:\n",
        "            # 已經是嵌入向量\n",
        "            image_features = torch.tensor(image).unsqueeze(0).to(self.device)\n",
        "        else:\n",
        "            # 是圖像，需要提取嵌入\n",
        "            image_features = torch.tensor(self.get_image_embedding(image)).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # calulate similarity\n",
        "        text_tokens = clip.tokenize(text_queries).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = self.model.encode_text(text_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            similarity = similarity.cpu().numpy()[0] if self.device == \"cuda\" else similarity.numpy()[0]\n",
        "\n",
        "        # display results\n",
        "        result = {}\n",
        "        for i, query in enumerate(text_queries):\n",
        "            result[query] = float(similarity[i])\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "-LTmls2hz4bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile scene_analyzer.py\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "# from spatial_analyzer import SpatialAnalyzer\n",
        "# from scene_description import SceneDescriptor\n",
        "# from enhance_scene_describer import EnhancedSceneDescriber\n",
        "# from clip_analyzer import CLIPAnalyzer\n",
        "# from llm_enhancer import LLMEnhancer\n",
        "# from scene_type import SCENE_TYPES\n",
        "# from object_categories import OBJECT_CATEGORIES\n",
        "\n",
        "class SceneAnalyzer:\n",
        "    \"\"\"\n",
        "    Core class for scene analysis and understanding based on object detection results.\n",
        "    Analyzes detected objects, their relationships, and infers the scene type.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_names: Dict[int, str] = None, use_llm: bool = True, llm_model_path: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the scene analyzer with optional class name mappings.\n",
        "        Args:\n",
        "            class_names: Dictionary mapping class IDs to class names (optional)\n",
        "        \"\"\"\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # 加載場景類型和物體類別\n",
        "        self.SCENE_TYPES = SCENE_TYPES\n",
        "        self.OBJECT_CATEGORIES = OBJECT_CATEGORIES\n",
        "\n",
        "        # 初始化其他組件，將數據傳遞給 SceneDescriptor\n",
        "        self.spatial_analyzer = SpatialAnalyzer(class_names=class_names, object_categories=self.OBJECT_CATEGORIES)\n",
        "        self.descriptor = SceneDescriptor(scene_types=self.SCENE_TYPES, object_categories=self.OBJECT_CATEGORIES)\n",
        "        self.scene_describer = EnhancedSceneDescriber(scene_types=self.SCENE_TYPES)\n",
        "\n",
        "        # 初始化 CLIP 分析器\n",
        "        try:\n",
        "            self.clip_analyzer = CLIPAnalyzer()\n",
        "            self.use_clip = True\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not initialize CLIP analyzer: {e}\")\n",
        "            print(\"Scene analysis will proceed without CLIP. Install CLIP with 'pip install clip' for enhanced scene understanding.\")\n",
        "            self.use_clip = False\n",
        "\n",
        "        # 初始化LLM Model\n",
        "        self.use_llm = use_llm\n",
        "        if use_llm:\n",
        "            try:\n",
        "                # from llm_enhancer import LLMEnhancer\n",
        "                self.llm_enhancer = LLMEnhancer(model_path=llm_model_path)\n",
        "                print(f\"LLM enhancer initialized successfully.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not initialize LLM enhancer: {e}\")\n",
        "                print(\"Scene analysis will proceed without LLM. Make sure required packages are installed.\")\n",
        "                self.use_llm = False\n",
        "\n",
        "    def generate_scene_description(self,\n",
        "                             scene_type,\n",
        "                             detected_objects,\n",
        "                             confidence,\n",
        "                             lighting_info=None,\n",
        "                             functional_zones=None):\n",
        "        \"\"\"\n",
        "        生成場景描述。\n",
        "        Args:\n",
        "            scene_type: 識別的場景類型\n",
        "            detected_objects: 檢測到的物體列表\n",
        "            confidence: 場景分類置信度\n",
        "            lighting_info: 照明條件信息（可選）\n",
        "            functional_zones: 功能區域信息（可選）\n",
        "        Returns:\n",
        "            str: 生成的場景描述\n",
        "        \"\"\"\n",
        "        return self.scene_describer.generate_description(\n",
        "            scene_type,\n",
        "            detected_objects,\n",
        "            confidence,\n",
        "            lighting_info,\n",
        "            functional_zones\n",
        "        )\n",
        "\n",
        "    def _generate_scene_description(self, scene_type, detected_objects, confidence, lighting_info=None):\n",
        "        \"\"\"\n",
        "        Use new implement\n",
        "        \"\"\"\n",
        "        # get the functional zones info\n",
        "        functional_zones = self.spatial_analyzer._identify_functional_zones(detected_objects, scene_type)\n",
        "\n",
        "        return self.generate_scene_description(\n",
        "            scene_type,\n",
        "            detected_objects,\n",
        "            confidence,\n",
        "            lighting_info,\n",
        "            functional_zones\n",
        "        )\n",
        "\n",
        "    def _define_image_regions(self):\n",
        "        \"\"\"Define regions of the image for spatial analysis (3x3 grid)\"\"\"\n",
        "        self.regions = {\n",
        "            \"top_left\": (0, 0, 1/3, 1/3),\n",
        "            \"top_center\": (1/3, 0, 2/3, 1/3),\n",
        "            \"top_right\": (2/3, 0, 1, 1/3),\n",
        "            \"middle_left\": (0, 1/3, 1/3, 2/3),\n",
        "            \"middle_center\": (1/3, 1/3, 2/3, 2/3),\n",
        "            \"middle_right\": (2/3, 1/3, 1, 2/3),\n",
        "            \"bottom_left\": (0, 2/3, 1/3, 1),\n",
        "            \"bottom_center\": (1/3, 2/3, 2/3, 1),\n",
        "            \"bottom_right\": (2/3, 2/3, 1, 1)\n",
        "        }\n",
        "\n",
        "\n",
        "    def analyze(self, detection_result: Any, lighting_info: Optional[Dict] = None, class_confidence_threshold: float = 0.35, scene_confidence_threshold: float = 0.6) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze detection results to determine scene type and provide understanding.\n",
        "        Args:\n",
        "            detection_result: Detection result from YOLOv8\n",
        "            lighting_info: Optional lighting condition analysis results\n",
        "            class_confidence_threshold: Minimum confidence to consider an object\n",
        "            scene_confidence_threshold: Minimum confidence to determine a scene\n",
        "        Returns:\n",
        "            Dictionary with scene analysis results\n",
        "        \"\"\"\n",
        "        # If no result or no detections, handle with LLM if possible\n",
        "        if detection_result is None or len(detection_result.boxes) == 0:\n",
        "            if self.use_llm and self.use_clip and detection_result is not None:\n",
        "                # 使用CLIP和LLM分析無物體檢測的情況\n",
        "                try:\n",
        "                    original_image = detection_result.orig_img\n",
        "                    clip_analysis = self.clip_analyzer.analyze_image(original_image)\n",
        "                    llm_description = self.llm_enhancer.handle_no_detection(clip_analysis)\n",
        "\n",
        "                    return {\n",
        "                        \"scene_type\": \"llm_inferred\",\n",
        "                        \"confidence\": clip_analysis.get(\"top_scene\", (\"unknown\", 0))[1],\n",
        "                        \"description\": \"No objects detected by standard detection.\",\n",
        "                        \"enhanced_description\": llm_description,\n",
        "                        \"objects_present\": [],\n",
        "                        \"object_count\": 0,\n",
        "                        \"regions\": {},\n",
        "                        \"possible_activities\": [],\n",
        "                        \"safety_concerns\": [],\n",
        "                        \"lighting_conditions\": lighting_info or {\"time_of_day\": \"unknown\", \"confidence\": 0}\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in LLM no-detection handling: {e}\")\n",
        "\n",
        "            # 如果無法使用LLM/CLIP或處理失敗，返回原始的無檢測結果\n",
        "            return {\n",
        "                \"scene_type\": \"unknown\",\n",
        "                \"confidence\": 0,\n",
        "                \"description\": \"No objects detected in the image.\",\n",
        "                \"objects_present\": [],\n",
        "                \"object_count\": 0,\n",
        "                \"regions\": {},\n",
        "                \"possible_activities\": [],\n",
        "                \"safety_concerns\": [],\n",
        "                \"lighting_conditions\": lighting_info or {\"time_of_day\": \"unknown\", \"confidence\": 0}\n",
        "            }\n",
        "\n",
        "        # Get class names from detection result if not already set\n",
        "        if self.class_names is None:\n",
        "            self.class_names = detection_result.names\n",
        "            # Also update class names in spatial analyzer\n",
        "            self.spatial_analyzer.class_names = self.class_names\n",
        "\n",
        "        # Extract detected objects with confidence above threshold\n",
        "        detected_objects = self.spatial_analyzer._extract_detected_objects(\n",
        "            detection_result,\n",
        "            confidence_threshold=class_confidence_threshold\n",
        "        )\n",
        "\n",
        "        # No objects above confidence threshold\n",
        "        if not detected_objects:\n",
        "            return {\n",
        "                \"scene_type\": \"unknown\",\n",
        "                \"confidence\": 0,\n",
        "                \"description\": \"No objects with sufficient confidence detected.\",\n",
        "                \"objects_present\": [],\n",
        "                \"object_count\": 0,\n",
        "                \"regions\": {},\n",
        "                \"possible_activities\": [],\n",
        "                \"safety_concerns\": [],\n",
        "                \"lighting_conditions\": lighting_info or {\"time_of_day\": \"unknown\", \"confidence\": 0}\n",
        "            }\n",
        "\n",
        "        # Analyze object distribution in regions\n",
        "        region_analysis = self.spatial_analyzer._analyze_regions(detected_objects)\n",
        "\n",
        "        # Compute scene type scores based on object detection\n",
        "        yolo_scene_scores = self._compute_scene_scores(detected_objects)\n",
        "\n",
        "        # 使用 CLIP 分析圖像\n",
        "        clip_scene_scores = {}\n",
        "        clip_analysis = None\n",
        "        if self.use_clip:\n",
        "            try:\n",
        "                # 獲取原始圖像\n",
        "                original_image = detection_result.orig_img\n",
        "\n",
        "                # Use CLIP analyze image\n",
        "                clip_analysis = self.clip_analyzer.analyze_image(original_image)\n",
        "\n",
        "                # get CLIP's score\n",
        "                clip_scene_scores = clip_analysis.get(\"scene_scores\", {})\n",
        "\n",
        "                if \"asian_commercial_street\" in clip_scene_scores and clip_scene_scores[\"asian_commercial_street\"] > 0.2:\n",
        "                    # 使用對比提示進一步區分室內/室外\n",
        "                    comparative_results = self.clip_analyzer.calculate_similarity(\n",
        "                        original_image,\n",
        "                        self.clip_analyzer.comparative_prompts[\"indoor_vs_outdoor\"]\n",
        "                    )\n",
        "\n",
        "                    # 分析對比結果\n",
        "                    indoor_score = sum(s for p, s in comparative_results.items() if \"indoor\" in p or \"enclosed\" in p)\n",
        "                    outdoor_score = sum(s for p, s in comparative_results.items() if \"outdoor\" in p or \"open-air\" in p)\n",
        "\n",
        "                    # 如果 CLIP 認為這是室外場景，且光照分析認為是室內\n",
        "                    if outdoor_score > indoor_score and lighting_info and lighting_info.get(\"is_indoor\", False):\n",
        "                        # 修正光照分析結果\n",
        "                        print(f\"CLIP indicates outdoor commercial street (score: {outdoor_score:.2f} vs {indoor_score:.2f}), adjusting lighting analysis\")\n",
        "                        lighting_info[\"is_indoor\"] = False\n",
        "                        lighting_info[\"indoor_probability\"] = 0.3\n",
        "                        # 把CLIP 分析結果加到光照診斷\n",
        "                        if \"diagnostics\" not in lighting_info:\n",
        "                            lighting_info[\"diagnostics\"] = {}\n",
        "                        lighting_info[\"diagnostics\"][\"clip_override\"] = {\n",
        "                            \"reason\": \"CLIP detected outdoor commercial street\",\n",
        "                            \"outdoor_score\": float(outdoor_score),\n",
        "                            \"indoor_score\": float(indoor_score)\n",
        "                        }\n",
        "\n",
        "                # 如果 CLIP 檢測到了光照條件但沒有提供 lighting_info\n",
        "                if not lighting_info and \"lighting_condition\" in clip_analysis:\n",
        "                    lighting_type, lighting_conf = clip_analysis[\"lighting_condition\"]\n",
        "                    lighting_info = {\n",
        "                        \"time_of_day\": lighting_type,\n",
        "                        \"confidence\": lighting_conf\n",
        "                    }\n",
        "            except Exception as e:\n",
        "                print(f\"Error in CLIP analysis: {e}\")\n",
        "\n",
        "        # 融合 YOLO 和 CLIP 的場景分數\n",
        "        scene_scores = self._fuse_scene_scores(yolo_scene_scores, clip_scene_scores)\n",
        "\n",
        "        # Determine best matching scene type\n",
        "        best_scene, scene_confidence = self._determine_scene_type(scene_scores)\n",
        "\n",
        "        # Generate possible activities based on scene\n",
        "        activities = self.descriptor._infer_possible_activities(best_scene, detected_objects)\n",
        "\n",
        "        # Identify potential safety concerns\n",
        "        safety_concerns = self.descriptor._identify_safety_concerns(detected_objects, best_scene)\n",
        "\n",
        "        # Calculate functional zones\n",
        "        functional_zones = self.spatial_analyzer._identify_functional_zones(detected_objects, best_scene)\n",
        "\n",
        "        # Generate scene description\n",
        "        scene_description = self.generate_scene_description(\n",
        "            best_scene,\n",
        "            detected_objects,\n",
        "            scene_confidence,\n",
        "            lighting_info=lighting_info,\n",
        "            functional_zones=functional_zones\n",
        "        )\n",
        "\n",
        "        # 使用LLM進行增強處理\n",
        "        enhanced_description = None\n",
        "        llm_verification = None\n",
        "\n",
        "        if self.use_llm:\n",
        "            try:\n",
        "                # 準備用於LLM的場景數據\n",
        "                scene_data = {\n",
        "                    \"original_description\": scene_description,\n",
        "                    \"scene_type\": best_scene,\n",
        "                    \"scene_name\": self.SCENE_TYPES.get(best_scene, {}).get(\"name\", \"Unknown\"),\n",
        "                    \"detected_objects\": detected_objects,\n",
        "                    \"confidence\": scene_confidence,\n",
        "                    \"lighting_info\": lighting_info,\n",
        "                    \"functional_zones\": functional_zones,\n",
        "                    \"activities\": activities,\n",
        "                    \"safety_concerns\": safety_concerns,\n",
        "                    \"clip_analysis\": clip_analysis\n",
        "                }\n",
        "\n",
        "                # 如果CLIP和YOLO結果之間存在顯著差異，使用LLM進行驗證\n",
        "                if self.use_clip and clip_analysis and \"top_scene\" in clip_analysis:\n",
        "                    clip_top_scene = clip_analysis[\"top_scene\"][0]\n",
        "                    clip_confidence = clip_analysis[\"top_scene\"][1]\n",
        "\n",
        "                    # 如果CLIP和YOLO的場景預測不同且都有較高的置信度，進行驗證\n",
        "                    if clip_top_scene != best_scene and clip_confidence > 0.4 and scene_confidence > 0.4:\n",
        "                        llm_verification = self.llm_enhancer.verify_detection(\n",
        "                            detected_objects,\n",
        "                            clip_analysis,\n",
        "                            best_scene,\n",
        "                            self.SCENE_TYPES.get(best_scene, {}).get(\"name\", \"Unknown\"),\n",
        "                            scene_confidence\n",
        "                        )\n",
        "\n",
        "                        # 將驗證結果添加到場景數據中\n",
        "                        scene_data[\"verification_result\"] = llm_verification.get(\"verification_text\", \"\")\n",
        "\n",
        "                # 使用LLM生成增強描述\n",
        "                enhanced_description = self.llm_enhancer.enhance_description(scene_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in LLM enhancement: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                enhanced_description = None\n",
        "\n",
        "        # Return comprehensive analysis\n",
        "        result = {\n",
        "            \"scene_type\": best_scene if scene_confidence >= scene_confidence_threshold else \"unknown\",\n",
        "            \"scene_name\": self.SCENE_TYPES.get(best_scene, {}).get(\"name\", \"Unknown\")\n",
        "                        if scene_confidence >= scene_confidence_threshold else \"Unknown Scene\",\n",
        "            \"confidence\": scene_confidence,\n",
        "            \"description\": scene_description,\n",
        "            \"enhanced_description\": enhanced_description,  # 添加LLM增強的描述\n",
        "            \"objects_present\": [\n",
        "                {\"class_id\": obj[\"class_id\"],\n",
        "                \"class_name\": obj[\"class_name\"],\n",
        "                \"confidence\": obj[\"confidence\"]}\n",
        "                for obj in detected_objects\n",
        "            ],\n",
        "            \"object_count\": len(detected_objects),\n",
        "            \"regions\": region_analysis,\n",
        "            \"possible_activities\": activities,\n",
        "            \"safety_concerns\": safety_concerns,\n",
        "            \"functional_zones\": functional_zones,\n",
        "            \"alternative_scenes\": self.descriptor._get_alternative_scenes(scene_scores, scene_confidence_threshold, top_k=2),\n",
        "            \"lighting_conditions\": lighting_info or {\"time_of_day\": \"unknown\", \"confidence\": 0}\n",
        "        }\n",
        "\n",
        "        # 如果有LLM驗證結果，添加到輸出中\n",
        "        if llm_verification:\n",
        "            result[\"llm_verification\"] = llm_verification.get(\"verification_text\")\n",
        "            if llm_verification.get(\"has_errors\", False):\n",
        "                result[\"detection_warnings\"] = \"LLM detected potential issues with object recognition\"\n",
        "\n",
        "        # 添加 CLIP 特定的結果\n",
        "        if clip_analysis and \"error\" not in clip_analysis:\n",
        "            result[\"clip_analysis\"] = {\n",
        "                \"top_scene\": clip_analysis.get(\"top_scene\", (\"unknown\", 0)),\n",
        "                \"cultural_analysis\": clip_analysis.get(\"cultural_analysis\", {})\n",
        "            }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _compute_scene_scores(self, detected_objects: List[Dict]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute confidence scores for each scene type based on detected objects.\n",
        "        Args:\n",
        "            detected_objects: List of detected objects\n",
        "        Returns:\n",
        "            Dictionary mapping scene types to confidence scores\n",
        "        \"\"\"\n",
        "        scene_scores = {}\n",
        "        detected_class_ids = [obj[\"class_id\"] for obj in detected_objects]\n",
        "        detected_classes_set = set(detected_class_ids)\n",
        "\n",
        "        # Count occurrence of each class\n",
        "        class_counts = {}\n",
        "        for obj in detected_objects:\n",
        "            class_id = obj[\"class_id\"]\n",
        "            if class_id not in class_counts:\n",
        "                class_counts[class_id] = 0\n",
        "            class_counts[class_id] += 1\n",
        "\n",
        "        # Evaluate each scene type\n",
        "        for scene_type, scene_def in self.SCENE_TYPES.items():\n",
        "            # Count required objects present\n",
        "            required_objects = set(scene_def[\"required_objects\"])\n",
        "            required_present = required_objects.intersection(detected_classes_set)\n",
        "\n",
        "            # Count optional objects present\n",
        "            optional_objects = set(scene_def[\"optional_objects\"])\n",
        "            optional_present = optional_objects.intersection(detected_classes_set)\n",
        "\n",
        "            # Skip if minimum required objects aren't present\n",
        "            if len(required_present) < scene_def[\"minimum_required\"]:\n",
        "                scene_scores[scene_type] = 0\n",
        "                continue\n",
        "\n",
        "            # Base score from required objects\n",
        "            required_ratio = len(required_present) / max(1, len(required_objects))\n",
        "            required_score = required_ratio * 0.7  # 70% of score from required objects\n",
        "\n",
        "            # Additional score from optional objects\n",
        "            optional_ratio = len(optional_present) / max(1, len(optional_objects))\n",
        "            optional_score = optional_ratio * 0.3  # 30% of score from optional objects\n",
        "\n",
        "            # Bonus for having multiple instances of key objects\n",
        "            multiple_bonus = 0\n",
        "            for class_id in required_present:\n",
        "                if class_counts.get(class_id, 0) > 1:\n",
        "                    multiple_bonus += 0.05  # 5% bonus per additional key object type\n",
        "\n",
        "            # Cap the bonus at 15%\n",
        "            multiple_bonus = min(0.15, multiple_bonus)\n",
        "\n",
        "            # Calculate final score\n",
        "            final_score = required_score + optional_score + multiple_bonus\n",
        "\n",
        "            if \"priority\" in scene_def:\n",
        "                final_score *= scene_def[\"priority\"]\n",
        "\n",
        "            # Normalize to 0-1 range\n",
        "            scene_scores[scene_type] = min(1.0, final_score)\n",
        "\n",
        "        return scene_scores\n",
        "\n",
        "    def _determine_scene_type(self, scene_scores: Dict[str, float]) -> Tuple[str, float]:\n",
        "        \"\"\"\n",
        "        Determine the most likely scene type based on scores.\n",
        "        Args:\n",
        "            scene_scores: Dictionary mapping scene types to confidence scores\n",
        "        Returns:\n",
        "            Tuple of (best_scene_type, confidence)\n",
        "        \"\"\"\n",
        "        if not scene_scores:\n",
        "            return \"unknown\", 0\n",
        "\n",
        "        # Find scene with highest score\n",
        "        best_scene = max(scene_scores, key=scene_scores.get)\n",
        "        best_score = scene_scores[best_scene]\n",
        "\n",
        "        return best_scene, best_score\n",
        "\n",
        "\n",
        "    def _fuse_scene_scores(self, yolo_scene_scores: Dict[str, float], clip_scene_scores: Dict[str, float]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        融合基於 YOLO 物體檢測和 CLIP 分析的場景分數。\n",
        "        Args:\n",
        "            yolo_scene_scores: 基於 YOLO 物體檢測的場景分數\n",
        "            clip_scene_scores: 基於 CLIP 分析的場景分數\n",
        "        Returns:\n",
        "            Dict: 融合後的場景分數\n",
        "        \"\"\"\n",
        "        # 如果沒有 CLIP 分數，直接返回 YOLO 分數\n",
        "        if not clip_scene_scores:\n",
        "            return yolo_scene_scores\n",
        "\n",
        "        # 如果沒有 YOLO 分數，直接返回 CLIP 分數\n",
        "        if not yolo_scene_scores:\n",
        "            return clip_scene_scores\n",
        "\n",
        "        # 融合分數\n",
        "        fused_scores = {}\n",
        "\n",
        "        # 獲取所有場景類型\n",
        "        all_scene_types = set(list(yolo_scene_scores.keys()) + list(clip_scene_scores.keys()))\n",
        "\n",
        "        for scene_type in all_scene_types:\n",
        "            # 獲取兩個模型的分數\n",
        "            yolo_score = yolo_scene_scores.get(scene_type, 0)\n",
        "            clip_score = clip_scene_scores.get(scene_type, 0)\n",
        "\n",
        "            # 設置基本權重\n",
        "            yolo_weight = 0.7  # YOLO 可提供比較好的物體資訊\n",
        "            clip_weight = 0.3  # CLIP 強項是理解整體的場景關係\n",
        "\n",
        "            # 對特定類型場景調整權重\n",
        "            # 文化特定場景或具有特殊布局的場景，CLIP可能比較能理解\n",
        "            if any(keyword in scene_type for keyword in [\"asian\", \"cultural\", \"aerial\"]):\n",
        "                yolo_weight = 0.3\n",
        "                clip_weight = 0.7\n",
        "\n",
        "            # 對室內家居場景，物體檢測通常更準確\n",
        "            elif any(keyword in scene_type for keyword in [\"room\", \"kitchen\", \"office\", \"bedroom\"]):\n",
        "                yolo_weight = 0.8\n",
        "                clip_weight = 0.2\n",
        "            elif scene_type == \"beach_water_recreation\":\n",
        "                yolo_weight = 0.8  # 衝浪板等特定物品的檢測\n",
        "                clip_weight = 0.2\n",
        "            elif scene_type == \"sports_venue\":\n",
        "                yolo_weight = 0.7\n",
        "                clip_weight = 0.3\n",
        "            elif scene_type == \"professional_kitchen\":\n",
        "                yolo_weight = 0.8  # 廚房用具的檢測非常重要\n",
        "                clip_weight = 0.2\n",
        "\n",
        "            # 計算加權分數\n",
        "            fused_scores[scene_type] = (yolo_score * yolo_weight) + (clip_score * clip_weight)\n",
        "\n",
        "        return fused_scores"
      ],
      "metadata": {
        "id": "bmj_nX2I0MDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVw5JMy6qhY3"
      },
      "outputs": [],
      "source": [
        "# %%writefile image_processor.py\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import uuid\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "\n",
        "# from detection_model import DetectionModel\n",
        "# from color_mapper import ColorMapper\n",
        "# from visualization_helper import VisualizationHelper\n",
        "# from evaluation_metrics import EvaluationMetrics\n",
        "# from lighting_analyzer import LightingAnalyzer\n",
        "# from scene_analyzer import SceneAnalyzer\n",
        "\n",
        "class ImageProcessor:\n",
        "    \"\"\"\n",
        "    Class for handling image processing and object detection operations\n",
        "    Separates processing logic from UI components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_llm=True, llm_model_path=None):\n",
        "        \"\"\"Initialize the image processor with required components\"\"\"\n",
        "        self.color_mapper = ColorMapper()\n",
        "        self.model_instances = {}\n",
        "        self.lighting_analyzer = LightingAnalyzer()\n",
        "        self.use_llm = use_llm\n",
        "        self.llm_model_path = llm_model_path\n",
        "\n",
        "    def get_model_instance(self, model_name: str, confidence: float = 0.25, iou: float = 0.25) -> DetectionModel:\n",
        "        \"\"\"\n",
        "        Get or create a model instance based on model name\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model to use\n",
        "            confidence: Confidence threshold for detection\n",
        "            iou: IoU threshold for non-maximum suppression\n",
        "\n",
        "        Returns:\n",
        "            DetectionModel instance\n",
        "        \"\"\"\n",
        "        if model_name not in self.model_instances:\n",
        "            print(f\"Creating new model instance for {model_name}\")\n",
        "            self.model_instances[model_name] = DetectionModel(\n",
        "                model_name=model_name,\n",
        "                confidence=confidence,\n",
        "                iou=iou\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Using existing model instance for {model_name}\")\n",
        "            self.model_instances[model_name].confidence = confidence\n",
        "\n",
        "        return self.model_instances[model_name]\n",
        "\n",
        "    def analyze_scene(self, detection_result: Any, lighting_info: Optional[Dict] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform scene analysis on detection results\n",
        "\n",
        "        Args:\n",
        "            detection_result: Object detection result from YOLOv8\n",
        "            lighting_info: Lighting condition analysis results (optional)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing scene analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Initialize scene analyzer if not already done\n",
        "            if not hasattr(self, 'scene_analyzer'):\n",
        "                self.scene_analyzer = SceneAnalyzer(\n",
        "                    class_names=detection_result.names,\n",
        "                    use_llm=self.use_llm,\n",
        "                    llm_model_path=self.llm_model_path\n",
        "                )\n",
        "\n",
        "            # 確保類名正確更新\n",
        "            if self.scene_analyzer.class_names is None:\n",
        "                self.scene_analyzer.class_names = detection_result.names\n",
        "                self.scene_analyzer.spatial_analyzer.class_names = detection_result.names\n",
        "\n",
        "            # Perform scene analysis with lighting info\n",
        "            scene_analysis = self.scene_analyzer.analyze(\n",
        "                detection_result=detection_result,\n",
        "                lighting_info=lighting_info,\n",
        "                class_confidence_threshold=0.35,\n",
        "                scene_confidence_threshold=0.6\n",
        "            )\n",
        "\n",
        "            return scene_analysis\n",
        "        except Exception as e:\n",
        "            print(f\"Error in scene analysis: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"scene_type\": \"unknown\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"description\": f\"Error during scene analysis: {str(e)}\",\n",
        "                \"objects_present\": [],\n",
        "                \"object_count\": 0,\n",
        "                \"regions\": {},\n",
        "                \"possible_activities\": [],\n",
        "                \"safety_concerns\": [],\n",
        "                \"lighting_conditions\": lighting_info or {\"time_of_day\": \"unknown\", \"confidence\": 0.0}\n",
        "            }\n",
        "\n",
        "    def analyze_lighting_conditions(self, image):\n",
        "        \"\"\"\n",
        "        分析光照條件。\n",
        "\n",
        "        Args:\n",
        "            image: 輸入圖像\n",
        "\n",
        "        Returns:\n",
        "            Dict: 光照分析結果\n",
        "        \"\"\"\n",
        "        return self.lighting_analyzer.analyze(image)\n",
        "\n",
        "    def process_image(self, image, model_name: str, confidence_threshold: float, filter_classes: Optional[List[int]] = None) -> Tuple[Any, str, Dict]:\n",
        "        \"\"\"\n",
        "        Process an image for object detection\n",
        "\n",
        "        Args:\n",
        "            image: Input image (numpy array or PIL Image)\n",
        "            model_name: Name of the model to use\n",
        "            confidence_threshold: Confidence threshold for detection\n",
        "            filter_classes: Optional list of classes to filter results\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (result_image, result_text, stats_data)\n",
        "        \"\"\"\n",
        "        # Get model instance\n",
        "        model_instance = self.get_model_instance(model_name, confidence_threshold)\n",
        "\n",
        "        # Initialize key variables\n",
        "        result = None\n",
        "        stats = {}\n",
        "        temp_path = None\n",
        "\n",
        "        try:\n",
        "            # Processing input image\n",
        "            if isinstance(image, np.ndarray):\n",
        "                # Convert BGR to RGB if needed\n",
        "                if image.shape[2] == 3:\n",
        "                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                else:\n",
        "                    image_rgb = image\n",
        "                pil_image = Image.fromarray(image_rgb)\n",
        "            elif image is None:\n",
        "                return None, \"No image provided. Please upload an image.\", {}\n",
        "            else:\n",
        "                pil_image = image\n",
        "\n",
        "            # Analyze lighting conditions\n",
        "            lighting_info = self.analyze_lighting_conditions(pil_image)\n",
        "\n",
        "            # Store temp files\n",
        "            temp_dir = tempfile.gettempdir()  # Use system temp directory\n",
        "            temp_filename = f\"temp_{uuid.uuid4().hex}.jpg\"\n",
        "            temp_path = os.path.join(temp_dir, temp_filename)\n",
        "            pil_image.save(temp_path)\n",
        "\n",
        "            # Object detection\n",
        "            result = model_instance.detect(temp_path)\n",
        "\n",
        "            if result is None:\n",
        "                return None, \"Detection failed. Please try again with a different image.\", {}\n",
        "\n",
        "            # Calculate stats\n",
        "            stats = EvaluationMetrics.calculate_basic_stats(result)\n",
        "\n",
        "            # Add space calculation\n",
        "            spatial_metrics = EvaluationMetrics.calculate_distance_metrics(result)\n",
        "            stats[\"spatial_metrics\"] = spatial_metrics\n",
        "\n",
        "            # Add lighting information\n",
        "            stats[\"lighting_conditions\"] = lighting_info\n",
        "\n",
        "            # Apply filter if specified\n",
        "            if filter_classes and len(filter_classes) > 0:\n",
        "                # Get classes, boxes, confidence\n",
        "                classes = result.boxes.cls.cpu().numpy().astype(int)\n",
        "                confs = result.boxes.conf.cpu().numpy()\n",
        "                boxes = result.boxes.xyxy.cpu().numpy()\n",
        "\n",
        "                mask = np.zeros_like(classes, dtype=bool)\n",
        "                for cls_id in filter_classes:\n",
        "                    mask = np.logical_or(mask, classes == cls_id)\n",
        "\n",
        "                filtered_stats = {\n",
        "                    \"total_objects\": int(np.sum(mask)),\n",
        "                    \"class_statistics\": {},\n",
        "                    \"average_confidence\": float(np.mean(confs[mask])) if np.any(mask) else 0,\n",
        "                    \"spatial_metrics\": stats[\"spatial_metrics\"],\n",
        "                    \"lighting_conditions\": lighting_info\n",
        "                }\n",
        "\n",
        "                # Update stats\n",
        "                names = result.names\n",
        "                for cls, conf in zip(classes[mask], confs[mask]):\n",
        "                    cls_name = names[int(cls)]\n",
        "                    if cls_name not in filtered_stats[\"class_statistics\"]:\n",
        "                        filtered_stats[\"class_statistics\"][cls_name] = {\n",
        "                            \"count\": 0,\n",
        "                            \"average_confidence\": 0\n",
        "                        }\n",
        "\n",
        "                    filtered_stats[\"class_statistics\"][cls_name][\"count\"] += 1\n",
        "                    filtered_stats[\"class_statistics\"][cls_name][\"average_confidence\"] = conf\n",
        "\n",
        "                stats = filtered_stats\n",
        "\n",
        "            viz_data = EvaluationMetrics.generate_visualization_data(\n",
        "                result,\n",
        "                self.color_mapper.get_all_colors()\n",
        "            )\n",
        "\n",
        "            result_image = VisualizationHelper.visualize_detection(\n",
        "                temp_path, result, color_mapper=self.color_mapper, figsize=(12, 12), return_pil=True, filter_classes=filter_classes\n",
        "            )\n",
        "\n",
        "            result_text = EvaluationMetrics.format_detection_summary(viz_data)\n",
        "\n",
        "            if result is not None:\n",
        "                # Perform scene analysis with lighting info\n",
        "                scene_analysis = self.analyze_scene(result, lighting_info)\n",
        "\n",
        "                # Add scene analysis to stats\n",
        "                stats[\"scene_analysis\"] = scene_analysis\n",
        "\n",
        "            return result_image, result_text, stats\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error Occurs: {str(e)}\"\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            print(error_message)\n",
        "            return None, error_message, {}\n",
        "\n",
        "        finally:\n",
        "            if temp_path and os.path.exists(temp_path):\n",
        "                try:\n",
        "                    os.remove(temp_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Cannot delete temp files {temp_path}: {str(e)}\")\n",
        "\n",
        "\n",
        "    def format_result_text(self, stats: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Format detection statistics into readable text with improved spacing\n",
        "\n",
        "        Args:\n",
        "            stats: Dictionary containing detection statistics\n",
        "\n",
        "        Returns:\n",
        "            Formatted text summary\n",
        "        \"\"\"\n",
        "        if not stats or \"total_objects\" not in stats:\n",
        "            return \"No objects detected.\"\n",
        "\n",
        "        # 減少不必要的空行\n",
        "        lines = [\n",
        "            f\"Detected {stats['total_objects']} objects.\",\n",
        "            f\"Average confidence: {stats.get('average_confidence', 0):.2f}\",\n",
        "            \"Objects by class:\"\n",
        "        ]\n",
        "\n",
        "        if \"class_statistics\" in stats and stats[\"class_statistics\"]:\n",
        "            # 按計數排序類別\n",
        "            sorted_classes = sorted(\n",
        "                stats[\"class_statistics\"].items(),\n",
        "                key=lambda x: x[1][\"count\"],\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            for cls_name, cls_stats in sorted_classes:\n",
        "                count = cls_stats[\"count\"]\n",
        "                conf = cls_stats.get(\"average_confidence\", 0)\n",
        "\n",
        "                item_text = \"item\" if count == 1 else \"items\"\n",
        "                lines.append(f\"• {cls_name}: {count} {item_text} (avg conf: {conf:.2f})\")\n",
        "        else:\n",
        "            lines.append(\"No class information available.\")\n",
        "\n",
        "        # 添加空間信息\n",
        "        if \"spatial_metrics\" in stats and \"spatial_distribution\" in stats[\"spatial_metrics\"]:\n",
        "            lines.append(\"Object Distribution:\")\n",
        "\n",
        "            dist = stats[\"spatial_metrics\"][\"spatial_distribution\"]\n",
        "            x_mean = dist.get(\"x_mean\", 0)\n",
        "            y_mean = dist.get(\"y_mean\", 0)\n",
        "\n",
        "            # 描述物體的大致位置\n",
        "            if x_mean < 0.33:\n",
        "                h_pos = \"on the left side\"\n",
        "            elif x_mean < 0.67:\n",
        "                h_pos = \"in the center\"\n",
        "            else:\n",
        "                h_pos = \"on the right side\"\n",
        "\n",
        "            if y_mean < 0.33:\n",
        "                v_pos = \"in the upper part\"\n",
        "            elif y_mean < 0.67:\n",
        "                v_pos = \"in the middle\"\n",
        "            else:\n",
        "                v_pos = \"in the lower part\"\n",
        "\n",
        "            lines.append(f\"• Most objects appear {h_pos} {v_pos} of the image\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def format_json_for_display(self, stats: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Format statistics JSON for better display\n",
        "\n",
        "        Args:\n",
        "            stats: Raw statistics dictionary\n",
        "\n",
        "        Returns:\n",
        "            Formatted statistics structure for display\n",
        "        \"\"\"\n",
        "        # Create a cleaner copy of the stats for display\n",
        "        display_stats = {}\n",
        "\n",
        "        # Add summary section\n",
        "        display_stats[\"summary\"] = {\n",
        "            \"total_objects\": stats.get(\"total_objects\", 0),\n",
        "            \"average_confidence\": round(stats.get(\"average_confidence\", 0), 3)\n",
        "        }\n",
        "\n",
        "        # Add class statistics in a more organized way\n",
        "        if \"class_statistics\" in stats and stats[\"class_statistics\"]:\n",
        "            # Sort classes by count (descending)\n",
        "            sorted_classes = sorted(\n",
        "                stats[\"class_statistics\"].items(),\n",
        "                key=lambda x: x[1].get(\"count\", 0),\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            class_stats = {}\n",
        "            for cls_name, cls_data in sorted_classes:\n",
        "                class_stats[cls_name] = {\n",
        "                    \"count\": cls_data.get(\"count\", 0),\n",
        "                    \"average_confidence\": round(cls_data.get(\"average_confidence\", 0), 3)\n",
        "                }\n",
        "\n",
        "            display_stats[\"detected_objects\"] = class_stats\n",
        "\n",
        "        # Simplify spatial metrics\n",
        "        if \"spatial_metrics\" in stats:\n",
        "            spatial = stats[\"spatial_metrics\"]\n",
        "\n",
        "            # Simplify spatial distribution\n",
        "            if \"spatial_distribution\" in spatial:\n",
        "                dist = spatial[\"spatial_distribution\"]\n",
        "                display_stats[\"spatial\"] = {\n",
        "                    \"distribution\": {\n",
        "                        \"x_mean\": round(dist.get(\"x_mean\", 0), 3),\n",
        "                        \"y_mean\": round(dist.get(\"y_mean\", 0), 3),\n",
        "                        \"x_std\": round(dist.get(\"x_std\", 0), 3),\n",
        "                        \"y_std\": round(dist.get(\"y_std\", 0), 3)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "            # Add simplified size information\n",
        "            if \"size_distribution\" in spatial:\n",
        "                size = spatial[\"size_distribution\"]\n",
        "                display_stats[\"spatial\"][\"size\"] = {\n",
        "                    \"mean_area\": round(size.get(\"mean_area\", 0), 3),\n",
        "                    \"min_area\": round(size.get(\"min_area\", 0), 3),\n",
        "                    \"max_area\": round(size.get(\"max_area\", 0), 3)\n",
        "                }\n",
        "\n",
        "        return display_stats\n",
        "\n",
        "    def prepare_visualization_data(self, stats: Dict, available_classes: Dict[int, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Prepare data for visualization based on detection statistics\n",
        "\n",
        "        Args:\n",
        "            stats: Detection statistics\n",
        "            available_classes: Dictionary of available class IDs and names\n",
        "\n",
        "        Returns:\n",
        "            Visualization data dictionary\n",
        "        \"\"\"\n",
        "        if not stats or \"class_statistics\" not in stats or not stats[\"class_statistics\"]:\n",
        "            return {\"error\": \"No detection data available\"}\n",
        "\n",
        "        # Prepare visualization data\n",
        "        viz_data = {\n",
        "            \"total_objects\": stats.get(\"total_objects\", 0),\n",
        "            \"average_confidence\": stats.get(\"average_confidence\", 0),\n",
        "            \"class_data\": []\n",
        "        }\n",
        "\n",
        "        # Class data\n",
        "        for cls_name, cls_stats in stats.get(\"class_statistics\", {}).items():\n",
        "            # Search class ID\n",
        "            class_id = -1\n",
        "            for id, name in available_classes.items():\n",
        "                if name == cls_name:\n",
        "                    class_id = id\n",
        "                    break\n",
        "\n",
        "            cls_data = {\n",
        "                \"name\": cls_name,\n",
        "                \"class_id\": class_id,\n",
        "                \"count\": cls_stats.get(\"count\", 0),\n",
        "                \"average_confidence\": cls_stats.get(\"average_confidence\", 0),\n",
        "                \"color\": self.color_mapper.get_color(class_id if class_id >= 0 else cls_name)\n",
        "            }\n",
        "\n",
        "            viz_data[\"class_data\"].append(cls_data)\n",
        "\n",
        "        # Descending order\n",
        "        viz_data[\"class_data\"].sort(key=lambda x: x[\"count\"], reverse=True)\n",
        "\n",
        "        return viz_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile video_processor.py\n",
        "import cv2\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# from image_processor import ImageProcessor\n",
        "# from evaluation_metrics import EvaluationMetrics\n",
        "# from scene_analyzer import SceneAnalyzer\n",
        "# from detection_model import DetectionModel\n",
        "\n",
        "class VideoProcessor:\n",
        "    \"\"\"\n",
        "    Handles the processing of video files, including object detection\n",
        "    and scene analysis on selected frames.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_processor: ImageProcessor):\n",
        "        \"\"\"\n",
        "        Initializes the VideoProcessor.\n",
        "\n",
        "        Args:\n",
        "            image_processor (ImageProcessor): An initialized ImageProcessor instance.\n",
        "        \"\"\"\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def process_video_file(self,\n",
        "                           video_path: str,\n",
        "                           model_name: str,\n",
        "                           confidence_threshold: float,\n",
        "                           process_interval: int = 5,\n",
        "                           scene_desc_interval_sec: int = 3) -> Tuple[Optional[str], str, Dict]:\n",
        "        \"\"\"\n",
        "        Processes an uploaded video file, performs detection and periodic scene analysis,\n",
        "        and returns the path to the annotated output video file along with a summary.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to the input video file.\n",
        "            model_name (str): Name of the YOLO model to use.\n",
        "            confidence_threshold (float): Confidence threshold for object detection.\n",
        "            process_interval (int): Process every Nth frame. Defaults to 5.\n",
        "            scene_desc_interval_sec (int): Update scene description every N seconds. Defaults to 3.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Optional[str], str, Dict]: (Path to output video or None, Summary text, Statistics dictionary)\n",
        "        \"\"\"\n",
        "        if not video_path or not os.path.exists(video_path):\n",
        "            print(f\"Error: Video file not found at {video_path}\")\n",
        "            return None, \"Error: Video file not found.\", {}\n",
        "\n",
        "        print(f\"Starting video processing for: {video_path}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video file {video_path}\")\n",
        "            return None, \"Error opening video file.\", {}\n",
        "\n",
        "        # Get video properties\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps <= 0: # Handle case where fps is not available or invalid\n",
        "             fps = 30 # Assume a default fps\n",
        "             print(f\"Warning: Could not get valid FPS for video. Assuming {fps} FPS.\")\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        print(f\"Video properties: {width}x{height} @ {fps:.2f} FPS, Total Frames: {total_frames_video}\")\n",
        "\n",
        "        # Calculate description update interval in frames\n",
        "        description_update_interval_frames = int(fps * scene_desc_interval_sec)\n",
        "        if description_update_interval_frames < 1:\n",
        "            description_update_interval_frames = int(fps) # Update at least once per second if interval is too short\n",
        "\n",
        "        object_trackers = {}  # 儲存ID與物體的映射\n",
        "        last_detected_objects = {}  # 儲存上一次檢測到的物體資訊\n",
        "        next_object_id = 0  # 下一個可用的物體ID\n",
        "        tracking_threshold = 0.6  # 相同物體的IoU\n",
        "        object_colors = {}  # 每個被追蹤的物體分配固定顏色\n",
        "\n",
        "        # Setup Output Video\n",
        "        output_filename = f\"processed_{uuid.uuid4().hex}_{os.path.basename(video_path)}\"\n",
        "        temp_dir = tempfile.gettempdir() # Use system's temp directory\n",
        "        output_path = os.path.join(temp_dir, output_filename)\n",
        "        # Ensure the output path has a compatible extension (like .mp4)\n",
        "        if not output_path.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "            output_path += \".mp4\"\n",
        "\n",
        "        # Use 'mp4v' for MP4, common and well-supported\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "        if not out.isOpened():\n",
        "            print(f\"Error: Could not open VideoWriter for path: {output_path}\")\n",
        "            cap.release()\n",
        "            return None, f\"Error creating output video file at {output_path}.\", {}\n",
        "        print(f\"Output video will be saved to: {output_path}\")\n",
        "\n",
        "        frame_count = 0\n",
        "        processed_frame_count = 0\n",
        "        all_stats = [] # Store stats for each processed frame\n",
        "        summary_lines = []\n",
        "        last_description = \"Analyzing scene...\" # Initial description\n",
        "        frame_since_last_desc = description_update_interval_frames # Trigger analysis on first processed frame\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break # End of video\n",
        "\n",
        "                frame_count += 1\n",
        "                frame_since_last_desc += 1\n",
        "                current_frame_annotated = False # Flag if this frame was processed and annotated\n",
        "\n",
        "                # Process frame based on interval\n",
        "                if frame_count % process_interval == 0:\n",
        "                    processed_frame_count += 1\n",
        "                    print(f\"Processing frame {frame_count}...\")\n",
        "                    current_frame_annotated = True\n",
        "\n",
        "                    # Use ImageProcessor for single-frame tasks\n",
        "                    # 1. Convert frame format BGR -> RGB -> PIL\n",
        "                    try:\n",
        "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                        pil_image = Image.fromarray(frame_rgb)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error converting frame {frame_count}: {e}\")\n",
        "                        continue # Skip this frame\n",
        "\n",
        "                    # 2. Get appropriate model instance\n",
        "                    # Confidence is passed from UI, model_name too\n",
        "                    model_instance = self.image_processor.get_model_instance(model_name, confidence_threshold)\n",
        "                    if not model_instance or not model_instance.is_model_loaded:\n",
        "                         print(f\"Error: Model {model_name} not loaded. Skipping frame {frame_count}.\")\n",
        "                         # Draw basic frame without annotation\n",
        "                         cv2.putText(frame, f\"Scene: {last_description[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3, cv2.LINE_AA)\n",
        "                         cv2.putText(frame, f\"Scene: {last_description[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                         out.write(frame)\n",
        "                         continue\n",
        "\n",
        "\n",
        "                    # 3. Perform detection\n",
        "                    detection_result = model_instance.detect(pil_image) # Use PIL image\n",
        "\n",
        "                    current_description_for_frame = last_description # Default to last known description\n",
        "                    scene_analysis_result = None\n",
        "                    stats = {}\n",
        "\n",
        "                    if detection_result and hasattr(detection_result, 'boxes') and len(detection_result.boxes) > 0:\n",
        "                        # Ensure SceneAnalyzer is ready within ImageProcessor\n",
        "                        if not hasattr(self.image_processor, 'scene_analyzer') or self.image_processor.scene_analyzer is None:\n",
        "                             print(\"Initializing SceneAnalyzer...\")\n",
        "                             # Pass class names from the current detection result\n",
        "                             self.image_processor.scene_analyzer = SceneAnalyzer(class_names=detection_result.names)\n",
        "                        elif self.image_processor.scene_analyzer.class_names is None:\n",
        "                             # Update class names if they were missing\n",
        "                             self.image_processor.scene_analyzer.class_names = detection_result.names\n",
        "                             if hasattr(self.image_processor.scene_analyzer, 'spatial_analyzer'):\n",
        "                                 self.image_processor.scene_analyzer.spatial_analyzer.class_names = detection_result.names\n",
        "\n",
        "\n",
        "                        # 4. Perform Scene Analysis (periodically)\n",
        "                        if frame_since_last_desc >= description_update_interval_frames:\n",
        "                            print(f\"Analyzing scene at frame {frame_count} (threshold: {description_update_interval_frames} frames)...\")\n",
        "                            # Pass lighting_info=None for now, as it's disabled for performance\n",
        "                            scene_analysis_result = self.image_processor.analyze_scene(detection_result, lighting_info=None)\n",
        "                            current_description_for_frame = scene_analysis_result.get(\"description\", last_description)\n",
        "                            last_description = current_description_for_frame # Cache the new description\n",
        "                            frame_since_last_desc = 0 # Reset counter\n",
        "\n",
        "                        # 5. Calculate Statistics for this frame\n",
        "                        stats = EvaluationMetrics.calculate_basic_stats(detection_result)\n",
        "                        stats['frame_number'] = frame_count # Add frame number to stats\n",
        "                        all_stats.append(stats)\n",
        "\n",
        "                        # 6. Draw annotations\n",
        "                        names = detection_result.names\n",
        "                        boxes = detection_result.boxes.xyxy.cpu().numpy()\n",
        "                        classes = detection_result.boxes.cls.cpu().numpy().astype(int)\n",
        "                        confs = detection_result.boxes.conf.cpu().numpy()\n",
        "\n",
        "                        def calculate_iou(box1, box2):\n",
        "                            \"\"\"Calculate Intersection IOU value\"\"\"\n",
        "                            x1_1, y1_1, x2_1, y2_1 = box1\n",
        "                            x1_2, y1_2, x2_2, y2_2 = box2\n",
        "\n",
        "                            xi1 = max(x1_1, x1_2)\n",
        "                            yi1 = max(y1_1, y1_2)\n",
        "                            xi2 = min(x2_1, x2_2)\n",
        "                            yi2 = min(y2_1, y2_2)\n",
        "\n",
        "                            inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "                            box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "                            box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "\n",
        "                            union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "                            return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "                        # 處理當前幀中的所有檢測\n",
        "                        current_detected_objects = {}\n",
        "\n",
        "                        for box, cls_id, conf in zip(boxes, classes, confs):\n",
        "                            x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                            # 查找最匹配的已追蹤物體\n",
        "                            best_match_id = None\n",
        "                            best_match_iou = 0\n",
        "\n",
        "                            for obj_id, (old_box, old_cls_id, _) in last_detected_objects.items():\n",
        "                                if old_cls_id == cls_id:  # 同一類別才比較\n",
        "                                    iou = calculate_iou(box, old_box)\n",
        "                                    if iou > tracking_threshold and iou > best_match_iou:\n",
        "                                        best_match_id = obj_id\n",
        "                                        best_match_iou = iou\n",
        "\n",
        "                            # 如果找到匹配，使用現有ID；否則分配新ID\n",
        "                            if best_match_id is not None:\n",
        "                                obj_id = best_match_id\n",
        "                            else:\n",
        "                                obj_id = next_object_id\n",
        "                                next_object_id += 1\n",
        "\n",
        "                                # 使用更明顯的顏色\n",
        "                                bright_colors = [\n",
        "                                    (0, 0, 255),    # red\n",
        "                                    (0, 255, 0),    # green\n",
        "                                    (255, 0, 0),    # blue\n",
        "                                    (0, 255, 255),  # yellow\n",
        "                                    (255, 0, 255),  # purple\n",
        "                                    (255, 128, 0),  # orange\n",
        "                                    (128, 0, 255)   # purple\n",
        "                                ]\n",
        "                                object_colors[obj_id] = bright_colors[obj_id % len(bright_colors)]\n",
        "\n",
        "                            # update tracking info\n",
        "                            current_detected_objects[obj_id] = (box, cls_id, conf)\n",
        "\n",
        "                            color = object_colors.get(obj_id, (0, 255, 0))  # default is green\n",
        "                            label = f\"{names.get(cls_id, 'Unknown')}-{obj_id}: {conf:.2f}\"\n",
        "\n",
        "                            # 平滑化邊界框：如果是已知物體，與上一幀位置平均\n",
        "                            if obj_id in last_detected_objects:\n",
        "                                old_box, _, _ = last_detected_objects[obj_id]\n",
        "                                old_x1, old_y1, old_x2, old_y2 = map(int, old_box)\n",
        "                                # 平滑係數\n",
        "                                alpha = 0.7  # current weight\n",
        "                                beta = 0.3   # history weight\n",
        "\n",
        "                                x1 = int(alpha * x1 + beta * old_x1)\n",
        "                                y1 = int(alpha * y1 + beta * old_y1)\n",
        "                                x2 = int(alpha * x2 + beta * old_x2)\n",
        "                                y2 = int(alpha * y2 + beta * old_y2)\n",
        "\n",
        "                            # draw box and label\n",
        "                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                            # add text\n",
        "                            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "                            cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1 - 10), color, -1)\n",
        "                            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "                        # update tracking info\n",
        "                        last_detected_objects = current_detected_objects.copy()\n",
        "\n",
        "\n",
        "                    # Draw the current scene description on the frame\n",
        "                    cv2.putText(frame, f\"Scene: {current_description_for_frame[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3, cv2.LINE_AA) # Black outline\n",
        "                    cv2.putText(frame, f\"Scene: {current_description_for_frame[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA) # White text\n",
        "\n",
        "                # Write the frame (annotated or original) to the output video\n",
        "                # Draw last known description if this frame wasn't processed\n",
        "                if not current_frame_annotated:\n",
        "                    cv2.putText(frame, f\"Scene: {last_description[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3, cv2.LINE_AA)\n",
        "                    cv2.putText(frame, f\"Scene: {last_description[:80]}...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "                out.write(frame) # Write frame to output file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video processing loop for {video_path}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            summary_lines.append(f\"An error occurred during processing: {e}\")\n",
        "        finally:\n",
        "            # Release resources\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            print(f\"Video processing finished. Resources released. Output path: {output_path}\")\n",
        "            if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "                print(f\"Error: Output video file was not created or is empty at {output_path}\")\n",
        "                summary_lines.append(\"Error: Failed to create output video.\")\n",
        "                output_path = None\n",
        "\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        summary_lines.insert(0, f\"Finished processing in {processing_time:.2f} seconds.\")\n",
        "        summary_lines.insert(1, f\"Processed {processed_frame_count} frames out of {frame_count} (interval: {process_interval} frames).\")\n",
        "        summary_lines.insert(2, f\"Scene description updated approximately every {scene_desc_interval_sec} seconds.\")\n",
        "\n",
        "        # Generate Aggregate Statistics\n",
        "        aggregated_stats = {\n",
        "            \"total_frames_read\": frame_count,\n",
        "            \"total_frames_processed\": processed_frame_count,\n",
        "            \"avg_objects_per_processed_frame\": 0, # Calculate below\n",
        "            \"cumulative_detections\": {}, # Total times each class was detected\n",
        "            \"max_concurrent_detections\": {} # Max count of each class in a single processed frame\n",
        "            }\n",
        "        object_cumulative_counts = {}\n",
        "        object_max_concurrent_counts = {} # Store the max count found for each object type\n",
        "        total_detected_in_processed = 0\n",
        "\n",
        "        # Iterate through stats collected from each processed frame\n",
        "        for frame_stats in all_stats:\n",
        "            total_objects_in_frame = frame_stats.get(\"total_objects\", 0)\n",
        "            total_detected_in_processed += total_objects_in_frame\n",
        "\n",
        "            # Iterate through object classes detected in this frame\n",
        "            for obj_name, obj_data in frame_stats.get(\"class_statistics\", {}).items():\n",
        "                count_in_frame = obj_data.get(\"count\", 0)\n",
        "\n",
        "                # Cumulative count\n",
        "                if obj_name not in object_cumulative_counts:\n",
        "                    object_cumulative_counts[obj_name] = 0\n",
        "                object_cumulative_counts[obj_name] += count_in_frame\n",
        "\n",
        "                # Max concurrent count\n",
        "                if obj_name not in object_max_concurrent_counts:\n",
        "                    object_max_concurrent_counts[obj_name] = 0\n",
        "                # Update the max count if the current frame's count is higher\n",
        "                object_max_concurrent_counts[obj_name] = max(object_max_concurrent_counts[obj_name], count_in_frame)\n",
        "\n",
        "        # Add sorted results to the final dictionary\n",
        "        aggregated_stats[\"cumulative_detections\"] = dict(sorted(object_cumulative_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "        aggregated_stats[\"max_concurrent_detections\"] = dict(sorted(object_max_concurrent_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        # Calculate average objects per processed frame\n",
        "        if processed_frame_count > 0:\n",
        "             aggregated_stats[\"avg_objects_per_processed_frame\"] = round(total_detected_in_processed / processed_frame_count, 2)\n",
        "\n",
        "        summary_text = \"\\n\".join(summary_lines)\n",
        "        print(\"Generated Summary:\\n\", summary_text)\n",
        "        print(\"Aggregated Stats (Revised):\\n\", aggregated_stats) # Print the revised stats\n",
        "\n",
        "        # Return the potentially updated output_path\n",
        "        return output_path, summary_text, aggregated_stats"
      ],
      "metadata": {
        "id": "aGtfLV0N2g10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile llm_enhancer.py\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import logging\n",
        "\n",
        "class LLMEnhancer:\n",
        "    \"\"\"\n",
        "    負責使用LLM (Large Language Model) 增強場景理解和描述。\n",
        "    未來可以再整合Llama或其他LLM模型進行場景描述的生成和豐富化。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                model_path: Optional[str] = None,\n",
        "                tokenizer_path: Optional[str] = None,\n",
        "                device: Optional[str] = None,\n",
        "                max_length: int = 2048,\n",
        "                temperature: float = 0.3,\n",
        "                top_p: float = 0.85):\n",
        "        \"\"\"\n",
        "        初始化LLM增強器\n",
        "\n",
        "        Args:\n",
        "            model_path: LLM模型的路徑或HuggingFace log in，默認使用Llama 3.2\n",
        "            tokenizer_path: token處理器的路徑，通常與model_path相同\n",
        "            device: 設備檢查 ('cpu'或'cuda')\n",
        "            max_length: 生成文本的最大長度\n",
        "            temperature: 生成文本的溫度（較高比較有創意，較低會偏保守）\n",
        "            top_p: 生成文本時的核心採樣機率閾值\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(\"LLMEnhancer\")\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        handler = logging.StreamHandler()\n",
        "        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
        "        self.logger.addHandler(handler)\n",
        "\n",
        "        # 默認用 Llama3.2\n",
        "        self.model_path = model_path or \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "        self.tokenizer_path = tokenizer_path or self.model_path\n",
        "\n",
        "        # 確定運行設備\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # create parameters\n",
        "        self.max_length = max_length\n",
        "        self.temperature = temperature\n",
        "        self.top_p = top_p\n",
        "\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # 計數器，用來追蹤模型調用次數\n",
        "        self.call_count = 0\n",
        "\n",
        "        self._initialize_prompts()\n",
        "\n",
        "        # only if need to load the model\n",
        "        self._model_loaded = False\n",
        "\n",
        "        try:\n",
        "            self.hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "            if self.hf_token:\n",
        "                self.logger.info(\"Logging in to Hugging Face with token\")\n",
        "                from huggingface_hub import login\n",
        "                login(token=self.hf_token)\n",
        "            else:\n",
        "                self.logger.warning(\"HF_TOKEN not found in environment variables. Access to gated models may be limited.\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during Hugging Face login: {e}\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"只在首次需要時加載，使用 8 位量化以節省記憶體\"\"\"\n",
        "        if self._model_loaded:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading LLM model from {self.model_path} with 8-bit quantization\")\n",
        "            import torch\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                free_in_GB = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                print(f\"Total GPU memory: {free_in_GB:.2f} GB\")\n",
        "\n",
        "            # 設置 8 位元配置(節省記憶體空間)\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_enable_fp32_cpu_offload=True\n",
        "            )\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.tokenizer_path,\n",
        "                padding_side=\"left\",\n",
        "                use_fast=False,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "\n",
        "            # 特殊標記\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # 加載 8 位量化模型\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                low_cpu_mem_usage=True,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "\n",
        "            self.logger.info(\"Model loaded successfully with 8-bit quantization\")\n",
        "            self._model_loaded = True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading LLM model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def _initialize_prompts(self):\n",
        "        \"\"\"Return an optimized prompt template specifically for Zephyr model\"\"\"\n",
        "        # the critical prompt for the model\n",
        "        self.enhance_description_template = \"\"\"\n",
        "            <|system|>\n",
        "            You are an expert visual analyst. Your task is to improve the readability and fluency of scene descriptions using STRICT factual accuracy.\n",
        "\n",
        "            Your **top priority is to avoid hallucination** or fabrication. You are working in a computer vision pipeline using object detection (YOLO) and image embeddings. You MUST treat the input object list as a whitelist. Do not speculate beyond this list.\n",
        "\n",
        "            </|system|>\n",
        "\n",
        "            <|user|>\n",
        "            Rewrite the following scene description to be fluent and clear. DO NOT add any objects, events, or spatial relationships that are not explicitly present in the original or object list.\n",
        "\n",
        "            ORIGINAL:\n",
        "            {original_description}\n",
        "\n",
        "            CRITICAL RULES:\n",
        "            1. NEVER assume room type, object function, or scene purpose unless directly stated.\n",
        "            2. NEVER invent object types. You are limited to: {object_list}\n",
        "            3. NEVER speculate on object quantity. If the description says \"10 people\" , DO NOT say \"dozens\" or \"many\". Maintain the original quantity unless specified.\n",
        "            4. Use terms like \"in the scene\", \"visible in the background\", or \"positioned in the lower left\" instead of assuming direction or layout logic.\n",
        "            5. You MAY describe confirmed materials, colors, and composition style if visually obvious and non-speculative.\n",
        "            6. Write 2–4 complete, well-structured sentences with punctuation.\n",
        "            7. Final output MUST be a single fluent paragraph of 60–200 words (not longer).\n",
        "            8. NEVER include explanations, reasoning, or tags. ONLY provide the enhanced description.\n",
        "            9. Do not repeat any sentence structure or phrase more than once.\n",
        "            </|user|>\n",
        "\n",
        "            <|assistant|>\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "        # 錯誤檢測的prompt\n",
        "        self.verify_detection_template = \"\"\"\n",
        "            Task: You are an advanced vision system that verifies computer vision detections for accuracy.\n",
        "\n",
        "            Analyze the following detection results and identify any potential errors or inconsistencies:\n",
        "\n",
        "            SCENE TYPE: {scene_type}\n",
        "            SCENE NAME: {scene_name}\n",
        "            CONFIDENCE: {confidence:.2f}\n",
        "\n",
        "            DETECTED OBJECTS: {detected_objects}\n",
        "\n",
        "            CLIP ANALYSIS RESULTS:\n",
        "            {clip_analysis}\n",
        "\n",
        "            Possible Errors to Check:\n",
        "            1. Objects misidentified (e.g., architectural elements labeled as vehicles)\n",
        "            2. Cultural elements misunderstood (e.g., Asian temple structures labeled as boats)\n",
        "            3. Objects that seem out of place for this type of scene\n",
        "            4. Inconsistencies between different detection systems\n",
        "\n",
        "            If you find potential errors, list them clearly with explanations. If the detections seem reasonable, state that they appear accurate.\n",
        "\n",
        "            Verification Results:\n",
        "            \"\"\"\n",
        "\n",
        "        # 無檢測處理的prompt\n",
        "        self.no_detection_template = \"\"\"\n",
        "            Task: You are an advanced scene understanding system analyzing an image where standard object detection failed to identify specific objects.\n",
        "\n",
        "            Based on advanced image embeddings (CLIP analysis), we have the following information:\n",
        "\n",
        "            MOST LIKELY SCENE: {top_scene} (confidence: {top_confidence:.2f})\n",
        "            VIEWPOINT: {viewpoint}\n",
        "            LIGHTING: {lighting_condition}\n",
        "\n",
        "            CULTURAL ANALYSIS: {cultural_analysis}\n",
        "\n",
        "            Create a detailed description of what might be in this scene, considering:\n",
        "            1. The most likely type of location or setting\n",
        "            2. Possible architectural or natural elements present\n",
        "            3. The lighting and atmosphere\n",
        "            4. Potential cultural or regional characteristics\n",
        "\n",
        "            Your description should be natural, flowing, and offer insights into what the image likely contains despite the lack of specific object detection.\n",
        "\n",
        "            Scene Description:\n",
        "            \"\"\"\n",
        "\n",
        "    def _clean_llama_response(self, response: str) -> str:\n",
        "        \"\"\"處理 Llama 模型特有的輸出格式問題\"\"\"\n",
        "        # 首先應用通用清理\n",
        "        response = self._clean_model_response(response)\n",
        "\n",
        "        # 移除 Llama 常見的前綴短語\n",
        "        prefixes_to_remove = [\n",
        "            \"Here's the enhanced description:\",\n",
        "            \"Enhanced description:\",\n",
        "            \"Here is the enhanced scene description:\",\n",
        "            \"I've enhanced the description while preserving all factual details:\"\n",
        "        ]\n",
        "\n",
        "        for prefix in prefixes_to_remove:\n",
        "            if response.lower().startswith(prefix.lower()):\n",
        "                response = response[len(prefix):].strip()\n",
        "\n",
        "        # 移除可能的後綴說明\n",
        "        suffixes_to_remove = [\n",
        "            \"I've maintained all the key factual elements\",\n",
        "            \"I've preserved all the factual details\",\n",
        "            \"All factual elements have been maintained\"\n",
        "        ]\n",
        "\n",
        "        for suffix in suffixes_to_remove:\n",
        "            if response.lower().endswith(suffix.lower()):\n",
        "                response = response[:response.rfind(suffix)].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    # For Future Usage\n",
        "    def _detect_scene_type(self, detected_objects: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Detect scene type based on object distribution and patterns\n",
        "        \"\"\"\n",
        "        # Default scene type\n",
        "        scene_type = \"intersection\"\n",
        "\n",
        "        # Count objects by class\n",
        "        object_counts = {}\n",
        "        for obj in detected_objects:\n",
        "            class_name = obj.get(\"class_name\", \"\")\n",
        "            if class_name not in object_counts:\n",
        "                object_counts[class_name] = 0\n",
        "            object_counts[class_name] += 1\n",
        "\n",
        "        # 辨識人\n",
        "        people_count = object_counts.get(\"person\", 0)\n",
        "\n",
        "        # 交通工具的\n",
        "        car_count = object_counts.get(\"car\", 0)\n",
        "        bus_count = object_counts.get(\"bus\", 0)\n",
        "        truck_count = object_counts.get(\"truck\", 0)\n",
        "        total_vehicles = car_count + bus_count + truck_count\n",
        "\n",
        "        # Simple scene type detection logic\n",
        "        if people_count > 8 and total_vehicles < 2:\n",
        "            scene_type = \"pedestrian_crossing\"\n",
        "        elif people_count > 5 and total_vehicles > 2:\n",
        "            scene_type = \"busy_intersection\"\n",
        "        elif people_count < 3 and total_vehicles > 3:\n",
        "            scene_type = \"traffic_junction\"\n",
        "\n",
        "        return scene_type\n",
        "\n",
        "    def _clean_scene_type(self, scene_type: str) -> str:\n",
        "        \"\"\"清理場景類型，使其更適合用於提示詞\"\"\"\n",
        "        if not scene_type:\n",
        "            return \"scene\"\n",
        "\n",
        "        # replace underline to space or sometime capital letter\n",
        "        if '_' in scene_type:\n",
        "            return ' '.join(word.capitalize() for word in scene_type.split('_'))\n",
        "\n",
        "        return scene_type\n",
        "\n",
        "    def _clean_model_response(self, response: str) -> str:\n",
        "        \"\"\"清理模型回應以移除常見的標記和前綴\"\"\"\n",
        "        # 移除任何可能殘留的系統樣式標記\n",
        "        response = re.sub(r'<\\|.*?\\|>', '', response)\n",
        "\n",
        "        # 移除任何 \"This european_plaza\" 或類似前綴\n",
        "        response = re.sub(r'^This [a-z_]+\\s+', '', response)\n",
        "\n",
        "        # 確保響應以大寫字母開頭\n",
        "        if response and not response[0].isupper():\n",
        "            response = response[0].upper() + response[1:]\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    def reset_context(self):\n",
        "        \"\"\"在處理新圖像前重置模型上下文\"\"\"\n",
        "        if self._model_loaded:\n",
        "            # 清除 GPU 緩存\n",
        "            torch.cuda.empty_cache()\n",
        "            self.logger.info(\"Model context reset\")\n",
        "        else:\n",
        "            self.logger.info(\"Model not loaded, no context to reset\")\n",
        "\n",
        "    def _remove_introduction_sentences(self, response: str) -> str:\n",
        "        \"\"\"移除生成文本中可能的介紹性句子\"\"\"\n",
        "        # 識別常見的介紹性模式\n",
        "        intro_patterns = [\n",
        "            r'^Here is the (?:rewritten|enhanced) .*?description:',\n",
        "            r'^The (?:rewritten|enhanced) description:',\n",
        "            r'^Here\\'s the (?:rewritten|enhanced) description of .*?:'\n",
        "        ]\n",
        "\n",
        "        for pattern in intro_patterns:\n",
        "            if re.match(pattern, response, re.IGNORECASE):\n",
        "                # 找到冒號後的內容\n",
        "                parts = re.split(r':', response, 1)\n",
        "                if len(parts) > 1:\n",
        "                    return parts[1].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    def enhance_description(self, scene_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"改進的場景描述增強器，處理各種場景類型並保留視角與光照資訊，並作為總窗口可運用於其他class\"\"\"\n",
        "        try:\n",
        "            # 重置上下文\n",
        "            self.reset_context()\n",
        "\n",
        "            # 確保模型已加載\n",
        "            if not self._model_loaded:\n",
        "                self._load_model()\n",
        "\n",
        "            # extract original description\n",
        "            original_desc = scene_data.get(\"original_description\", \"\")\n",
        "            if not original_desc:\n",
        "                return \"No original description provided.\"\n",
        "\n",
        "            # 獲取scene type 並標準化\n",
        "            scene_type = scene_data.get(\"scene_type\", \"unknown scene\")\n",
        "            scene_type = self._clean_scene_type(scene_type)\n",
        "\n",
        "            # 提取檢測到的物件並過濾低信心度物件\n",
        "            detected_objects = scene_data.get(\"detected_objects\", [])\n",
        "            filtered_objects = []\n",
        "\n",
        "            # 高信心度閾值，嚴格過濾物件\n",
        "            high_confidence_threshold = 0.65\n",
        "\n",
        "            for obj in detected_objects:\n",
        "                confidence = obj.get(\"confidence\", 0)\n",
        "                class_name = obj.get(\"class_name\", \"\")\n",
        "\n",
        "                # 為特殊類別設置更高閾值\n",
        "                special_classes = [\"airplane\", \"helicopter\", \"boat\"]\n",
        "                if class_name in special_classes:\n",
        "                    if confidence < 0.75:  # 為這些類別設置更高閾值\n",
        "                        continue\n",
        "\n",
        "                # 只保留高信心度物件\n",
        "                if confidence >= high_confidence_threshold:\n",
        "                    filtered_objects.append(obj)\n",
        "\n",
        "            # 計算物件列表和數量 - 僅使用過濾後的高信心度物件\n",
        "            object_counts = {}\n",
        "            for obj in filtered_objects:\n",
        "                class_name = obj.get(\"class_name\", \"\")\n",
        "                if class_name not in object_counts:\n",
        "                    object_counts[class_name] = 0\n",
        "                object_counts[class_name] += 1\n",
        "\n",
        "            # 將高置信度物件格式化為清單\n",
        "            high_confidence_objects = \", \".join([f\"{count} {obj}\" for obj, count in object_counts.items()])\n",
        "\n",
        "            # 如果沒有高信心度物件，回退到使用原始描述中的關鍵詞\n",
        "            if not high_confidence_objects:\n",
        "                # 從原始描述中提取物件提及\n",
        "                object_keywords = self._extract_objects_from_description(original_desc)\n",
        "                high_confidence_objects = \", \".join(object_keywords) if object_keywords else \"objects visible in the scene\"\n",
        "\n",
        "            # 保留原始描述中的關鍵視角信息\n",
        "            perspective = self._extract_perspective_from_description(original_desc)\n",
        "\n",
        "            # 提取光照資訊\n",
        "            lighting_description = \"unknown lighting\"\n",
        "            if \"lighting_info\" in scene_data:\n",
        "                lighting_info = scene_data.get(\"lighting_info\", {})\n",
        "                time_of_day = lighting_info.get(\"time_of_day\", \"unknown\")\n",
        "                is_indoor = lighting_info.get(\"is_indoor\", False)\n",
        "                lighting_description = f\"{'indoor' if is_indoor else 'outdoor'} {time_of_day} lighting\"\n",
        "\n",
        "            # 創建prompt，整合所有關鍵資訊\n",
        "            prompt = self.enhance_description_template.format(\n",
        "                scene_type=scene_type,\n",
        "                object_list=high_confidence_objects,\n",
        "                original_description=original_desc,\n",
        "                perspective=perspective,\n",
        "                lighting_description=lighting_description\n",
        "            )\n",
        "\n",
        "            # 生成增強描述\n",
        "            self.logger.info(\"Generating LLM response...\")\n",
        "            response = self._generate_llm_response(prompt)\n",
        "\n",
        "            # 檢查回應完整性的更嚴格標準\n",
        "            is_incomplete = (\n",
        "                len(response) < 100 or  # too short\n",
        "                (len(response) < 200 and \".\" not in response[-30:]) or  # 結尾沒有適當的標點符號\n",
        "                any(response.endswith(phrase) for phrase in [\"in the\", \"with the\", \"and the\"])  # 以不完整短語結尾\n",
        "            )\n",
        "\n",
        "            max_retries = 3\n",
        "            attempts = 0\n",
        "            while attempts < max_retries and is_incomplete:\n",
        "                self.logger.warning(f\"Generated incomplete response, retrying... Attempt {attempts+1}/{max_retries}\")\n",
        "                # 重新生成\n",
        "                response = self._generate_llm_response(prompt)\n",
        "                attempts += 1\n",
        "\n",
        "                # 重新檢查完整性\n",
        "                is_incomplete = (len(response) < 100 or\n",
        "                                (len(response) < 200 and \".\" not in response[-30:]) or\n",
        "                                any(response.endswith(phrase) for phrase in [\"in the\", \"with the\", \"and the\"]))\n",
        "\n",
        "            if not response or len(response.strip()) < 10:\n",
        "                self.logger.warning(\"Generated response was empty or too short, returning original description\")\n",
        "                return original_desc\n",
        "\n",
        "            # 使用與模型相符的清理方法\n",
        "            if \"llama\" in self.model_path.lower():\n",
        "                result = self._clean_llama_response(response)\n",
        "            else:\n",
        "                result = self._clean_model_response(response)\n",
        "\n",
        "            # 移除介紹性type句子\n",
        "            result = self._remove_introduction_sentences(result)\n",
        "\n",
        "            # 移除explanation\n",
        "            result = self._remove_explanatory_notes(result)\n",
        "\n",
        "            # fact check\n",
        "            result = self._verify_factual_accuracy(original_desc, result, high_confidence_objects)\n",
        "\n",
        "            # 確保場景類型和視角一致性\n",
        "            result = self._ensure_scene_type_consistency(result, scene_type, original_desc)\n",
        "            if perspective and perspective.lower() not in result.lower():\n",
        "                result = f\"{perspective}, {result[0].lower()}{result[1:]}\"\n",
        "\n",
        "            return str(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Enhancement failed: {str(e)}\")\n",
        "            import traceback\n",
        "            self.logger.error(traceback.format_exc())\n",
        "            return original_desc  # 發生任何錯誤時返回原始描述\n",
        "\n",
        "    def _verify_factual_accuracy(self, original: str, generated: str, object_list: str) -> str:\n",
        "        \"\"\"驗證生成的描述不包含原始描述或物體列表中沒有的信息\"\"\"\n",
        "\n",
        "        # 將原始描述和物體列表合併為授權詞彙源\n",
        "        authorized_content = original.lower() + \" \" + object_list.lower()\n",
        "\n",
        "        # 提取生成描述中具有實質意義的名詞\n",
        "        # 創建常見地點、文化和地域詞彙的列表\n",
        "        location_terms = [\"plaza\", \"square\", \"market\", \"mall\", \"avenue\", \"boulevard\"]\n",
        "        cultural_terms = [\"european\", \"asian\", \"american\", \"african\", \"western\", \"eastern\"]\n",
        "\n",
        "        # 檢查生成文本中的每個詞\n",
        "        for term in location_terms + cultural_terms:\n",
        "            # 僅當該詞出現在生成文本但不在授權內容中時進行替換\n",
        "            if term in generated.lower() and term not in authorized_content:\n",
        "                # 根據詞語類型選擇適當的替換詞\n",
        "                if term in location_terms:\n",
        "                    replacement = \"area\"\n",
        "                else:\n",
        "                    replacement = \"scene\"\n",
        "\n",
        "                # 使用正則表達式進行完整詞匹配替換\n",
        "                pattern = re.compile(r'\\b' + term + r'\\b', re.IGNORECASE)\n",
        "                generated = pattern.sub(replacement, generated)\n",
        "\n",
        "        return generated\n",
        "\n",
        "\n",
        "    def verify_detection(self,\n",
        "                       detected_objects: List[Dict],\n",
        "                       clip_analysis: Dict[str, Any],\n",
        "                       scene_type: str,\n",
        "                       scene_name: str,\n",
        "                       confidence: float) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        驗證並可能修正YOLO的檢測結果\n",
        "\n",
        "        Args:\n",
        "            detected_objects: YOLO檢測到的物體列表\n",
        "            clip_analysis: CLIP分析結果\n",
        "            scene_type: 識別的場景類型\n",
        "            scene_name: 場景名稱\n",
        "            confidence: 場景分類的信心度\n",
        "\n",
        "        Returns:\n",
        "            Dict: 包含驗證結果和建議的字典\n",
        "        \"\"\"\n",
        "        # 確保模型已加載\n",
        "        self._load_model()\n",
        "\n",
        "        # 格式化數據\n",
        "        objects_str = self._format_objects_for_prompt(detected_objects)\n",
        "        clip_str = self._format_clip_results(clip_analysis)\n",
        "\n",
        "        # 構建提示\n",
        "        prompt = self.verify_detection_template.format(\n",
        "            scene_type=scene_type,\n",
        "            scene_name=scene_name,\n",
        "            confidence=confidence,\n",
        "            detected_objects=objects_str,\n",
        "            clip_analysis=clip_str\n",
        "        )\n",
        "\n",
        "        # 調用LLM進行驗證\n",
        "        verification_result = self._generate_llm_response(prompt)\n",
        "\n",
        "        # 解析驗證結果\n",
        "        result = {\n",
        "            \"verification_text\": verification_result,\n",
        "            \"has_errors\": \"appear accurate\" not in verification_result.lower(),\n",
        "            \"corrected_objects\": None  # 可能在未來版本實現詳細錯誤修正\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _remove_explanatory_notes(self, response: str) -> str:\n",
        "        \"\"\"移除解釋性注釋、說明和其他非描述性內容\"\"\"\n",
        "\n",
        "        # 識別常見的注釋和解釋模式\n",
        "        note_patterns = [\n",
        "            r'(?:^|\\n)Note:.*?(?:\\n|$)',\n",
        "            r'(?:^|\\n)I have (?:followed|adhered to|ensured).*?(?:\\n|$)',\n",
        "            r'(?:^|\\n)This description (?:follows|adheres to|maintains).*?(?:\\n|$)',\n",
        "            r'(?:^|\\n)The enhanced description (?:maintains|preserves).*?(?:\\n|$)'\n",
        "        ]\n",
        "\n",
        "        # 尋找第一段完整的描述內容\n",
        "        paragraphs = [p.strip() for p in response.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        # 如果只有一個段落，檢查並清理它\n",
        "        if len(paragraphs) == 1:\n",
        "            for pattern in note_patterns:\n",
        "                paragraphs[0] = re.sub(pattern, '', paragraphs[0], flags=re.IGNORECASE)\n",
        "            return paragraphs[0].strip()\n",
        "\n",
        "        # 如果有多個段落，識別並移除注釋段落\n",
        "        content_paragraphs = []\n",
        "        for paragraph in paragraphs:\n",
        "            is_note = False\n",
        "            for pattern in note_patterns:\n",
        "                if re.search(pattern, paragraph, flags=re.IGNORECASE):\n",
        "                    is_note = True\n",
        "                    break\n",
        "\n",
        "            # 檢查段落是否以常見的注釋詞開頭\n",
        "            if paragraph.lower().startswith(('note:', 'please note:', 'remember:')):\n",
        "                is_note = True\n",
        "\n",
        "            if not is_note:\n",
        "                content_paragraphs.append(paragraph)\n",
        "\n",
        "        # 返回清理後的內容\n",
        "        return '\\n\\n'.join(content_paragraphs).strip()\n",
        "\n",
        "    def handle_no_detection(self, clip_analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        處理YOLO未檢測到物體的情況\n",
        "\n",
        "        Args:\n",
        "            clip_analysis: CLIP分析結果\n",
        "\n",
        "        Returns:\n",
        "            str: 生成的場景描述\n",
        "        \"\"\"\n",
        "        # 確保模型已加載\n",
        "        self._load_model()\n",
        "\n",
        "        # 提取CLIP結果\n",
        "        top_scene, top_confidence = clip_analysis.get(\"top_scene\", (\"unknown\", 0))\n",
        "        viewpoint = clip_analysis.get(\"viewpoint\", (\"standard\", 0))[0]\n",
        "        lighting = clip_analysis.get(\"lighting_condition\", (\"unknown\", 0))[0]\n",
        "\n",
        "        # 格式化文化分析\n",
        "        cultural_str = self._format_cultural_analysis(clip_analysis.get(\"cultural_analysis\", {}))\n",
        "\n",
        "        # 構建提示\n",
        "        prompt = self.no_detection_template.format(\n",
        "            top_scene=top_scene,\n",
        "            top_confidence=top_confidence,\n",
        "            viewpoint=viewpoint,\n",
        "            lighting_condition=lighting,\n",
        "            cultural_analysis=cultural_str\n",
        "        )\n",
        "\n",
        "        # 調用LLM生成描述\n",
        "        description = self._generate_llm_response(prompt)\n",
        "\n",
        "        # 優化輸出\n",
        "        return self._clean_llm_response(description)\n",
        "\n",
        "    def _clean_input_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        對輸入文本進行通用的格式清理，處理常見的格式問題。\n",
        "\n",
        "        Args:\n",
        "            text: 輸入文本\n",
        "\n",
        "        Returns:\n",
        "            清理後的文本\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # 清理格式的問題\n",
        "        # 1. 處理連續標點符號問題\n",
        "        text = re.sub(r'([.,;:!?])\\1+', r'\\1', text)\n",
        "\n",
        "        # 2. 修復不完整句子的標點（如 \"Something,\" 後沒有繼續接續下去）\n",
        "        text = re.sub(r',\\s*$', '.', text)\n",
        "\n",
        "        # 3. 修復如 \"word.\" 後未加空格即接下一句的問題\n",
        "        text = re.sub(r'([.!?])([A-Z])', r'\\1 \\2', text)\n",
        "\n",
        "        # 4. 移除多餘空格\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 5. 確保句子正確結束（句尾加句號）\n",
        "        if text and not text[-1] in '.!?':\n",
        "            text += '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _fact_check_description(self, original_desc: str, enhanced_desc: str, scene_type: str, detected_objects: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        驗證並可能修正增強後的描述，確保有保持事實準確性。\n",
        "\n",
        "        Args:\n",
        "            original_desc: 原始場景描述\n",
        "            enhanced_desc: 增強後的描述待驗證\n",
        "            scene_type: 場景類型\n",
        "            detected_objects: 檢測到的物體名稱列表\n",
        "\n",
        "        Returns:\n",
        "            經過事實檢查的描述\n",
        "        \"\"\"\n",
        "        # 如果增強描述為空或太短，返回原始描述\n",
        "        if not enhanced_desc or len(enhanced_desc) < 30:\n",
        "            return original_desc\n",
        "\n",
        "        # 1. 檢查數值一致性（如人數、物體數量等）\n",
        "        # 從原始描述中提取數字和相關名詞\n",
        "        number_patterns = [\n",
        "            (r'(\\d+)\\s+(people|person|pedestrians|individuals)', r'\\1', r'\\2'), # 人數\n",
        "            (r'(\\d+)\\s+(cars|vehicles|automobiles)', r'\\1', r'\\2'),            # 車輛數\n",
        "            (r'(\\d+)\\s+(buildings|structures)', r'\\1', r'\\2')                  # 建築數\n",
        "        ]\n",
        "\n",
        "        # 檢查原始描述中的每個數字\n",
        "        for pattern, num_group, word_group in number_patterns:\n",
        "            original_matches = re.finditer(pattern, original_desc, re.IGNORECASE)\n",
        "            for match in original_matches:\n",
        "                number = match.group(1)\n",
        "                noun = match.group(2)\n",
        "\n",
        "                # 檢查增強描述中是否保留了這個數字\n",
        "                # 創建一個更通用的模式來檢查增強描述中是否包含此數字和對象類別\n",
        "                enhanced_pattern = r'(\\d+)\\s+(' + re.escape(noun) + r'|' + re.escape(noun.rstrip('s')) + r'|' + re.escape(noun + 's') + r')'\n",
        "                enhanced_matches = list(re.finditer(enhanced_pattern, enhanced_desc, re.IGNORECASE))\n",
        "\n",
        "                if not enhanced_matches:\n",
        "                    # 數字+名詞未在增強描述中找到\n",
        "                    plural_form = noun if noun.endswith('s') or number == '1' else noun + 's'\n",
        "                    if enhanced_desc.startswith(\"This\") or enhanced_desc.startswith(\"The\"):\n",
        "                        enhanced_desc = enhanced_desc.replace(\"This \", f\"This scene with {number} {plural_form} \", 1)\n",
        "                        enhanced_desc = enhanced_desc.replace(\"The \", f\"The scene with {number} {plural_form} \", 1)\n",
        "                    else:\n",
        "                        enhanced_desc = f\"The scene includes {number} {plural_form}. \" + enhanced_desc\n",
        "                elif enhanced_matches and match.group(1) != number:\n",
        "                    # 存在但數字不一致，就要更正數字\n",
        "                    for ematch in enhanced_matches:\n",
        "                        wrong_number = ematch.group(1)\n",
        "                        enhanced_desc = enhanced_desc.replace(f\"{wrong_number} {ematch.group(2)}\", f\"{number} {ematch.group(2)}\")\n",
        "\n",
        "        # 2. 檢查視角的一致性\n",
        "        perspective_terms = {\n",
        "            \"aerial\": [\"aerial\", \"bird's-eye\", \"overhead\", \"top-down\", \"above\", \"looking down\"],\n",
        "            \"ground\": [\"street-level\", \"ground level\", \"eye-level\", \"standing\"],\n",
        "            \"indoor\": [\"inside\", \"interior\", \"indoor\", \"within\"],\n",
        "            \"close-up\": [\"close-up\", \"detailed view\", \"close shot\"]\n",
        "        }\n",
        "\n",
        "        # 確定原始視角\n",
        "        original_perspective = None\n",
        "        for persp, terms in perspective_terms.items():\n",
        "            if any(term in original_desc.lower() for term in terms):\n",
        "                original_perspective = persp\n",
        "                break\n",
        "\n",
        "        # 檢查是否保留了視角方面\n",
        "        if original_perspective:\n",
        "            enhanced_has_perspective = any(term in enhanced_desc.lower() for term in perspective_terms[original_perspective])\n",
        "\n",
        "            if not enhanced_has_perspective:\n",
        "                # 添加之前缺的視角方面\n",
        "                perspective_prefixes = {\n",
        "                    \"aerial\": \"From an aerial perspective, \",\n",
        "                    \"ground\": \"From street level, \",\n",
        "                    \"indoor\": \"In this indoor setting, \",\n",
        "                    \"close-up\": \"In this close-up view, \"\n",
        "                }\n",
        "\n",
        "                prefix = perspective_prefixes.get(original_perspective, \"\")\n",
        "                if prefix:\n",
        "                    if enhanced_desc[0].isupper():\n",
        "                        enhanced_desc = prefix + enhanced_desc[0].lower() + enhanced_desc[1:]\n",
        "                    else:\n",
        "                        enhanced_desc = prefix + enhanced_desc\n",
        "\n",
        "        # 3. 檢查場景類型一致性\n",
        "        if scene_type and scene_type.lower() != \"unknown\" and scene_type.lower() not in enhanced_desc.lower():\n",
        "            # 添加場景類型\n",
        "            if enhanced_desc.startswith(\"This \") or enhanced_desc.startswith(\"The \"):\n",
        "                # 避免產生 \"This scene\" 和 \"This intersection\" 的重複\n",
        "                if \"scene\" in enhanced_desc[:15].lower():\n",
        "                    fixed_type = scene_type.lower()\n",
        "                    enhanced_desc = enhanced_desc.replace(\"scene\", fixed_type, 1)\n",
        "                else:\n",
        "                    enhanced_desc = enhanced_desc.replace(\"This \", f\"This {scene_type} \", 1)\n",
        "                    enhanced_desc = enhanced_desc.replace(\"The \", f\"The {scene_type} \", 1)\n",
        "            else:\n",
        "                enhanced_desc = f\"This {scene_type} \" + enhanced_desc\n",
        "\n",
        "        # 4. 確保文字長度適當，這邊的限制要與prompt相同,否則會產生矛盾\n",
        "        words = enhanced_desc.split()\n",
        "        if len(words) > 200:\n",
        "            # 找尋接近字數限制的句子結束處\n",
        "            truncated = ' '.join(words[:200])\n",
        "            last_period = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))\n",
        "\n",
        "            if last_period > 0:\n",
        "                enhanced_desc = truncated[:last_period+1]\n",
        "            else:\n",
        "                enhanced_desc = truncated + '.'\n",
        "\n",
        "        return enhanced_desc\n",
        "\n",
        "    def _extract_perspective_from_description(self, description: str) -> str:\n",
        "        \"\"\"從原始描述中提取視角/透視信息\"\"\"\n",
        "        perspective_terms = {\n",
        "            \"aerial\": [\"aerial perspective\", \"aerial view\", \"bird's-eye view\", \"overhead view\", \"from above\"],\n",
        "            \"ground\": [\"ground level\", \"eye level\", \"street level\"],\n",
        "            \"indoor\": [\"indoor setting\", \"inside\", \"interior\"]\n",
        "        }\n",
        "\n",
        "        for persp_type, terms in perspective_terms.items():\n",
        "            for term in terms:\n",
        "                if term.lower() in description.lower():\n",
        "                    return term\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_objects_from_description(self, description: str) -> List[str]:\n",
        "        \"\"\"從原始描述中提取物件提及\"\"\"\n",
        "        # 常見物件正則表達式模式\n",
        "        object_patterns = [\n",
        "            r'(\\d+)\\s+(people|persons|pedestrians|individuals)',\n",
        "            r'(\\d+)\\s+(cars|vehicles|automobiles)',\n",
        "            r'(\\d+)\\s+(buildings|structures)',\n",
        "            r'(\\d+)\\s+(plants|potted plants|flowers)',\n",
        "            r'(\\d+)\\s+(beds|furniture|tables|chairs)'\n",
        "        ]\n",
        "\n",
        "        extracted_objects = []\n",
        "\n",
        "        for pattern in object_patterns:\n",
        "            matches = re.finditer(pattern, description, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                number = match.group(1)\n",
        "                object_type = match.group(2)\n",
        "                extracted_objects.append(f\"{number} {object_type}\")\n",
        "\n",
        "        return extracted_objects\n",
        "\n",
        "    def _ensure_scene_type_consistency(self, description: str, scene_type: str, original_desc: str) -> str:\n",
        "        \"\"\"確保描述中的場景類型與指定的場景類型一致\"\"\"\n",
        "        # 禁止使用的錯誤場景詞列表\n",
        "        prohibited_scene_words = [\"plaza\", \"square\", \"european\", \"asian\", \"american\"]\n",
        "\n",
        "        # 檢查是否包含禁止的場景詞\n",
        "        for word in prohibited_scene_words:\n",
        "            if word in description.lower() and word not in original_desc.lower() and word not in scene_type.lower():\n",
        "                # 替換錯誤場景詞為正確場景類型\n",
        "                pattern = re.compile(r'\\b' + word + r'\\b', re.IGNORECASE)\n",
        "                description = pattern.sub(scene_type, description)\n",
        "\n",
        "        # 確保場景類型在描述中被提及\n",
        "        if scene_type.lower() not in description.lower():\n",
        "            # 尋找通用場景詞並替換\n",
        "            for general_term in [\"scene\", \"area\", \"place\", \"location\"]:\n",
        "                if general_term in description.lower():\n",
        "                    pattern = re.compile(r'\\b' + general_term + r'\\b', re.IGNORECASE)\n",
        "                    description = pattern.sub(scene_type, description, count=1)\n",
        "                    break\n",
        "            else:\n",
        "                # 如果沒有找到通用詞，在開頭添加場景類型\n",
        "                if description.startswith(\"The \"):\n",
        "                    description = description.replace(\"The \", f\"The {scene_type} \", 1)\n",
        "                elif description.startswith(\"This \"):\n",
        "                    description = description.replace(\"This \", f\"This {scene_type} \", 1)\n",
        "                else:\n",
        "                    description = f\"This {scene_type} \" + description\n",
        "\n",
        "        return description\n",
        "\n",
        "    def _generate_llm_response(self, prompt: str) -> str:\n",
        "        \"\"\"生成 LLM 的回應\"\"\"\n",
        "        self._load_model()\n",
        "\n",
        "        try:\n",
        "            self.call_count += 1\n",
        "            self.logger.info(f\"LLM call #{self.call_count}\")\n",
        "\n",
        "            # 清除 GPU 緩存\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # 設置固定種子以提高一致性\n",
        "            torch.manual_seed(42)\n",
        "\n",
        "            # 準備輸入\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length).to(self.device)\n",
        "\n",
        "            # 根據模型類型調整參數\n",
        "            generation_params = {\n",
        "                \"max_new_tokens\": 120,\n",
        "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "                \"attention_mask\": inputs.attention_mask,\n",
        "                \"use_cache\": True,\n",
        "            }\n",
        "\n",
        "            # 為 Llama 模型設置特定參數\n",
        "            if \"llama\" in self.model_path.lower():\n",
        "                generation_params.update({\n",
        "                    \"temperature\": 0.4,        # 不要太高, 否則模型可能會太有主觀意見\n",
        "                    \"max_new_tokens\": 600,\n",
        "                    \"do_sample\": True,\n",
        "                    \"top_p\": 0.8,\n",
        "                    \"repetition_penalty\": 1.2,  # 重複的懲罰權重,可避免掉重複字\n",
        "                    \"num_beams\": 4 ,\n",
        "                    \"length_penalty\": 1.2,\n",
        "                })\n",
        "\n",
        "            else:\n",
        "                # 如果用其他模型的參數\n",
        "                generation_params.update({\n",
        "                    \"temperature\": 0.6,\n",
        "                    \"max_new_tokens\": 300,\n",
        "                    \"top_p\": 0.9,\n",
        "                    \"do_sample\": True,\n",
        "                    \"num_beams\": 1,\n",
        "                    \"repetition_penalty\": 1.05\n",
        "                })\n",
        "\n",
        "            # 生成回應\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(inputs.input_ids, **generation_params)\n",
        "\n",
        "            # 解碼完整輸出\n",
        "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # 提取生成的響應部分\n",
        "            assistant_tag = \"<|assistant|>\"\n",
        "            if assistant_tag in full_response:\n",
        "                response = full_response.split(assistant_tag)[-1].strip()\n",
        "\n",
        "                # 檢查是否有未閉合的 <|assistant|>\n",
        "                user_tag = \"<|user|>\"\n",
        "                if user_tag in response:\n",
        "                    response = response.split(user_tag)[0].strip()\n",
        "            else:\n",
        "                # 移除輸入提示\n",
        "                input_text = self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
        "                response = full_response\n",
        "                if response.startswith(input_text):\n",
        "                    response = response[len(input_text):].strip()\n",
        "\n",
        "            # 確保不返回空響應\n",
        "            if not response or len(response.strip()) < 10:\n",
        "                self.logger.warning(\"生成的回應為空的或太短，返回默認回應\")\n",
        "                return \"No detailed description could be generated.\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"生成 LLM 響應時出錯: {str(e)}\")\n",
        "            import traceback\n",
        "            self.logger.error(traceback.format_exc())\n",
        "            return \"Unable to generate enhanced description.\"\n",
        "\n",
        "    def _clean_llm_response(self, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean the LLM response to ensure the output contains only clean descriptive text.\n",
        "        Sometimes it will not only display the description but display tags, notes...etc\n",
        "\n",
        "        Args:\n",
        "            response: Original response from the LLM\n",
        "\n",
        "        Returns:\n",
        "            Cleaned description text\n",
        "        \"\"\"\n",
        "        if not response:\n",
        "            return \"\"\n",
        "\n",
        "        # Save original response as backup\n",
        "        original_response = response\n",
        "\n",
        "        # 1. Extract content between markers (if present)\n",
        "        output_start = response.find(\"[OUTPUT_START]\")\n",
        "        output_end = response.find(\"[OUTPUT_END]\")\n",
        "        if output_start != -1 and output_end != -1 and output_end > output_start:\n",
        "            response = response[output_start + len(\"[OUTPUT_START]\"):output_end].strip()\n",
        "\n",
        "        # 2. Remove all remaining section markers and instructions\n",
        "        section_markers = [\n",
        "            r'\\[.*?\\]',                      # [any text]\n",
        "            r'OUTPUT_START\\s*:|OUTPUT_END\\s*:',  # OUTPUT_START: or OUTPUT_END:\n",
        "            r'ENHANCED DESCRIPTION\\s*:',      # ENHANCED DESCRIPTION:\n",
        "            r'Scene Type\\s*:.*?(?=\\n|$)',    # Scene Type: text\n",
        "            r'Original Description\\s*:.*?(?=\\n|$)', # Original Description: text\n",
        "            r'GOOD\\s*:|BAD\\s*:',             # GOOD: or BAD:\n",
        "            r'PROBLEM\\s*:.*?(?=\\n|$)',       # PROBLEM: text\n",
        "            r'</?\\|(?:assistant|system|user)\\|>',  # Dialog markers\n",
        "            r'\\(Note:.*?\\)',                 # Notes in parentheses\n",
        "            r'\\(.*?I\\'ve.*?\\)',              # Common explanatory content\n",
        "            r'\\(.*?as per your request.*?\\)' # References to instructions\n",
        "        ]\n",
        "\n",
        "        for marker in section_markers:\n",
        "            response = re.sub(marker, '', response, flags=re.IGNORECASE)\n",
        "\n",
        "        # 3. Remove common prefixes and suffixes\n",
        "        prefixes_to_remove = [\n",
        "            \"Enhanced Description:\",\n",
        "            \"Scene Description:\",\n",
        "            \"Description:\",\n",
        "            \"Here is the enhanced description:\",\n",
        "            \"Here's the enhanced description:\"\n",
        "        ]\n",
        "\n",
        "        for prefix in prefixes_to_remove:\n",
        "            if response.lower().startswith(prefix.lower()):\n",
        "                response = response[len(prefix):].strip()\n",
        "\n",
        "        # 4. Remove any Context tags or text containing Context\n",
        "        response = re.sub(r'<\\s*Context:.*?>', '', response)\n",
        "        response = re.sub(r'Context:.*?(?=\\n|$)', '', response)\n",
        "        response = re.sub(r'Note:.*?(?=\\n|$)', '', response, flags=re.IGNORECASE)\n",
        "\n",
        "        # 5. Clean improper scene type references\n",
        "        scene_type_pattern = r'This ([a-zA-Z_]+) (features|shows|displays|contains)'\n",
        "        match = re.search(scene_type_pattern, response)\n",
        "        if match and '_' in match.group(1):\n",
        "            fixed_text = f\"This scene {match.group(2)}\"\n",
        "            response = re.sub(scene_type_pattern, fixed_text, response)\n",
        "\n",
        "        # 6. Reduce dash usage for more natural punctuation\n",
        "        response = re.sub(r'—', ', ', response)\n",
        "        response = re.sub(r' - ', ', ', response)\n",
        "\n",
        "        # 7. Remove excess whitespace and line breaks\n",
        "        response = response.replace('\\r', ' ')\n",
        "        response = re.sub(r'\\n+', ' ', response)  # 將所有換行符替換為空格\n",
        "        response = re.sub(r'\\s{2,}', ' ', response)  # 將多個空格替換為單個空格\n",
        "\n",
        "        # 8. Remove Markdown formatting\n",
        "        response = re.sub(r'\\*\\*|\\*|__|\\|', '', response)  # Remove Markdown indicators\n",
        "\n",
        "        # 9. Detect and remove sentence duplicates\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', response)\n",
        "        unique_sentences = []\n",
        "        seen_content = set()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Skip empty sentences\n",
        "            if not sentence.strip():\n",
        "                continue\n",
        "\n",
        "            # Create simplified version for comparison (lowercase, no punctuation)\n",
        "            simplified = re.sub(r'[^\\w\\s]', '', sentence.lower())\n",
        "            simplified = ' '.join(simplified.split())  # Standardize whitespace\n",
        "\n",
        "            # Check if we've seen a similar sentence\n",
        "            is_duplicate = False\n",
        "            for existing in seen_content:\n",
        "                if len(simplified) > 10 and (existing in simplified or simplified in existing):\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "\n",
        "            if not is_duplicate and simplified:\n",
        "                unique_sentences.append(sentence)\n",
        "                seen_content.add(simplified)\n",
        "\n",
        "        # Recombine unique sentences\n",
        "        response = ' '.join(unique_sentences)\n",
        "\n",
        "        # 10. Ensure word count is within limits (50-150 words)\n",
        "        words = response.split()\n",
        "        if len(words) > 200:\n",
        "            # Find sentence ending near the word limit\n",
        "            truncated = ' '.join(words[:200])\n",
        "            last_period = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))\n",
        "\n",
        "            if last_period > 0:\n",
        "                response = truncated[:last_period+1]\n",
        "            else:\n",
        "                response = truncated + \".\"\n",
        "\n",
        "        # 11. Check sentence completeness\n",
        "        if response and not response.strip()[-1] in ['.', '!', '?']:\n",
        "            # Find the last preposition or conjunction\n",
        "            common_prepositions = [\"into\", \"onto\", \"about\", \"above\", \"across\", \"after\", \"along\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"by\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \"inside\", \"near\", \"of\", \"off\", \"on\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"upon\", \"with\", \"within\"]\n",
        "\n",
        "            # Check if ending with preposition or conjunction\n",
        "            last_word = response.strip().split()[-1].lower() if response.strip().split() else \"\"\n",
        "            if last_word in common_prepositions or last_word in [\"and\", \"or\", \"but\"]:\n",
        "                # Find the last complete sentence\n",
        "                last_period = max(response.rfind('.'), response.rfind('!'), response.rfind('?'))\n",
        "                if last_period > 0:\n",
        "                    response = response[:last_period+1]\n",
        "                else:\n",
        "                    # If no complete sentence found, modify the ending\n",
        "                    words = response.strip().split()\n",
        "                    if words:\n",
        "                        # Remove the last preposition or conjunction\n",
        "                        response = \" \".join(words[:-1]) + \".\"\n",
        "\n",
        "        # 12. Ensure haven't over-filtered\n",
        "        if not response or len(response) < 40:\n",
        "            # Try to get the first meaningful paragraph from the original response\n",
        "            paragraphs = [p for p in original_response.split('\\n\\n') if p.strip()]\n",
        "            if paragraphs:\n",
        "                # Choose the longest paragraph as it's most likely the actual description\n",
        "                best_para = max(paragraphs, key=len)\n",
        "                # Clean it using a subset of the above rules\n",
        "                best_para = re.sub(r'\\[.*?\\]', '', best_para)  # Remove [SECTION] markers\n",
        "                best_para = re.sub(r'\\s{2,}', ' ', best_para).strip()  # Clean whitespace\n",
        "\n",
        "                if len(best_para) >= 40:\n",
        "                    return best_para\n",
        "\n",
        "            # If still no good content, return a simple message\n",
        "            return \"Unable to generate a valid enhanced description.\"\n",
        "\n",
        "        # 13. Final cleaning - catch any missed special cases\n",
        "        response = re.sub(r'</?\\|.*?\\|>', '', response)  # Any remaining tags\n",
        "        response = re.sub(r'\\(.*?\\)', '', response)  # Any remaining parenthetical content\n",
        "        response = re.sub(r'Note:.*?(?=\\n|$)', '', response, flags=re.IGNORECASE)  # Any remaining notes\n",
        "\n",
        "        # Ensure proper spacing after punctuation\n",
        "        response = re.sub(r'([.!?])([A-Z])', r'\\1 \\2', response)\n",
        "\n",
        "        # Ensure first letter is capitalized\n",
        "        if response and response[0].islower():\n",
        "            response = response[0].upper() + response[1:]\n",
        "\n",
        "        # 14. 統一格式 - 確保輸出始終是單一段落\n",
        "        response = re.sub(r'\\s*\\n\\s*', ' ', response)  # 將所有換行符替換為空格\n",
        "        response = ' '.join(response.split())\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    def _format_objects_for_prompt(self, objects: List[Dict]) -> str:\n",
        "        \"\"\"格式化物體列表以用於提示\"\"\"\n",
        "        if not objects:\n",
        "            return \"No objects detected\"\n",
        "\n",
        "        formatted = []\n",
        "        for obj in objects:\n",
        "            formatted.append(f\"{obj['class_name']} (confidence: {obj['confidence']:.2f})\")\n",
        "\n",
        "        return \"\\n- \" + \"\\n- \".join(formatted)\n",
        "\n",
        "\n",
        "    def _format_clip_results(self, clip_analysis: Dict) -> str:\n",
        "        \"\"\"格式化CLIP分析結果以用於提示\"\"\"\n",
        "        if not clip_analysis or \"error\" in clip_analysis:\n",
        "            return \"No CLIP analysis available\"\n",
        "\n",
        "        parts = [\"CLIP Analysis Results:\"]\n",
        "\n",
        "        # 加上頂級場景\n",
        "        top_scene, confidence = clip_analysis.get(\"top_scene\", (\"unknown\", 0))\n",
        "        parts.append(f\"- Most likely scene: {top_scene} (confidence: {confidence:.2f})\")\n",
        "\n",
        "        # 加上視角\n",
        "        viewpoint, vp_conf = clip_analysis.get(\"viewpoint\", (\"standard\", 0))\n",
        "        parts.append(f\"- Camera viewpoint: {viewpoint} (confidence: {vp_conf:.2f})\")\n",
        "\n",
        "        # 加上物體組合\n",
        "        if \"object_combinations\" in clip_analysis:\n",
        "            combos = []\n",
        "            for combo, score in clip_analysis[\"object_combinations\"][:3]:\n",
        "                combos.append(f\"{combo} ({score:.2f})\")\n",
        "            parts.append(f\"- Object combinations: {', '.join(combos)}\")\n",
        "\n",
        "        # 加上文化分析\n",
        "        if \"cultural_analysis\" in clip_analysis:\n",
        "            parts.append(\"- Cultural analysis:\")\n",
        "            for culture_type, data in clip_analysis[\"cultural_analysis\"].items():\n",
        "                best_desc = data.get(\"best_description\", \"\")\n",
        "                desc_conf = data.get(\"confidence\", 0)\n",
        "                parts.append(f\"  * {culture_type}: {best_desc} ({desc_conf:.2f})\")\n",
        "\n",
        "        return \"\\n\".join(parts)\n",
        "\n",
        "    def _format_cultural_analysis(self, cultural_analysis: Dict) -> str:\n",
        "        \"\"\"格式化文化分析結果\"\"\"\n",
        "        if not cultural_analysis:\n",
        "            return \"No specific cultural elements detected\"\n",
        "\n",
        "        parts = []\n",
        "        for culture_type, data in cultural_analysis.items():\n",
        "            best_desc = data.get(\"best_description\", \"\")\n",
        "            desc_conf = data.get(\"confidence\", 0)\n",
        "            parts.append(f\"{culture_type}: {best_desc} (confidence: {desc_conf:.2f})\")\n",
        "\n",
        "        return \"\\n\".join(parts)"
      ],
      "metadata": {
        "id": "rlITUwrijwFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a79c121-2edf-4944-f5e4-ab62c79b7404"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm_enhancer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import uuid\n",
        "# import spaces\n",
        "\n",
        "# from detection_model import DetectionModel\n",
        "# from color_mapper import ColorMapper\n",
        "# from evaluation_metrics import EvaluationMetrics\n",
        "# from style import Style\n",
        "# from image_processor import ImageProcessor\n",
        "# from video_processor import VideoProcessor\n",
        "# from llm_enhancer import LLMEnhancer\n",
        "\n",
        "# Initialize Processors with LLM support\n",
        "image_processor = ImageProcessor(use_llm=True, llm_model_path=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "video_processor = VideoProcessor(image_processor)\n",
        "\n",
        "# Helper Function\n",
        "def get_all_classes():\n",
        "    \"\"\"Gets all available COCO classes.\"\"\"\n",
        "    # Try to get from a loaded model first\n",
        "    if image_processor and image_processor.model_instances:\n",
        "         for model_instance in image_processor.model_instances.values():\n",
        "              if model_instance and model_instance.is_model_loaded:\n",
        "                   try:\n",
        "                        # Ensure class_names is a dict {id: name}\n",
        "                        if isinstance(model_instance.class_names, dict):\n",
        "                             return sorted([(int(idx), name) for idx, name in model_instance.class_names.items()])\n",
        "                   except Exception as e:\n",
        "                        print(f\"Error getting class names from model: {e}\")\n",
        "\n",
        "    # Fallback to standard COCO (ensure keys are ints)\n",
        "    default_classes = {\n",
        "        0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n",
        "        6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "        11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n",
        "        16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "        22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag',\n",
        "        27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard',\n",
        "        32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n",
        "        36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle',\n",
        "        40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n",
        "        46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n",
        "        51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair',\n",
        "        57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet',\n",
        "        62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "        67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink',\n",
        "        72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors',\n",
        "        77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n",
        "    }\n",
        "    return sorted(default_classes.items())\n",
        "\n",
        "# @spaces.GPU\n",
        "def handle_image_upload(image, model_name, confidence_threshold, filter_classes=None, use_llm=True):\n",
        "    \"\"\"Processes a single uploaded image.\"\"\"\n",
        "    print(f\"Processing image with model: {model_name}, confidence: {confidence_threshold}, use_llm: {use_llm}\")\n",
        "    try:\n",
        "        image_processor.use_llm = use_llm\n",
        "        if hasattr(image_processor, 'scene_analyzer'):\n",
        "            image_processor.scene_analyzer.use_llm = use_llm\n",
        "            print(f\"Updated existing scene_analyzer use_llm setting to: {use_llm}\")\n",
        "\n",
        "        class_ids_to_filter = None\n",
        "        if filter_classes:\n",
        "            class_ids_to_filter = []\n",
        "            available_classes_dict = dict(get_all_classes())\n",
        "            name_to_id = {name: id for id, name in available_classes_dict.items()}\n",
        "            for class_str in filter_classes:\n",
        "                class_name_or_id = class_str.split(\":\")[0].strip()\n",
        "                class_id = -1\n",
        "                try:\n",
        "                    class_id = int(class_name_or_id)\n",
        "                    if class_id not in available_classes_dict:\n",
        "                        class_id = -1\n",
        "                except ValueError:\n",
        "                    if class_name_or_id in name_to_id:\n",
        "                        class_id = name_to_id[class_name_or_id]\n",
        "                    elif class_str in name_to_id: # Check full string \"id: name\"\n",
        "                        class_id = name_to_id[class_str]\n",
        "\n",
        "                if class_id != -1:\n",
        "                    class_ids_to_filter.append(class_id)\n",
        "                else:\n",
        "                    print(f\"Warning: Could not parse class filter: {class_str}\")\n",
        "            print(f\"Filtering image results for class IDs: {class_ids_to_filter}\")\n",
        "\n",
        "        # Call the existing image processing logic\n",
        "        result_image, result_text, stats = image_processor.process_image(\n",
        "            image,\n",
        "            model_name,\n",
        "            confidence_threshold,\n",
        "            class_ids_to_filter\n",
        "        )\n",
        "\n",
        "        # Format stats for JSON display\n",
        "        formatted_stats = image_processor.format_json_for_display(stats)\n",
        "\n",
        "        # Prepare visualization data for the plot\n",
        "        plot_figure = None\n",
        "        if stats and \"class_statistics\" in stats and stats[\"class_statistics\"]:\n",
        "            available_classes_dict = dict(get_all_classes())\n",
        "            viz_data = image_processor.prepare_visualization_data(stats, available_classes_dict)\n",
        "            if \"error\" not in viz_data:\n",
        "                 plot_figure = EvaluationMetrics.create_enhanced_stats_plot(viz_data)\n",
        "            else:\n",
        "                 fig, ax = plt.subplots(figsize=(8, 6))\n",
        "                 ax.text(0.5, 0.5, viz_data[\"error\"], ha='center', va='center', fontsize=12)\n",
        "                 ax.axis('off')\n",
        "                 plot_figure = fig\n",
        "        else:\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            ax.text(0.5, 0.5, \"No detection data for plot\", ha='center', va='center', fontsize=12)\n",
        "            ax.axis('off')\n",
        "            plot_figure = fig\n",
        "\n",
        "        # Extract scene analysis info\n",
        "        scene_analysis = stats.get(\"scene_analysis\", {})\n",
        "        scene_desc = scene_analysis.get(\"description\", \"Scene analysis requires detected objects.\")\n",
        "        # Ensure scene_desc is a string before adding HTML\n",
        "        if not isinstance(scene_desc, str):\n",
        "            scene_desc = str(scene_desc)\n",
        "\n",
        "        def clean_description(desc):\n",
        "            if not desc:\n",
        "                return \"\"\n",
        "\n",
        "            # 先過濾問答格式\n",
        "            if \"Questions:\" in desc:\n",
        "                desc = desc.split(\"Questions:\")[0].strip()\n",
        "            if \"Answers:\" in desc:\n",
        "                desc = desc.split(\"Answers:\")[0].strip()\n",
        "\n",
        "            # 然後按行過濾代碼和其他非敘述內容\n",
        "            lines = desc.split('\\n')\n",
        "            clean_lines = []\n",
        "            skip_block = False\n",
        "\n",
        "            for line in lines:\n",
        "                # 檢測問題格式\n",
        "                if re.match(r'^\\d+\\.\\s+(What|How|Why|When|Where|Who|The)', line):\n",
        "                    continue\n",
        "\n",
        "                # 檢查需要跳過的行\n",
        "                if line.strip().startswith(':param') or line.strip().startswith('\"\"\"'):\n",
        "                    continue\n",
        "                if line.strip().startswith(\"Exercise\") or \"class SceneDescriptionSystem\" in line:\n",
        "                    skip_block = True\n",
        "                    continue\n",
        "                if ('def generate_scene_description' in line or\n",
        "                    'def enhance_scene_descriptions' in line or\n",
        "                    'def __init__' in line):\n",
        "                    skip_block = True\n",
        "                    continue\n",
        "                if line.strip().startswith('#TEST'):\n",
        "                    skip_block = True\n",
        "                    continue\n",
        "\n",
        "                if skip_block and line.strip() == \"\":\n",
        "                    skip_block = False\n",
        "\n",
        "                # 如果不需要跳過\n",
        "                if not skip_block:\n",
        "                    clean_lines.append(line)\n",
        "\n",
        "            cleaned_text = '\\n'.join(clean_lines)\n",
        "\n",
        "            # 如果清理後為空，返回原始描述的第一段作為保險\n",
        "            if not cleaned_text.strip():\n",
        "                paragraphs = [p.strip() for p in desc.split('\\n\\n') if p.strip()]\n",
        "                if paragraphs:\n",
        "                    return paragraphs[0]\n",
        "                return desc\n",
        "\n",
        "            return cleaned_text\n",
        "\n",
        "        # 獲取和處理場景描述\n",
        "        scene_analysis = stats.get(\"scene_analysis\", {})\n",
        "        print(\"Processing scene_analysis:\", scene_analysis.keys())\n",
        "\n",
        "        # 獲取原始描述\n",
        "        scene_desc = scene_analysis.get(\"description\", \"Scene analysis requires detected objects.\")\n",
        "        if not isinstance(scene_desc, str):\n",
        "            scene_desc = str(scene_desc)\n",
        "\n",
        "        print(f\"Original scene description (first 50 chars): {scene_desc[:50]}...\")\n",
        "\n",
        "        # 確保使用的是有效的描述\n",
        "        clean_scene_desc = clean_description(scene_desc)\n",
        "        print(f\"Cleaned scene description (first 50 chars): {clean_scene_desc[:50]}...\")\n",
        "\n",
        "        # 即使清理後為空也確保顯示原始內容\n",
        "        if not clean_scene_desc.strip():\n",
        "            clean_scene_desc = scene_desc\n",
        "\n",
        "        # 創建原始描述的HTML\n",
        "        scene_desc_html = f\"<div>{clean_scene_desc}</div>\"\n",
        "\n",
        "        # 獲取LLM增強描述並且確保設置默認值為空字符串而非 None，不然會有None type Error\n",
        "        enhanced_description = scene_analysis.get(\"enhanced_description\", \"\")\n",
        "        if enhanced_description is None:\n",
        "            enhanced_description = \"\"\n",
        "\n",
        "        if not enhanced_description or not enhanced_description.strip():\n",
        "            print(\"WARNING: LLM enhanced description is empty!\")\n",
        "\n",
        "        # 準備徽章和描述標籤\n",
        "        llm_badge = \"\"\n",
        "        description_to_show = \"\"\n",
        "\n",
        "        if use_llm and enhanced_description:\n",
        "            llm_badge = '<span style=\"display:inline-block; margin-left:8px; padding:3px 10px; border-radius:12px; background: linear-gradient(90deg, #38b2ac, #4299e1); color:white; font-size:0.7rem; font-weight:bold; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); border: 1px solid rgba(255, 255, 255, 0.2);\">LLM Enhanced</span>'\n",
        "            description_to_show = enhanced_description\n",
        "            # 在 Original Scene Analysis 折疊區顯示原始的描述\n",
        "        else:\n",
        "            llm_badge = '<span style=\"display:inline-block; margin-left:8px; padding:3px 10px; border-radius:12px; background-color:#718096; color:white; font-size:0.7rem; font-weight:bold;\">Basic</span>'\n",
        "            description_to_show = clean_scene_desc\n",
        "            # 不使用 LLM 時，折疊區不顯示內容\n",
        "\n",
        "        # 使用LLM敘述時會有徽章標籤在標題上\n",
        "        scene_description_html = f'''\n",
        "        <div>\n",
        "            <div class=\"section-heading\" style=\"font-size:1.2rem; margin-top:15px;\">Scene Description {llm_badge}\n",
        "                <span style=\"font-size:0.8rem; color:#666; font-weight:normal; display:block; margin-top:2px;\">\n",
        "                    {('(Enhanced by AI language model)' if use_llm and enhanced_description else '(Based on object detection)')}\n",
        "                </span>\n",
        "            </div>\n",
        "            <div style=\"padding:15px; background-color:#ffffff; border-radius:8px; border:1px solid #e2e8f0; margin-bottom:20px; box-shadow:0 1px 3px rgba(0,0,0,0.05);\">\n",
        "                {description_to_show}\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "        # 原始描述只在使用 LLM 且有增強描述時在折疊區顯示\n",
        "        original_desc_visibility = \"block\" if use_llm and enhanced_description else \"none\"\n",
        "        original_desc_html = f'''\n",
        "        <div id=\"original_scene_analysis_accordion\" style=\"display: {original_desc_visibility};\">\n",
        "            <div style=\"padding:15px; background-color:#f0f0f0; border-radius:8px; border:1px solid #e2e8f0;\">\n",
        "                {clean_scene_desc}\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "        # Prepare activities list\n",
        "        activities_list = scene_analysis.get(\"possible_activities\", [])\n",
        "        if not activities_list:\n",
        "            activities_list_data = [[\"No specific activities inferred\"]] # Data for Dataframe\n",
        "        else:\n",
        "            activities_list_data = [[activity] for activity in activities_list]\n",
        "\n",
        "        # Prepare safety concerns list\n",
        "        safety_concerns_list = scene_analysis.get(\"safety_concerns\", [])\n",
        "        if not safety_concerns_list:\n",
        "            safety_data = [[\"No safety concerns detected\"]] # Data for Dataframe\n",
        "        else:\n",
        "            safety_data = [[concern] for concern in safety_concerns_list]\n",
        "\n",
        "        zones = scene_analysis.get(\"functional_zones\", {})\n",
        "        lighting = scene_analysis.get(\"lighting_conditions\", {\"time_of_day\": \"unknown\", \"confidence\": 0})\n",
        "\n",
        "        # 如果描述為空，記錄警告\n",
        "        if not clean_scene_desc.strip():\n",
        "            print(\"WARNING: Scene description is empty after cleaning!\")\n",
        "        if not enhanced_description.strip():\n",
        "            print(\"WARNING: LLM enhanced description is empty!\")\n",
        "\n",
        "        return (result_image, result_text, formatted_stats, plot_figure,\n",
        "            scene_description_html, original_desc_html,\n",
        "            activities_list_data, safety_data, zones, lighting)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in handle_image_upload: {e}\")\n",
        "        import traceback\n",
        "        error_msg = f\"Error processing image: {str(e)}\\n{traceback.format_exc()}\"\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, \"Processing Error\", color=\"red\", ha=\"center\", va=\"center\")\n",
        "        ax.axis('off')\n",
        "        # Ensure return structure matches outputs even on error\n",
        "        return (None, error_msg, {}, fig, f\"<div>Error: {str(e)}</div>\", \"Error\",\n",
        "            [[\"Error\"]], [[\"Error\"]], {}, {\"time_of_day\": \"error\", \"confidence\": 0})\n",
        "\n",
        "def download_video_from_url(video_url, max_duration_minutes=10):\n",
        "    \"\"\"\n",
        "    Downloads a video from a YouTube URL and returns the local path to the downloaded file.\n",
        "\n",
        "    Args:\n",
        "        video_url (str): URL of the YouTube video to download\n",
        "        max_duration_minutes (int): Maximum allowed video duration in minutes\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Path to the downloaded video file or None, Error message or None)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a temporary directory to store the video\n",
        "        temp_dir = tempfile.gettempdir()\n",
        "        output_filename = f\"downloaded_{uuid.uuid4().hex}.mp4\"\n",
        "        output_path = os.path.join(temp_dir, output_filename)\n",
        "\n",
        "        # Check if it's a YouTube URL\n",
        "        if \"youtube.com\" in video_url or \"youtu.be\" in video_url:\n",
        "            # Import yt-dlp here to avoid dependency if not needed\n",
        "            import yt_dlp\n",
        "\n",
        "            # Setup yt-dlp options\n",
        "            ydl_opts = {\n",
        "                'format': 'best[ext=mp4]/best',  # Best quality MP4 or best available format\n",
        "                'outtmpl': output_path,\n",
        "                'noplaylist': True,\n",
        "                'quiet': False,  # Set to True to reduce output\n",
        "                'no_warnings': False,\n",
        "            }\n",
        "\n",
        "            # First extract info to check duration\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                print(f\"Extracting info from YouTube URL: {video_url}\")\n",
        "                info_dict = ydl.extract_info(video_url, download=False)\n",
        "\n",
        "                # Check if video exists\n",
        "                if not info_dict:\n",
        "                    return None, \"Could not retrieve video information. Please check the URL.\"\n",
        "\n",
        "                video_title = info_dict.get('title', 'Unknown Title')\n",
        "                duration = info_dict.get('duration', 0)\n",
        "\n",
        "                print(f\"Video title: {video_title}\")\n",
        "                print(f\"Video duration: {duration} seconds\")\n",
        "\n",
        "                # Check video duration\n",
        "                if duration > max_duration_minutes * 60:\n",
        "                    return None, f\"Video is too long ({duration} seconds). Maximum duration is {max_duration_minutes} minutes.\"\n",
        "\n",
        "                # Download the video\n",
        "                print(f\"Downloading YouTube video: {video_title}\")\n",
        "                ydl.download([video_url])\n",
        "\n",
        "            # Verify the file exists and has content\n",
        "            if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "                return None, \"Download failed: Empty or missing file.\"\n",
        "\n",
        "            print(f\"Successfully downloaded video to: {output_path}\")\n",
        "            return output_path, None\n",
        "        else:\n",
        "            return None, \"Only YouTube URLs are supported at this time. Please enter a valid YouTube URL.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_details = traceback.format_exc()\n",
        "        print(f\"Error downloading video: {e}\\n{error_details}\")\n",
        "        return None, f\"Error downloading video: {str(e)}\"\n",
        "\n",
        "\n",
        "# @spaces.GPU\n",
        "def handle_video_upload(video_input, video_url, input_type, model_name, confidence_threshold, process_interval):\n",
        "    \"\"\"Handles video upload or URL input and calls the VideoProcessor.\"\"\"\n",
        "\n",
        "    print(f\"Received video request: input_type={input_type}\")\n",
        "    video_path = None\n",
        "\n",
        "    # Handle based on input type\n",
        "    if input_type == \"upload\" and video_input:\n",
        "        print(f\"Processing uploaded video file\")\n",
        "        video_path = video_input\n",
        "    elif input_type == \"url\" and video_url:\n",
        "        print(f\"Processing video from URL: {video_url}\")\n",
        "        # Download video from URL\n",
        "        video_path, error_message = download_video_from_url(video_url)\n",
        "        if error_message:\n",
        "            error_html = f\"<div class='video-summary-content-wrapper'><pre>{error_message}</pre></div>\"\n",
        "            return None, error_html, {\"error\": error_message}\n",
        "    else:\n",
        "        print(\"No valid video input provided.\")\n",
        "        return None, \"<div class='video-summary-content-wrapper'><pre>Please upload a video file or provide a valid video URL.</pre></div>\", {}\n",
        "\n",
        "    print(f\"Starting video processing with: model={model_name}, confidence={confidence_threshold}, interval={process_interval}\")\n",
        "    try:\n",
        "        # Call the VideoProcessor method\n",
        "        output_video_path, summary_text, stats_dict = video_processor.process_video_file(\n",
        "            video_path=video_path,\n",
        "            model_name=model_name,\n",
        "            confidence_threshold=confidence_threshold,\n",
        "            process_interval=int(process_interval) # Ensure interval is int\n",
        "        )\n",
        "        print(f\"Video processing function returned: path={output_video_path}, summary length={len(summary_text)}\")\n",
        "\n",
        "        # Wrap processing summary in HTML tags for consistent styling with scene understanding page\n",
        "        summary_html = f\"<div class='video-summary-content-wrapper'><pre>{summary_text}</pre></div>\"\n",
        "\n",
        "        # Format statistics for better display\n",
        "        formatted_stats = {}\n",
        "        if stats_dict and isinstance(stats_dict, dict):\n",
        "            formatted_stats = stats_dict\n",
        "\n",
        "        return output_video_path, summary_html, formatted_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in handle_video_upload: {e}\")\n",
        "        import traceback\n",
        "        error_msg = f\"Error processing video: {str(e)}\\n{traceback.format_exc()}\"\n",
        "        error_html = f\"<div class='video-summary-content-wrapper'><pre>{error_msg}</pre></div>\"\n",
        "        return None, error_html, {\"error\": str(e)}\n",
        "\n",
        "\n",
        "# Create Gradio Interface\n",
        "def create_interface():\n",
        "    \"\"\"Creates the Gradio interface with Tabs.\"\"\"\n",
        "    css = Style.get_css()\n",
        "    available_models = DetectionModel.get_available_models()\n",
        "    model_choices = [model[\"model_file\"] for model in available_models]\n",
        "    class_choices_formatted = [f\"{id}: {name}\" for id, name in get_all_classes()] # Use formatted choices\n",
        "\n",
        "    with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue=\"teal\", secondary_hue=\"blue\")) as demo:\n",
        "\n",
        "        # Header\n",
        "        with gr.Group(elem_classes=\"app-header\"):\n",
        "              gr.HTML(\"\"\"\n",
        "                    <div style=\"text-align: center; width: 100%; padding: 2rem 0 3rem 0; background: linear-gradient(135deg, #f0f9ff, #e1f5fe);\">\n",
        "                        <h1 style=\"font-size: 3.5rem; margin-bottom: 0.5rem; background: linear-gradient(90deg, #38b2ac, #4299e1); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: bold; font-family: 'Arial', sans-serif;\">VisionScout</h1>\n",
        "                        <h2 style=\"color: #4A5568; font-size: 1.2rem; font-weight: 400; margin-top: 0.5rem; margin-bottom: 1.5rem; font-family: 'Arial', sans-serif;\">Object Detection and Scene Understanding</h2>\n",
        "                        <div style=\"display: flex; justify-content: center; gap: 10px; margin: 0.5rem 0;\"><div style=\"height: 3px; width: 80px; background: linear-gradient(90deg, #38b2ac, #4299e1);\"></div></div>\n",
        "                        <div style=\"display: flex; justify-content: center; gap: 25px; margin-top: 1.5rem;\">\n",
        "                            <div style=\"padding: 8px 15px; border-radius: 20px; background: rgba(66, 153, 225, 0.15); color: #2b6cb0; font-weight: 500; font-size: 0.9rem;\"><span style=\"margin-right: 6px;\">🖼️</span> Image Analysis</div>\n",
        "                            <div style=\"padding: 8px 15px; border-radius: 20px; background: rgba(56, 178, 172, 0.15); color: #2b6cb0; font-weight: 500; font-size: 0.9rem;\"><span style=\"margin-right: 6px;\">🎬</span> Video Analysis</div>\n",
        "                        </div>\n",
        "                         <div style=\"margin-top: 20px; padding: 10px 15px; background-color: rgba(255, 248, 230, 0.9); border-left: 3px solid #f6ad55; border-radius: 6px; max-width: 600px; margin-left: auto; margin-right: auto; text-align: left;\">\n",
        "                             <p style=\"margin: 0; font-size: 0.9rem; color: #805ad5; font-weight: 500;\">\n",
        "                                 <span style=\"margin-right: 5px;\">📱</span> iPhone users: HEIC images may not be supported.\n",
        "                                 <a href=\"https://cloudconvert.com/heic-to-jpg\" target=\"_blank\" style=\"color: #3182ce; text-decoration: underline;\">Convert HEIC to JPG</a> before uploading if needed.\n",
        "                             </p>\n",
        "                         </div>\n",
        "                    </div>\n",
        "                \"\"\")\n",
        "\n",
        "        # Main Content with Tabs\n",
        "        with gr.Tabs(elem_classes=\"tabs\"):\n",
        "\n",
        "            # Tab 1: Image Processing\n",
        "            with gr.Tab(\"Image Processing\"):\n",
        "                current_image_model = gr.State(\"yolov8m.pt\") # State for image model selection\n",
        "                with gr.Row(equal_height=False): # Allow columns to have different heights\n",
        "                    # Left Column: Image Input & Controls\n",
        "                    with gr.Column(scale=4, elem_classes=\"input-panel\"):\n",
        "                        with gr.Group():\n",
        "                            gr.HTML('<div class=\"section-heading\">Upload Image</div>')\n",
        "                            image_input = gr.Image(type=\"pil\", label=\"Upload an image\", elem_classes=\"upload-box\")\n",
        "\n",
        "                            with gr.Accordion(\"Image Analysis Settings\", open=False):\n",
        "                                image_model_dropdown = gr.Dropdown(\n",
        "                                    choices=model_choices,\n",
        "                                    value=\"yolov8m.pt\", # Default for images\n",
        "                                    label=\"Select Model\",\n",
        "                                    info=\"Choose speed vs. accuracy (n=fast, m=balanced, x=accurate)\"\n",
        "                                )\n",
        "                                # Display model info\n",
        "                                image_model_info = gr.Markdown(DetectionModel.get_model_description(\"yolov8m.pt\"))\n",
        "\n",
        "                                image_confidence = gr.Slider(\n",
        "                                    minimum=0.1, maximum=0.9, value=0.25, step=0.05,\n",
        "                                    label=\"Confidence Threshold\",\n",
        "                                    info=\"Minimum confidence for displaying a detected object\"\n",
        "                                )\n",
        "\n",
        "                                use_llm = gr.Checkbox(\n",
        "                                    label=\"Use LLM for enhanced scene descriptions\",\n",
        "                                    value=True,\n",
        "                                    info=\"Provides more detailed and natural language descriptions (may increase processing time)\"\n",
        "                                )\n",
        "\n",
        "                                with gr.Accordion(\"Filter Classes\", open=False):\n",
        "                                     gr.HTML('<div class=\"section-heading\" style=\"font-size: 1rem;\">Common Categories</div>')\n",
        "                                     with gr.Row():\n",
        "                                         people_btn = gr.Button(\"People\", size=\"sm\")\n",
        "                                         vehicles_btn = gr.Button(\"Vehicles\", size=\"sm\")\n",
        "                                         animals_btn = gr.Button(\"Animals\", size=\"sm\")\n",
        "                                         objects_btn = gr.Button(\"Common Objects\", size=\"sm\")\n",
        "                                     image_class_filter = gr.Dropdown(\n",
        "                                         choices=class_choices_formatted, # Use formatted choices\n",
        "                                         multiselect=True,\n",
        "                                         label=\"Select Classes to Display\",\n",
        "                                         info=\"Leave empty to show all detected objects\"\n",
        "                                     )\n",
        "\n",
        "                        image_detect_btn = gr.Button(\"Analyze Image\", variant=\"primary\", elem_classes=\"detect-btn\")\n",
        "\n",
        "                        with gr.Group(elem_classes=\"how-to-use\"):\n",
        "                             gr.HTML('<div class=\"section-heading\">How to Use (Image)</div>')\n",
        "                             gr.Markdown(\"\"\"\n",
        "                                    1. Upload an image or use the camera\n",
        "                                    2. (Optional) Adjust settings like confidence threshold or model size (n, m=balanced, x=accurate)\n",
        "                                    3. In Analysis Settings, you can uncheck \"Use LLM for enhanced scene descriptions\" if you prefer faster processing\n",
        "                                    4. Optionally filter to specific object classes\n",
        "                                    5. Click **Detect Objects** button\n",
        "                                \"\"\")\n",
        "                        # Image Examples\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/room_01.jpg\",\n",
        "                                \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/room_02.jpg\",\n",
        "                                \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/street_02.jpg\",\n",
        "                                \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/street_04.jpg\"\n",
        "                                ],\n",
        "                            inputs=image_input,\n",
        "                            label=\"Example Images\"\n",
        "                         )\n",
        "\n",
        "                    # Right Column: Image Results\n",
        "                    with gr.Column(scale=6, elem_classes=\"output-panel\"):\n",
        "                        with gr.Tabs(elem_classes=\"tabs\"):\n",
        "                            with gr.Tab(\"Detection Result\"):\n",
        "                                image_result_image = gr.Image(type=\"pil\", label=\"Detection Result\")\n",
        "                                gr.HTML('<div class=\"section-heading\">Detection Details</div>')\n",
        "                                image_result_text = gr.Textbox(label=None, lines=10, elem_id=\"detection-details\", container=False)\n",
        "\n",
        "                            with gr.Tab(\"Scene Understanding\"):\n",
        "                                gr.HTML('<div class=\"section-heading\">Scene Analysis</div>')\n",
        "                                gr.HTML(\"\"\"\n",
        "                                    <details class=\"info-details\" style=\"margin: 5px 0 15px 0;\">\n",
        "                                        <summary style=\"padding: 8px; background-color: #f0f7ff; border-radius: 6px; border-left: 3px solid #4299e1; font-weight: bold; cursor: pointer; color: #2b6cb0;\">\n",
        "                                            🔍 The AI Vision Scout Report: Click for important notes about this analysis\n",
        "                                        </summary>\n",
        "                                        <div style=\"margin-top: 8px; padding: 10px; background-color: #f8f9fa; border-radius: 6px; border: 1px solid #e2e8f0;\">\n",
        "                                            <p style=\"font-size: 13px; color: #718096; margin: 0;\">\n",
        "                                                <b>About this analysis:</b> This analysis is the model's best guess based on visible objects.\n",
        "                                                Like human scouts, it sometimes gets lost or sees things that aren't there (but don't we all?).\n",
        "                                                Consider this an educated opinion rather than absolute truth. For critical applications, always verify with human eyes! 🧐\n",
        "                                            </p>\n",
        "                                        </div>\n",
        "                                    </details>\n",
        "                                \"\"\")\n",
        "\n",
        "                                gr.HTML('''\n",
        "                                    <div style=\"margin-top: 5px; padding: 6px 10px; background-color: #f0f9ff; border-radius: 4px; border-left: 3px solid #63b3ed; font-size: 12px; margin-bottom: 10px;\">\n",
        "                                        <p style=\"margin: 0; color: #4a5568;\">\n",
        "                                            <b>Note:</b> AI descriptions may vary slightly with each generation, reflecting the creative nature of AI. This is similar to how a person might use different words each time they describe the same image. Processing time may be longer during first use or when analyzing complex scenes, as the LLM enhancement requires additional computational resources.\n",
        "                                        </p>\n",
        "                                    </div>\n",
        "                                    ''')\n",
        "                                image_scene_description_html = gr.HTML(label=None, elem_id=\"scene_analysis_description_text\")\n",
        "\n",
        "                                # 使用LLM增強敘述時也會顯示原本敘述內容\n",
        "                                with gr.Accordion(\"Original Scene Analysis\", open=False, elem_id=\"original_scene_analysis_accordion\"):\n",
        "                                    image_llm_description = gr.HTML(label=None, elem_id=\"original_scene_description_text\")\n",
        "\n",
        "                                with gr.Row():\n",
        "                                     with gr.Column(scale=1):\n",
        "                                         gr.HTML('<div class=\"section-heading\" style=\"font-size:1rem; text-align:left;\">Possible Activities</div>')\n",
        "                                         image_activities_list = gr.Dataframe(headers=[\"Activity\"], datatype=[\"str\"], row_count=5, col_count=1, wrap=True)\n",
        "\n",
        "                                     with gr.Column(scale=1):\n",
        "                                         gr.HTML('<div class=\"section-heading\" style=\"font-size:1rem; text-align:left;\">Safety Concerns</div>')\n",
        "                                         image_safety_list = gr.Dataframe(headers=[\"Concern\"], datatype=[\"str\"], row_count=5, col_count=1, wrap=True)\n",
        "\n",
        "                                gr.HTML('<div class=\"section-heading\">Functional Zones</div>')\n",
        "                                image_zones_json = gr.JSON(label=None, elem_classes=\"json-box\")\n",
        "\n",
        "                                gr.HTML('<div class=\"section-heading\">Lighting Conditions</div>')\n",
        "                                image_lighting_info = gr.JSON(label=None, elem_classes=\"json-box\")\n",
        "\n",
        "                            with gr.Tab(\"Statistics\"):\n",
        "                                with gr.Row():\n",
        "                                    with gr.Column(scale=3, elem_classes=\"plot-column\"):\n",
        "                                        gr.HTML('<div class=\"section-heading\">Object Distribution</div>')\n",
        "                                        image_plot_output = gr.Plot(label=None, elem_classes=\"large-plot-container\")\n",
        "                                    with gr.Column(scale=2, elem_classes=\"stats-column\"):\n",
        "                                        gr.HTML('<div class=\"section-heading\">Detection Statistics</div>')\n",
        "                                        image_stats_json = gr.JSON(label=None, elem_classes=\"enhanced-json-display\")\n",
        "\n",
        "            # Tab 2: Video Processing\n",
        "            with gr.Tab(\"Video Processing\"):\n",
        "                with gr.Row(equal_height=False):\n",
        "                    # Left Column: Video Input & Controls\n",
        "                    with gr.Column(scale=4, elem_classes=\"input-panel\"):\n",
        "                        with gr.Group():\n",
        "                            gr.HTML('<div class=\"section-heading\">Video Input</div>')\n",
        "\n",
        "                            # Add input type selection\n",
        "                            video_input_type = gr.Radio(\n",
        "                                [\"upload\", \"url\"],\n",
        "                                label=\"Input Method\",\n",
        "                                value=\"upload\",\n",
        "                                info=\"Choose how to provide the video\"\n",
        "                            )\n",
        "\n",
        "                            # File upload (will be shown/hidden based on selection)\n",
        "                            with gr.Group(elem_id=\"upload-video-group\"):\n",
        "                                video_input = gr.Video(\n",
        "                                    label=\"Upload a video file (MP4, AVI, MOV)\",\n",
        "                                    sources=[\"upload\"],\n",
        "                                    visible=True\n",
        "                                )\n",
        "\n",
        "                            # URL input (will be shown/hidden based on selection)\n",
        "                            with gr.Group(elem_id=\"url-video-group\"):\n",
        "                                video_url_input = gr.Textbox(\n",
        "                                    label=\"Enter video URL (YouTube or direct video link)\",\n",
        "                                    placeholder=\"https://www.youtube.com/watch?v=...\",\n",
        "                                    visible=False,\n",
        "                                    elem_classes=\"custom-video-url-input\"\n",
        "                                )\n",
        "                                gr.HTML(\"\"\"\n",
        "                                    <div style=\"padding: 8px; margin-top: 5px; background-color: #fff8f8; border-radius: 4px; border-left: 3px solid #f87171; font-size: 12px;\">\n",
        "                                        <p style=\"margin: 0; color: #4b5563;\">\n",
        "                                            Note: Currently only YouTube URLs are supported. Maximum video duration is 10 minutes. Due to YouTube's anti-bot protection, some videos may not be downloadable. For protected videos, please upload a local video file instead.\n",
        "                                        </p>\n",
        "                                    </div>\n",
        "                                \"\"\")\n",
        "\n",
        "                            with gr.Accordion(\"Video Analysis Settings\", open=True):\n",
        "                                video_model_dropdown = gr.Dropdown(\n",
        "                                    choices=model_choices,\n",
        "                                    value=\"yolov8n.pt\", # Default 'n' for video\n",
        "                                    label=\"Select Model (Video)\",\n",
        "                                    info=\"Faster models (like 'n') are recommended\"\n",
        "                                )\n",
        "                                video_confidence = gr.Slider(\n",
        "                                    minimum=0.1, maximum=0.9, value=0.4, step=0.05,\n",
        "                                    label=\"Confidence Threshold (Video)\"\n",
        "                                )\n",
        "                                video_process_interval = gr.Slider(\n",
        "                                    minimum=1, maximum=60, value=10, step=1, # Allow up to 60 frame interval\n",
        "                                    label=\"Processing Interval (Frames)\",\n",
        "                                    info=\"Analyze every Nth frame (higher value = faster)\"\n",
        "                                )\n",
        "                        video_process_btn = gr.Button(\"Process Video\", variant=\"primary\", elem_classes=\"detect-btn\")\n",
        "\n",
        "                        with gr.Group(elem_classes=\"how-to-use\"):\n",
        "                            gr.HTML('<div class=\"section-heading\">How to Use (Video)</div>')\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            1. Choose your input method: Upload a file or enter a URL.\n",
        "                            2. Adjust settings if needed (using a faster model and larger interval is recommended for longer videos).\n",
        "                            3. Click \"Process Video\". **Processing can take a significant amount of time.**\n",
        "                            4. The annotated video and summary will appear on the right when finished.\n",
        "                            \"\"\")\n",
        "\n",
        "                        # Add video examples\n",
        "                        gr.HTML('<div class=\"section-heading\">Example Videos</div>')\n",
        "                        gr.HTML(\"\"\"\n",
        "                            <div style=\"padding: 10px; background-color: #f0f7ff; border-radius: 6px; margin-bottom: 15px;\">\n",
        "                                <p style=\"font-size: 14px; color: #4A5568; margin: 0;\">\n",
        "                                    Upload any video containing objects that YOLO can detect. For testing, find sample videos\n",
        "                                    <a href=\"https://www.pexels.com/search/videos/street/\" target=\"_blank\" style=\"color: #3182ce; text-decoration: underline;\">here</a>.\n",
        "                                </p>\n",
        "                            </div>\n",
        "                        \"\"\")\n",
        "\n",
        "                    # Right Column: Video Results\n",
        "                    with gr.Column(scale=6, elem_classes=\"output-panel video-result-panel\"):\n",
        "                        gr.HTML(\"\"\"\n",
        "                            <div class=\"section-heading\">Video Result</div>\n",
        "                            <details class=\"info-details\" style=\"margin: 5px 0 15px 0;\">\n",
        "                                <summary style=\"padding: 8px; background-color: #f0f7ff; border-radius: 6px; border-left: 3px solid #4299e1; font-weight: bold; cursor: pointer; color: #2b6cb0;\">\n",
        "                                    🎬 Video Processing Notes\n",
        "                                </summary>\n",
        "                                <div style=\"margin-top: 8px; padding: 10px; background-color: #f8f9fa; border-radius: 6px; border: 1px solid #e2e8f0;\">\n",
        "                                    <p style=\"font-size: 13px; color: #718096; margin: 0;\">\n",
        "                                        The processed video includes bounding boxes around detected objects. For longer videos,\n",
        "                                        consider using a faster model (like YOLOv8n) and a higher frame interval to reduce processing time.\n",
        "                                    </p>\n",
        "                                </div>\n",
        "                            </details>\n",
        "                        \"\"\")\n",
        "                        video_output = gr.Video(label=\"Processed Video\", elem_classes=\"video-output-container\") # Output for the processed video file\n",
        "\n",
        "                        gr.HTML('<div class=\"section-heading\">Processing Summary</div>')\n",
        "                        # 使用HTML顯示影片的摘要\n",
        "                        video_summary_text = gr.HTML(\n",
        "                            label=None,\n",
        "                            elem_id=\"video-summary-html-output\"\n",
        "                        )\n",
        "\n",
        "                        gr.HTML('<div class=\"section-heading\">Aggregated Statistics</div>')\n",
        "                        video_stats_json = gr.JSON(label=None, elem_classes=\"video-stats-display\") # Display statistics\n",
        "\n",
        "        # Event Listeners\n",
        "        # Image Model Change Handler\n",
        "        image_model_dropdown.change(\n",
        "            fn=lambda model: (model, DetectionModel.get_model_description(model)),\n",
        "            inputs=[image_model_dropdown],\n",
        "            outputs=[current_image_model, image_model_info] # Update state and description\n",
        "        )\n",
        "\n",
        "        # Image Filter Buttons\n",
        "        available_classes_list = get_all_classes() # Get list of (id, name)\n",
        "        people_classes_ids = [0]\n",
        "        vehicles_classes_ids = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "        animals_classes_ids = list(range(14, 24))\n",
        "        common_objects_ids = [39, 41, 42, 43, 44, 45, 56, 57, 60, 62, 63, 67, 73] # Bottle, cup, fork, knife, spoon, bowl, chair, couch, table, tv, laptop, phone, book\n",
        "\n",
        "        people_btn.click(lambda: [f\"{id}: {name}\" for id, name in available_classes_list if id in people_classes_ids], outputs=image_class_filter)\n",
        "        vehicles_btn.click(lambda: [f\"{id}: {name}\" for id, name in available_classes_list if id in vehicles_classes_ids], outputs=image_class_filter)\n",
        "        animals_btn.click(lambda: [f\"{id}: {name}\" for id, name in available_classes_list if id in animals_classes_ids], outputs=image_class_filter)\n",
        "        objects_btn.click(lambda: [f\"{id}: {name}\" for id, name in available_classes_list if id in common_objects_ids], outputs=image_class_filter)\n",
        "\n",
        "        video_input_type.change(\n",
        "            fn=lambda input_type: [\n",
        "                # Show/hide file upload\n",
        "                gr.update(visible=(input_type == \"upload\")),\n",
        "                # Show/hide URL input\n",
        "                gr.update(visible=(input_type == \"url\"))\n",
        "            ],\n",
        "            inputs=[video_input_type],\n",
        "            outputs=[video_input, video_url_input]\n",
        "        )\n",
        "\n",
        "        image_detect_btn.click(\n",
        "            fn=handle_image_upload,\n",
        "            inputs=[image_input, image_model_dropdown, image_confidence, image_class_filter, use_llm],\n",
        "            outputs=[\n",
        "                image_result_image, image_result_text, image_stats_json, image_plot_output,\n",
        "                image_scene_description_html, image_llm_description, image_activities_list, image_safety_list, image_zones_json,\n",
        "                image_lighting_info\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        video_process_btn.click(\n",
        "            fn=handle_video_upload,\n",
        "            inputs=[\n",
        "                video_input,\n",
        "                video_url_input,\n",
        "                video_input_type,\n",
        "                video_model_dropdown,\n",
        "                video_confidence,\n",
        "                video_process_interval\n",
        "            ],\n",
        "            outputs=[video_output, video_summary_text, video_stats_json]\n",
        "        )\n",
        "\n",
        "        # Footer\n",
        "        gr.HTML(\"\"\"\n",
        "             <div class=\"footer\" style=\"padding: 25px 0; text-align: center; background: linear-gradient(to right, #f5f9fc, #e1f5fe); border-top: 1px solid #e2e8f0; margin-top: 30px;\">\n",
        "                 <div style=\"margin-bottom: 15px;\">\n",
        "                     <p style=\"font-size: 14px; color: #4A5568; margin: 5px 0;\">Powered by YOLOv8, CLIP, Meta Llama3.2 and Ultralytics • Created with Gradio</p>\n",
        "                 </div>\n",
        "                 <div style=\"display: flex; align-items: center; justify-content: center; gap: 20px; margin-top: 15px;\">\n",
        "                     <p style=\"font-family: 'Arial', sans-serif; font-size: 14px; font-weight: 500; letter-spacing: 2px; background: linear-gradient(90deg, #38b2ac, #4299e1); -webkit-background-clip: text; -webkit-text-fill-color: transparent; margin: 0; text-transform: uppercase; display: inline-block;\">EXPLORE THE CODE →</p>\n",
        "                     <a href=\"https://github.com/Eric-Chung-0511/Learning-Record/tree/main/Data%20Science%20Projects/VisionScout\" target=\"_blank\" style=\"text-decoration: none;\">\n",
        "                         <img src=\"https://img.shields.io/badge/GitHub-VisionScout-4299e1?logo=github&style=for-the-badge\">\n",
        "                     </a>\n",
        "                 </div>\n",
        "             </div>\n",
        "         \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_interface = create_interface()\n",
        "\n",
        "    demo_interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1bb603d06c2646d692b8198350355182",
            "1d9fe5b3e7c24024b3380d8d6ef816e4",
            "ce7dd6f353bf4913be85a3d434a618cb",
            "66635c7f7c0142bcb193728be092eb45",
            "815a4e2c9b404747912dccb3d36b43c1",
            "f4c0fad1a4d64a20befd4f4e5d949f7a",
            "dd3352a6a26246b1a3a6041c3649a063",
            "2aaa72ec2745489d8e94b6cb350f4afc",
            "9fd0e09fc9754b7e86744d1d5ae8218a",
            "3629da459e1b4c3fbc7f2e6963663237",
            "1dafd28d8dff41609b250e5bbe46e193"
          ]
        },
        "id": "XoMFwi652rCU",
        "outputId": "3ab0fdd5-f4b4-47b4-ffee-f60bb510fa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://117a130c808b33a7e3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://117a130c808b33a7e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image with model: yolov8m.pt, confidence: 0.25, use_llm: True\n",
            "Creating new model instance for yolov8m.pt\n",
            "Loading model: yolov8m.pt\n",
            "Successfully loaded model: yolov8m.pt\n",
            "Number of classes the model can recognize: 80\n",
            "\n",
            "image 1/1 /tmp/temp_768a6094d7514bc3a98abdfa6db793f7.jpg: 640x448 16 persons, 2 cars, 1 airplane, 2 skateboards, 134.4ms\n",
            "Speed: 26.7ms preprocess, 134.4ms inference, 467.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Loading CLIP model ViT-B/32 on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-14 05:29:29,617 - LLMEnhancer - INFO - Using device: cuda\n",
            "INFO:LLMEnhancer:Using device: cuda\n",
            "2025-05-14 05:29:29,621 - LLMEnhancer - WARNING - HF_TOKEN not found in environment variables. Access to gated models may be limited.\n",
            "WARNING:LLMEnhancer:HF_TOKEN not found in environment variables. Access to gated models may be limited.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP model loaded successfully.\n",
            "LLM enhancer initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-14 05:29:30,098 - LLMEnhancer - INFO - Model not loaded, no context to reset\n",
            "INFO:LLMEnhancer:Model not loaded, no context to reset\n",
            "2025-05-14 05:29:30,100 - LLMEnhancer - INFO - Loading LLM model from meta-llama/Llama-3.2-3B-Instruct with 8-bit quantization\n",
            "INFO:LLMEnhancer:Loading LLM model from meta-llama/Llama-3.2-3B-Instruct with 8-bit quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPU memory: 14.74 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb603d06c2646d692b8198350355182"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-14 05:30:10,074 - LLMEnhancer - INFO - Model loaded successfully with 8-bit quantization\n",
            "INFO:LLMEnhancer:Model loaded successfully with 8-bit quantization\n",
            "2025-05-14 05:30:10,075 - LLMEnhancer - INFO - Generating LLM response...\n",
            "INFO:LLMEnhancer:Generating LLM response...\n",
            "2025-05-14 05:30:10,077 - LLMEnhancer - INFO - LLM call #1\n",
            "INFO:LLMEnhancer:LLM call #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing scene_analysis: dict_keys(['scene_type', 'scene_name', 'confidence', 'description', 'enhanced_description', 'objects_present', 'object_count', 'regions', 'possible_activities', 'safety_concerns', 'functional_zones', 'alternative_scenes', 'lighting_conditions', 'clip_analysis'])\n",
            "Original scene description (first 50 chars): From an aerial perspective,. An aerial view showin...\n",
            "Cleaned scene description (first 50 chars): From an aerial perspective,. An aerial view showin...\n",
            "Processing image with model: yolov8m.pt, confidence: 0.25, use_llm: True\n",
            "Updated existing scene_analyzer use_llm setting to: True\n",
            "Using existing model instance for yolov8m.pt\n",
            "\n",
            "image 1/1 /tmp/temp_62ce2cb1c79a4beea9446907ef46f346.jpg: 640x448 1 person, 6 cars, 5 traffic lights, 1 handbag, 27.5ms\n",
            "Speed: 3.6ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-14 05:34:04,071 - LLMEnhancer - INFO - Model context reset\n",
            "INFO:LLMEnhancer:Model context reset\n",
            "2025-05-14 05:34:04,074 - LLMEnhancer - INFO - Generating LLM response...\n",
            "INFO:LLMEnhancer:Generating LLM response...\n",
            "2025-05-14 05:34:04,075 - LLMEnhancer - INFO - LLM call #2\n",
            "INFO:LLMEnhancer:LLM call #2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing scene_analysis: dict_keys(['scene_type', 'scene_name', 'confidence', 'description', 'enhanced_description', 'objects_present', 'object_count', 'regions', 'possible_activities', 'safety_concerns', 'functional_zones', 'alternative_scenes', 'lighting_conditions', 'clip_analysis'])\n",
            "Original scene description (first 50 chars): A busy urban crossroad with pedestrian crossings a...\n",
            "Cleaned scene description (first 50 chars): A busy urban crossroad with pedestrian crossings a...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://117a130c808b33a7e3.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikiFwTh3WG75"
      },
      "outputs": [],
      "source": [
        "# %%writefile requirements.txt\n",
        "# torch>=2.0.0\n",
        "# torchvision>=0.15.0\n",
        "# ultralytics>=8.0.0\n",
        "# opencv-python>=4.7.0\n",
        "# pillow>=9.4.0\n",
        "# numpy>=1.23.5\n",
        "# matplotlib>=3.7.0\n",
        "# gradio>=3.32.0\n",
        "# git+https://github.com/openai/CLIP.git\n",
        "# yt-dlp>=2023.3.4\n",
        "# requests>=2.28.1\n",
        "# transformers\n",
        "# accelerate\n",
        "# bitsandbytes\n",
        "# sentencepiece\n",
        "# huggingface_hub>=0.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y8l2_CT9W2Vl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "15daa589-18cb-47b7-d128-4c3a1f939d41"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0c713272-caa1-4ce2-832d-bccb7c5587aa\", \"llm_enhancer.py\", 48070)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download('detection_model.py')\n",
        "# files.download('color_mapper.py')\n",
        "# files.download('visualization_helper.py')\n",
        "# files.download('evaluation_metrics.py')\n",
        "# files.download('style.py')\n",
        "# files.download('scene_type.py')\n",
        "# files.download('confifence_templates.py')\n",
        "# files.download('scene_detail_templates.py')\n",
        "# files.download('object_template_fillers.py')\n",
        "# files.download('safety_templates.py')\n",
        "# files.download('activity_templates.py')\n",
        "# files.download('object_categories.py')\n",
        "# files.download('lighting_conditions.py')\n",
        "# files.download('viewpoint_templates.py')\n",
        "# files.download('cultural_templates.py')\n",
        "# files.download('spatial_analyzer.py')\n",
        "# files.download('enhance_scene_describer.py')\n",
        "# files.download('lighting_analyzer.py')\n",
        "# files.download('scene_description.py')\n",
        "# files.download('clip_prompts.py')\n",
        "# files.download('clip_analyzer.py')\n",
        "# files.download('scene_analyzer.py')\n",
        "# files.download('image_processor.py')\n",
        "# files.download('video_processor.py')\n",
        "# files.download('llm_enhancer.py')\n",
        "# files.download('app.py')\n",
        "\n",
        "# files.download('requirements.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVswpsr-Wfr8"
      },
      "outputs": [],
      "source": [
        "#    # Set up example images\n",
        "# examples=[\n",
        "#             \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/room_01.jpg\",\n",
        "#             \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/room_02.jpg\",\n",
        "#             \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/street_02.jpg\",\n",
        "#             \"/content/drive/Othercomputers/我的 MacBook Pro/Learning/VisionScout/test_images/street_04.jpg\"\n",
        "#             ],"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bb603d06c2646d692b8198350355182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d9fe5b3e7c24024b3380d8d6ef816e4",
              "IPY_MODEL_ce7dd6f353bf4913be85a3d434a618cb",
              "IPY_MODEL_66635c7f7c0142bcb193728be092eb45"
            ],
            "layout": "IPY_MODEL_815a4e2c9b404747912dccb3d36b43c1"
          }
        },
        "1d9fe5b3e7c24024b3380d8d6ef816e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c0fad1a4d64a20befd4f4e5d949f7a",
            "placeholder": "​",
            "style": "IPY_MODEL_dd3352a6a26246b1a3a6041c3649a063",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ce7dd6f353bf4913be85a3d434a618cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aaa72ec2745489d8e94b6cb350f4afc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fd0e09fc9754b7e86744d1d5ae8218a",
            "value": 2
          }
        },
        "66635c7f7c0142bcb193728be092eb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3629da459e1b4c3fbc7f2e6963663237",
            "placeholder": "​",
            "style": "IPY_MODEL_1dafd28d8dff41609b250e5bbe46e193",
            "value": " 2/2 [00:30&lt;00:00, 13.85s/it]"
          }
        },
        "815a4e2c9b404747912dccb3d36b43c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c0fad1a4d64a20befd4f4e5d949f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd3352a6a26246b1a3a6041c3649a063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2aaa72ec2745489d8e94b6cb350f4afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd0e09fc9754b7e86744d1d5ae8218a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3629da459e1b4c3fbc7f2e6963663237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dafd28d8dff41609b250e5bbe46e193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}