{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1R0gNgrP42L7nsXg5UdGh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eric-Chung-0511/Learning-Record/blob/main/Data%20Science%20Projects/PawMatchAI/%5BICARL%5D%5B88_70%5DPawMatchAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import copy\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "KyFJePstiAui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_ICnAOwuSc-"
      },
      "outputs": [],
      "source": [
        "class ICARLIncrementalLearner:\n",
        "\n",
        "    def __init__(self, model_path, json_path, evaluator, device='cuda', memory_size=2000):\n",
        "        \"\"\"\n",
        "        # Initialization Function Explanation:\n",
        "        This is the initialization function of the ICARL incremental learner, setting up all necessary components and parameters\n",
        "\n",
        "        Parameter Description:\n",
        "        - model_path: Path to the pre-trained model file\n",
        "        - json_path: Path to JSON file containing breed information\n",
        "        - evaluator: Performance evaluator instance\n",
        "        - device: Computing device, defaults to CUDA\n",
        "        - memory_size: Feature memory size, defaults to 2000\n",
        "        \"\"\"\n",
        "        # Basic setup\n",
        "        self.device = device\n",
        "        self.json_path = json_path\n",
        "        self.memory_size = memory_size\n",
        "        self.evaluator = evaluator\n",
        "\n",
        "        # Temperature parameter setup for knowledge distillation\n",
        "        self.initial_temperature = 1  # Initial temperature for training start\n",
        "        self.max_temperature = 2.5    # Maximum temperature for late training\n",
        "        self.min_temperature = 1      # Minimum temperature for early training\n",
        "\n",
        "        # Load breed information\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            self.breeds_data = json.load(f)  # Load complete breed data\n",
        "            self.breeds = self.breeds_data['breeds']  # Extract breed list\n",
        "\n",
        "        # Load pre-trained model\n",
        "        self.model_path = model_path\n",
        "        self._load_model()  # Call model loading function\n",
        "\n",
        "        # Initialize memory-related components\n",
        "        self.feature_memory = {}      # Store features for each breed\n",
        "        self.prototype_memory = {}    # Store prototype features for each breed\n",
        "\n",
        "        # Training-related parameters\n",
        "        self.best_loss = float('inf')  # Record best loss value\n",
        "        self.best_model_state = None   # Store best model state\n",
        "\n",
        "        # Protection mechanism attributes\n",
        "        self.feature_importance = {}  # Store feature importance weights\n",
        "        self.similarity_matrix = {}   # Store breed similarity matrix\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Model Loading Function Explanation:\n",
        "        Responsible for loading the pre-trained model and establishing the teacher model\n",
        "        Includes error handling and support for multiple checkpoint formats\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Step 1: Create base model instance\n",
        "            self.base_model = BaseModel(\n",
        "                num_classes=len(self.breeds),  # Set output class number\n",
        "                device=self.device  # Specify computing device\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Step 2: Load model weights\n",
        "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
        "\n",
        "            # Step 3: Handle different checkpoint formats\n",
        "            if 'base_model' in checkpoint:\n",
        "                self.base_model.load_state_dict(checkpoint['base_model'])\n",
        "            elif 'model_state_dict' in checkpoint:\n",
        "                self.base_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            elif isinstance(checkpoint, collections.OrderedDict):\n",
        "                self.base_model.load_state_dict(checkpoint)\n",
        "            else:\n",
        "                raise ValueError(\"Unexpected checkpoint format\")\n",
        "\n",
        "            # Step 4: Create teacher model (for knowledge distillation)\n",
        "            self.teacher_model = copy.deepcopy(self.base_model).to(self.device)\n",
        "            self.teacher_model.eval()\n",
        "\n",
        "            print(f\"Successfully loaded model from {self.model_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
        "\n",
        "    def _match_breed_name(self, query_breed):\n",
        "        \"\"\"\n",
        "        Function: Match breed names with fuzzy matching support\n",
        "\n",
        "        Args:\n",
        "            query_breed (str): The breed name to query\n",
        "\n",
        "        Returns:\n",
        "            str: Complete breed name, returns None if not found\n",
        "        \"\"\"\n",
        "        # Convert query string to lowercase and clean\n",
        "        query = query_breed.lower().strip()\n",
        "\n",
        "        # Exact match\n",
        "        for breed in self.breeds:\n",
        "            if breed.lower() == query:\n",
        "                return breed\n",
        "\n",
        "        # Partial match\n",
        "        for breed in self.breeds:\n",
        "            if query in breed.lower():\n",
        "                return breed\n",
        "\n",
        "        # Use more lenient matching rules\n",
        "        for breed in self.breeds:\n",
        "            # Compare after removing parenthetical content\n",
        "            clean_breed = re.sub(r'\\([^)]*\\)', '', breed.lower()).strip()\n",
        "            if query in clean_breed:\n",
        "                return breed\n",
        "\n",
        "            # Handle hyphen and space cases\n",
        "            normalized_breed = clean_breed.replace('-', ' ').replace('_', ' ')\n",
        "            normalized_query = query.replace('-', ' ').replace('_', ' ')\n",
        "\n",
        "            if normalized_query in normalized_breed:\n",
        "                return breed\n",
        "\n",
        "        # Return None if no match is found\n",
        "        return None\n",
        "\n",
        "    def find_best_lr(self, train_loader, init_value=1e-8, final_value=1e-5, num_iter=100):\n",
        "        \"\"\"\n",
        "        Function to find the optimal learning rate:\n",
        "        Uses a learning rate range test to determine the best learning rate.\n",
        "        \"\"\"\n",
        "        # Step 1: Save the current model state\n",
        "        model_state = copy.deepcopy(self.base_model.state_dict())\n",
        "\n",
        "        # Step 2: Set up the optimizer and learning rate range\n",
        "        optimizer = optim.AdamW([{\n",
        "            'params': self.base_model.parameters(),\n",
        "            'initial_lr': init_value\n",
        "        }], lr=init_value)\n",
        "\n",
        "        # Step 3: Compute the learning rate growth factor\n",
        "        gamma = (final_value / init_value) ** (1 / num_iter)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "        # Step 4: Initialize recording lists\n",
        "        losses = []\n",
        "        learning_rates = []\n",
        "        best_loss = float('inf')\n",
        "        best_lr = init_value\n",
        "\n",
        "        # Step 5: Conduct the learning rate test\n",
        "        self.base_model.train()\n",
        "\n",
        "        for iteration in range(num_iter):\n",
        "            try:  # Fetch a batch of data\n",
        "                inputs, labels = next(iter(train_loader))\n",
        "            except StopIteration:\n",
        "                train_iter = iter(train_loader)\n",
        "                inputs, labels = next(train_iter)\n",
        "\n",
        "            # Move data to the designated device\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Forward pass and loss computation\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = self.base_model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            # Update the best learning rate\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                best_lr = current_lr\n",
        "\n",
        "            # Record results\n",
        "            losses.append(loss.item())\n",
        "            learning_rates.append(current_lr)\n",
        "\n",
        "            # Backpropagation and update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Use the evaluator method to plot the graph\n",
        "        self.evaluator.plot_lr_finder(learning_rates, losses)\n",
        "\n",
        "        self.base_model.load_state_dict(model_state)\n",
        "        return best_lr\n",
        "\n",
        "    def update_feature_memory(self, images, breed_name):\n",
        "        \"\"\"\n",
        "        Function: Update Feature Memory\n",
        "        Purpose: Store image features for specific breeds in the memory bank\n",
        "\n",
        "        # Key Steps:\n",
        "        1. Set model to evaluation mode\n",
        "        2. Batch process images to get features\n",
        "        3. Store features in memory bank\n",
        "\n",
        "        # Key Parameters:\n",
        "        images: Input image collection\n",
        "        breed_name: Breed name\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode to avoid batch normalization effects\n",
        "        self.base_model.eval()\n",
        "        # Use torch.no_grad() to avoid gradient calculation, improving efficiency and reducing memory usage\n",
        "        with torch.no_grad():\n",
        "            features = []\n",
        "            for img in images:\n",
        "                # unsqueeze(0) converts single image to batch format\n",
        "                # Extract features, _ ignores model classification output, keeping only features\n",
        "                _, feature = self.base_model(img.unsqueeze(0).to(self.device))\n",
        "                features.append(feature)\n",
        "            # Concatenate all features along batch dimension to form feature collection for this breed\n",
        "            self.feature_memory[breed_name] = torch.cat(features, dim=0)\n",
        "\n",
        "    def update_prototypes(self):\n",
        "        \"\"\"\n",
        "        Function: Update Prototype Memory\n",
        "        Purpose: Calculate mean features as prototypes for each breed\n",
        "\n",
        "        # Key Steps:\n",
        "        1. Iterate through all breeds in feature memory\n",
        "        2. Calculate mean features for each breed\n",
        "        3. Normalize the features\n",
        "        \"\"\"\n",
        "        for breed, features in self.feature_memory.items():\n",
        "            # Calculate mean features as breed prototype representation\n",
        "            mean_feature = features.mean(dim=0)\n",
        "            # Apply L2 normalization to ensure feature comparability across breeds\n",
        "            self.prototype_memory[breed] = F.normalize(mean_feature, p=2, dim=0)\n",
        "\n",
        "    def _calculate_feature_importance(self):\n",
        "        \"\"\"\n",
        "        Function: Feature Importance Calculation\n",
        "\n",
        "        Implementation Principle:\n",
        "        Uses absolute value means of features to measure their importance\n",
        "        Helps identify most influential feature dimensions for classification\n",
        "        \"\"\"\n",
        "        self.base_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for breed, features in self.feature_memory.items():\n",
        "                # Calculate mean absolute value for each feature dimension as importance indicator\n",
        "                importance = torch.abs(features).mean(dim=0)\n",
        "                self.feature_importance[breed] = importance\n",
        "\n",
        "    def _update_similarity_matrix(self):\n",
        "        \"\"\"\n",
        "        Function Purpose:\n",
        "        Updates the similarity matrix between breeds.\n",
        "\n",
        "        Key Steps:\n",
        "        1. Iterate through all breed pairs.\n",
        "        2. Compute cosine similarity between them.\n",
        "        3. Store the results in the similarity matrix.\n",
        "\n",
        "        Application Scenario:\n",
        "        Used for feature protection mechanisms to ensure that features of similar breeds do not interfere with each other.\n",
        "        \"\"\"\n",
        "        # Iterate through all possible breed pairs\n",
        "        for breed1 in self.breeds:\n",
        "            if breed1 not in self.similarity_matrix:  # Initialize the similarity dictionary for the breed\n",
        "                self.similarity_matrix[breed1] = {}\n",
        "\n",
        "            for breed2 in self.breeds:\n",
        "                if breed1 == breed2:  # Skip self-similarity calculation\n",
        "                    continue\n",
        "\n",
        "                if breed2 not in self.similarity_matrix[breed1]:  # Avoid redundant similarity calculations\n",
        "                    feat1 = self.prototype_memory.get(breed1)\n",
        "                    feat2 = self.prototype_memory.get(breed2)\n",
        "\n",
        "                    if feat1 is not None and feat2 is not None:  # Compute similarity only if both breeds have prototype features\n",
        "                        similarity = F.cosine_similarity(feat1, feat2, dim=0)  # Use cosine similarity to measure feature vector similarity\n",
        "                        self.similarity_matrix[breed1][breed2] = similarity.item()\n",
        "\n",
        "    def _get_most_similar_breeds(self, target_breed, top_k=5):\n",
        "        \"\"\"\n",
        "        Function: Retrieve the most similar breeds to a target breed.\n",
        "\n",
        "        Parameter Explanation:\n",
        "            - target_breed: The name of the target breed.\n",
        "            - top_k: The number of most similar breeds to return, default is 5.\n",
        "\n",
        "        Returns: A list of breeds sorted by similarity.\n",
        "        \"\"\"\n",
        "        if target_breed in self.similarity_matrix:  # Check if the target breed exists in the similarity matrix\n",
        "            similarities = self.similarity_matrix[target_breed]  # Retrieve similarity scores for the breed\n",
        "            return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]  # Sort by similarity in descending order and return the top k\n",
        "        return []  # Return an empty list if the target breed is not found\n",
        "\n",
        "\n",
        "    def _calculate_protection_loss(self, student_features, teacher_features, breed_name):\n",
        "        \"\"\"\n",
        "        Function: Calculate Feature Protection Loss\n",
        "\n",
        "        Key Components:\n",
        "        1. Importance-based feature protection\n",
        "        2. Similar breed feature protection consideration\n",
        "        3. Weighted loss calculation\n",
        "\n",
        "        Protection Mechanism:\n",
        "        Ensures important features are protected during training through importance masks and similarity weights\n",
        "        \"\"\"\n",
        "        protection_loss = 0  # Calculate feature protection loss for target breed\n",
        "\n",
        "        # Get important feature mask\n",
        "        if breed_name in self.feature_importance:\n",
        "            importance_mask = self.feature_importance[breed_name]\n",
        "\n",
        "            # Calculate MSE loss for important features\n",
        "            weighted_mse = F.mse_loss(\n",
        "                student_features * importance_mask,\n",
        "                teacher_features * importance_mask,\n",
        "                reduction='none'\n",
        "            ).mean()\n",
        "\n",
        "            protection_loss += weighted_mse\n",
        "\n",
        "        # Consider feature protection for similar breeds\n",
        "        similar_breeds = self._get_most_similar_breeds(breed_name)\n",
        "        for similar_breed, similarity in similar_breeds:\n",
        "            if similar_breed in self.feature_importance:\n",
        "                sim_importance = self.feature_importance[similar_breed]  # Get feature importance of similar breed\n",
        "                sim_weight = similarity * 0.5  # Weight coefficient based on similarity\n",
        "\n",
        "                # Calculate feature protection loss for similar breed\n",
        "                sim_loss = F.mse_loss(\n",
        "                    student_features * sim_importance,\n",
        "                    teacher_features * sim_importance,\n",
        "                    reduction='none'\n",
        "                ).mean() * sim_weight\n",
        "\n",
        "                protection_loss += sim_loss\n",
        "\n",
        "        return protection_loss\n",
        "\n",
        "    def _prepare_data_loader(self, images, batch_size, breed_name):\n",
        "        \"\"\"\n",
        "        Function: Prepare Data Loader\n",
        "        Main Tasks:\n",
        "            1. Organize image data into batches\n",
        "            2. Generate corresponding labels\n",
        "            3. Configure data loading parameters\n",
        "\n",
        "        Parameter Description:\n",
        "            - images: Input image collection\n",
        "            - batch_size: Size of each batch\n",
        "            - breed_name: Breed name (for label generation)\n",
        "        \"\"\"\n",
        "        breed_idx = self.breeds.index(breed_name)  # Get numeric label for breed\n",
        "        labels = torch.full((len(images),), breed_idx, dtype=torch.long)  # Create label tensor filled with breed index\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(images, labels)  # Create dataset object pairing images with labels\n",
        "\n",
        "        # Return configured data loader with shuffle enabled\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    def enhance_breed(self, images, breed_name, epochs=100, lr=None, batch_size=8):\n",
        "        \"\"\"\n",
        "        Function: Main entry function for breed enhancement.\n",
        "\n",
        "        Execution Process:\n",
        "            1. Breed name validation.\n",
        "            2. Data splitting (training and evaluation sets).\n",
        "            3. Pre-enhancement evaluation.\n",
        "            4. Perform enhancement learning.\n",
        "            5. Post-enhancement evaluation.\n",
        "            6. Update feature memory.\n",
        "        \"\"\"\n",
        "        full_breed_name = self._match_breed_name(breed_name)  # Breed name matching and validation\n",
        "        if not full_breed_name:\n",
        "            raise ValueError(f\"Breed {breed_name} not found in database\")\n",
        "\n",
        "        # Perform evaluation before enhancement\n",
        "        eval_images = images[:len(images)//5]  # Use 20% of images as the evaluation set\n",
        "        train_images = images[len(images)//5:]  # Use the remaining 80% for training\n",
        "\n",
        "        # Pre-enhancement evaluation - using the matched breed name\n",
        "        pre_enhancement_results = self.evaluator.evaluate_breed_performance(\n",
        "            self.base_model,\n",
        "            eval_images,\n",
        "            full_breed_name,  # Use the fully matched name\n",
        "            self.breeds,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        # Automatically determine the learning rate\n",
        "        if lr is None:\n",
        "            train_loader = self._prepare_data_loader(train_images, batch_size, full_breed_name)\n",
        "            lr = self.find_best_lr(train_loader)\n",
        "            print(f\"Found optimal learning rate: {lr:.2e}\")\n",
        "\n",
        "        try:\n",
        "            # Perform enhancement learning\n",
        "            best_model = self._perform_enhancement(\n",
        "                train_images, full_breed_name, epochs, lr, batch_size)\n",
        "\n",
        "            if best_model is not None:\n",
        "                self.base_model.load_state_dict(best_model)\n",
        "\n",
        "                # Post-enhancement evaluation - using the matched breed name\n",
        "                post_enhancement_results = self.evaluator.evaluate_breed_performance(\n",
        "                    self.base_model,\n",
        "                    eval_images,\n",
        "                    full_breed_name,  # Use the fully matched name\n",
        "                    self.breeds,\n",
        "                    self.device\n",
        "                )\n",
        "\n",
        "                # Generate an enhancement performance report\n",
        "                self.evaluator.generate_enhancement_report(\n",
        "                    full_breed_name,  # Use the fully matched name\n",
        "                    pre_enhancement_results,\n",
        "                    post_enhancement_results\n",
        "                )\n",
        "\n",
        "                # Update feature memory and prototypes\n",
        "                self.update_feature_memory(train_images, breed_name)\n",
        "                self.update_prototypes()\n",
        "\n",
        "                self.evaluator.save_enhanced_model(self.base_model, breed_name)\n",
        "                return True\n",
        "            else:\n",
        "                print(\"Enhancement failed, rolling back...\")\n",
        "                self.base_model.load_state_dict(original_state)\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Enhancement failed: {str(e)}\")\n",
        "            self.base_model.load_state_dict(original_state)\n",
        "            return False\n",
        "\n",
        "        def validate_enhancement(model, images, breed_name):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                for img in images:\n",
        "                    img = img.unsqueeze(0).to(self.device)\n",
        "                    logits, _ = model(img)\n",
        "                    pred = torch.argmax(logits, dim=1)\n",
        "                    if self.breeds[pred.item()] == breed_name:\n",
        "                        correct += 1\n",
        "                return correct / len(images)\n",
        "\n",
        "\n",
        "    def _calculate_temperature(self, current_epoch, total_epochs):\n",
        "        \"\"\"\n",
        "        Function: Implement smooth temperature adjustment using cosine function\n",
        "\n",
        "        Args:\n",
        "            current_epoch (int): Current training epoch\n",
        "            total_epochs (int): Total number of training epochs\n",
        "\n",
        "        Returns:\n",
        "            float: Calculated current temperature value\n",
        "        \"\"\"\n",
        "        progress = current_epoch / total_epochs\n",
        "\n",
        "        if progress <= 0.3:  # Early learning phase: maintain low temperature\n",
        "            return self.min_temperature\n",
        "        elif progress >= 0.7:  # Late learning phase: maintain high temperature\n",
        "            else:\n",
        "                # Mid-phase: smooth transition using cosine function\n",
        "                # Map progress to [0, Ï€] interval\n",
        "                normalized_progress = (progress - 0.3) / 0.4 * math.pi\n",
        "                cos_value = math.cos(normalized_progress)\n",
        "                # Map cosine value (-1 to 1) to temperature range, calculate smooth transition temperature\n",
        "                return self.min_temperature + (self.max_temperature - self.min_temperature) * (1 - (cos_value + 1) / 2)\n",
        "\n",
        "\n",
        "    def _perform_enhancement(self, images, breed_name, epochs, lr, batch_size):\n",
        "        \"\"\"\n",
        "        Function: Execute the core enhancement learning process.\n",
        "\n",
        "        Main Objective:\n",
        "        Improve the recognition capability of a specific breed through knowledge distillation and feature protection.\n",
        "\n",
        "        Processing Flow:\n",
        "        Training, evaluation, early stopping mechanism, dynamic temperature adjustment.\n",
        "        \"\"\"\n",
        "        self.base_model.train()\n",
        "        train_loader = self._prepare_data_loader(images, batch_size, breed_name)\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        optimizer = optim.AdamW(self.base_model.parameters(), lr=lr)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=epochs,\n",
        "            eta_min=1e-7\n",
        "        )\n",
        "\n",
        "        # Compute initial feature importance and similarity matrix\n",
        "        self._calculate_feature_importance()\n",
        "        self._update_similarity_matrix()\n",
        "\n",
        "        # Early Stopping settings\n",
        "        best_loss = float('inf')\n",
        "        best_model_state = None\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training records\n",
        "        train_losses = []\n",
        "        learning_rates = []\n",
        "        temperatures = []  # New: Track temperature changes\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "            # Compute the temperature value for the current epoch\n",
        "            current_temperature = self._calculate_temperature(epoch, epochs)\n",
        "            temperatures.append(current_temperature)  # Track temperature changes\n",
        "\n",
        "            for batch_images, labels in train_loader:\n",
        "                batch_images = batch_images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Teacher model prediction using the current temperature (fixed parameter)\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits, teacher_features = self.teacher_model(batch_images)\n",
        "                    teacher_probs = F.softmax(teacher_logits / current_temperature, dim=1)  # Apply temperature-scaled softmax for teacher model probability output\n",
        "\n",
        "                # Student model prediction (parameters to be optimized)\n",
        "                optimizer.zero_grad()  # Clear gradients\n",
        "                student_logits, student_features = self.base_model(batch_images)\n",
        "                student_probs = F.softmax(student_logits / current_temperature, dim=1)\n",
        "\n",
        "                # 1. Knowledge distillation loss using the current temperature\n",
        "                distill_loss = F.kl_div(\n",
        "                    F.log_softmax(student_logits / current_temperature, dim=1),\n",
        "                    teacher_probs,\n",
        "                    reduction='batchmean'\n",
        "                ) * (current_temperature ** 2)  # Apply temperature squared factor\n",
        "\n",
        "                # 2. Cross-entropy loss\n",
        "                ce_loss = F.cross_entropy(student_logits, labels)\n",
        "\n",
        "                # 3. Feature distillation loss\n",
        "                feature_loss = F.mse_loss(student_features, teacher_features)\n",
        "\n",
        "                # 4. Feature protection loss\n",
        "                protection_loss = self._calculate_protection_loss(\n",
        "                    student_features,\n",
        "                    teacher_features,\n",
        "                    breed_name\n",
        "                )\n",
        "\n",
        "                # Combine all losses, incorporating protection loss\n",
        "                loss = (\n",
        "                    (0.3 * distill_loss) +   # Reduce weight for distillation loss, so the model will no give too much attention to the distillation loss\n",
        "                    (0.4 * ce_loss) +        # Maintain weight for cross-entropy loss\n",
        "                    (0.2 * feature_loss) +   # Reduce weight for feature loss\n",
        "                    (0.1 * protection_loss)   # Add protection loss\n",
        "                )\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.base_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                # Accumulate loss\n",
        "                epoch_loss += loss.item()\n",
        "                batch_count += 1\n",
        "\n",
        "            # Compute average loss\n",
        "            avg_loss = epoch_loss / batch_count\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Record training information\n",
        "            train_losses.append(avg_loss)\n",
        "            learning_rates.append(current_lr)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.2e}, Temperature: {current_temperature:.4f}\")\n",
        "\n",
        "            # Early Stopping check\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                best_model_state = copy.deepcopy(self.base_model.state_dict())\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "                break\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "        # Use evaluator to plot training curves (including temperature changes)\n",
        "        self.evaluator.plot_training_curves(train_losses, learning_rates, temperatures)\n",
        "\n",
        "        return best_model_state if best_model_state is not None else None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceEvaluator:\n",
        "\n",
        "    def __init__(self, save_dir):\n",
        "        \"\"\"\n",
        "        Initialize the PerformanceEvaluator.\n",
        "\n",
        "        Args:\n",
        "            save_dir: Directory where evaluation results will be saved.\n",
        "        \"\"\"\n",
        "        self.save_dir = save_dir\n",
        "        self.visualization_dir = os.path.join(save_dir, 'visualizations')\n",
        "        os.makedirs(self.visualization_dir, exist_ok=True)\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    def evaluate(self, model, test_loader, breeds, device):\n",
        "        \"\"\"\n",
        "        Evaluate model performance.\n",
        "\n",
        "        Args:\n",
        "            model: The model to be evaluated.\n",
        "            test_loader: DataLoader containing test dataset.\n",
        "            breeds: List of breed names.\n",
        "            device: Computing device.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (All ground-truth labels, All predicted labels)\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        pred_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                logits, _ = model(images)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                pred_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        # Generate and save evaluation report\n",
        "        self._save_classification_report(all_labels, all_preds, breeds)\n",
        "\n",
        "        # Plot and save confusion matrix\n",
        "        self._plot_confusion_matrix(all_labels, all_preds, breeds)\n",
        "\n",
        "        # Compute and save confidence distribution for each breed\n",
        "        self._analyze_confidence_distribution(pred_probs, all_labels, breeds)\n",
        "\n",
        "        return all_labels, all_preds\n",
        "\n",
        "    def _save_classification_report(self, y_true, y_pred, breeds):\n",
        "        \"\"\"\n",
        "        Generate and save a classification report.\n",
        "\n",
        "        Args:\n",
        "            y_true: Ground-truth labels.\n",
        "            y_pred: Predicted labels.\n",
        "            breeds: List of breed names.\n",
        "        \"\"\"\n",
        "        report = classification_report(y_true, y_pred,\n",
        "                                       target_names=breeds,\n",
        "                                       output_dict=True)\n",
        "\n",
        "        # Save as CSV\n",
        "        report_df = pd.DataFrame(report).transpose()\n",
        "        report_path = os.path.join(self.save_dir,\n",
        "                                   f'classification_report_{self.timestamp}.csv')\n",
        "        report_df.to_csv(report_path)\n",
        "\n",
        "        # Print the report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=breeds))\n",
        "\n",
        "    def _plot_confusion_matrix(self, y_true, y_pred, breeds):\n",
        "        \"\"\"\n",
        "        Plot and save the confusion matrix.\n",
        "\n",
        "        Args:\n",
        "            y_true: Ground-truth labels.\n",
        "            y_pred: Predicted labels.\n",
        "            breeds: List of breed names.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(20, 20))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        # Compute percentage values\n",
        "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "        # Use seaborn to plot the heatmap\n",
        "        sns.heatmap(cm_percent, annot=True, fmt='.1f',\n",
        "                    xticklabels=breeds,\n",
        "                    yticklabels=breeds,\n",
        "                    cmap='Blues')\n",
        "\n",
        "        plt.title('Confusion Matrix (%)')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "\n",
        "        # Rotate labels for better readability\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=45)\n",
        "\n",
        "        # Adjust layout and save\n",
        "        plt.tight_layout()\n",
        "        cm_path = os.path.join(self.save_dir,\n",
        "                               f'confusion_matrix_{self.timestamp}.png')\n",
        "        plt.savefig(cm_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def _analyze_confidence_distribution(self, pred_probs, true_labels, breeds):\n",
        "        \"\"\"\n",
        "        Analyze and visualize the confidence distribution of predictions.\n",
        "\n",
        "        Args:\n",
        "            pred_probs: List of predicted probabilities.\n",
        "            true_labels: Ground-truth labels.\n",
        "            breeds: List of breed names.\n",
        "        \"\"\"\n",
        "        pred_probs = np.array(pred_probs)\n",
        "        true_labels = np.array(true_labels)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Plot confidence distribution for correct and incorrect predictions\n",
        "        confidences = np.max(pred_probs, axis=1)\n",
        "        predictions = np.argmax(pred_probs, axis=1)\n",
        "        correct_mask = predictions == true_labels\n",
        "\n",
        "        plt.hist(confidences[correct_mask], bins=50, alpha=0.5,\n",
        "                 label='Correct Predictions', color='green')\n",
        "        plt.hist(confidences[~correct_mask], bins=50, alpha=0.5,\n",
        "                 label='Wrong Predictions', color='red')\n",
        "\n",
        "        plt.title('Prediction Confidence Distribution')\n",
        "        plt.xlabel('Confidence')\n",
        "        plt.ylabel('Count')\n",
        "        plt.legend()\n",
        "\n",
        "        # Save the figure\n",
        "        conf_path = os.path.join(self.save_dir,\n",
        "                                 f'confidence_distribution_{self.timestamp}.png')\n",
        "        plt.savefig(conf_path)\n",
        "        plt.close()\n",
        "\n",
        "    def get_evaluation_summary(self):\n",
        "        \"\"\"\n",
        "        Return an evaluation summary.\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary of evaluation results.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'results_dir': self.save_dir,\n",
        "            'timestamp': self.timestamp,\n",
        "            'files': {\n",
        "                'classification_report': f'classification_report_{self.timestamp}.csv',\n",
        "                'confusion_matrix': f'confusion_matrix_{self.timestamp}.png',\n",
        "                'confidence_distribution': f'confidence_distribution_{self.timestamp}.png'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_enhanced_model(self, model, breed_name):\n",
        "        \"\"\"\n",
        "        Save the enhanced model.\n",
        "\n",
        "        Args:\n",
        "            model: The model to be saved.\n",
        "            breed_name: The name of the breed.\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        save_path = os.path.join(\n",
        "            self.save_dir,\n",
        "            f'enhanced_{breed_name}_{timestamp}.pth'\n",
        "        )\n",
        "\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'breed_info': {\n",
        "                'breed_name': breed_name,\n",
        "                'timestamp': timestamp\n",
        "            }\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Enhanced model saved to: {save_path}\")\n",
        "\n",
        "    def evaluate_breed_performance(self, model, eval_images, breed_name, breeds, device):\n",
        "        \"\"\"\n",
        "        Evaluate the performance of a specific breed.\n",
        "\n",
        "        Args:\n",
        "            model: The model to be evaluated.\n",
        "            eval_images: List of evaluation images.\n",
        "            breed_name: Name of the target breed.\n",
        "            breeds: List of breed names.\n",
        "            device: Computing device.\n",
        "\n",
        "        Returns:\n",
        "            dict: Performance metrics for the breed.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        confidences = []\n",
        "        correct_count = 0\n",
        "        breed_idx = breeds.index(breed_name)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for img in eval_images:\n",
        "                img = img.unsqueeze(0).to(device)\n",
        "                logits, _ = model(img)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                conf = torch.max(probs, dim=1)[0]\n",
        "\n",
        "                if pred.item() == breed_idx:\n",
        "                    correct_count += 1\n",
        "\n",
        "                predictions.append(pred.item())\n",
        "                confidences.append(conf.item())\n",
        "\n",
        "        return {\n",
        "            'predictions': predictions,\n",
        "            'confidences': confidences,\n",
        "            'breed_name': breed_name,\n",
        "            'mean_confidence': np.mean(confidences),\n",
        "            'correct_predictions': correct_count\n",
        "        }\n",
        "\n",
        "    def generate_enhancement_report(self, breed_name, pre_results, post_results):\n",
        "        \"\"\"\n",
        "        Generate a comparison report for the enhancement process.\n",
        "\n",
        "        Args:\n",
        "            breed_name: The breed that was enhanced.\n",
        "            pre_results: Performance results before enhancement.\n",
        "            post_results: Performance results after enhancement.\n",
        "\n",
        "        Saves:\n",
        "            JSON file with pre- and post-enhancement comparison.\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            'breed_name': breed_name,\n",
        "            'pre_enhancement': {\n",
        "                'mean_confidence': pre_results['mean_confidence'],\n",
        "                'correct_predictions': pre_results['correct_predictions']\n",
        "            },\n",
        "            'post_enhancement': {\n",
        "                'mean_confidence': post_results['mean_confidence'],\n",
        "                'correct_predictions': post_results['correct_predictions']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        report_path = os.path.join(\n",
        "            self.save_dir,\n",
        "            f'enhancement_report_{breed_name}_{self.timestamp}.json'\n",
        "        )\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "\n",
        "    def plot_training_curves(self, losses, learning_rates, temperatures=None):\n",
        "        \"\"\"\n",
        "        Plot and save training curves for loss, learning rate, and temperature.\n",
        "\n",
        "        Args:\n",
        "            losses: List of loss values recorded during training.\n",
        "            learning_rates: List of learning rates used during training.\n",
        "            temperatures: (Optional) List of temperature values used in training, if applicable.\n",
        "\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Determine the number of subplots based on whether temperature data is available\n",
        "        num_plots = 3 if temperatures is not None else 2\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot the training loss curve\n",
        "        plt.subplot(1, num_plots, 1)\n",
        "        plt.plot(losses)\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        # Plot the learning rate curve\n",
        "        plt.subplot(1, num_plots, 2)\n",
        "        plt.plot(learning_rates)\n",
        "        plt.title('Learning Rate')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.yscale('log')  # Use log scale for better visualization\n",
        "\n",
        "        # If temperature values are provided, plot the temperature curve\n",
        "        if temperatures is not None:\n",
        "            plt.subplot(1, num_plots, 3)\n",
        "            plt.plot(temperatures)\n",
        "            plt.title('Temperature')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Temperature')\n",
        "\n",
        "        # Save the figure\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(\n",
        "            self.visualization_dir,\n",
        "            f'training_curves_{timestamp}.png'\n",
        "        )\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        print(f\"Training curves saved to: {save_path}\")\n",
        "\n",
        "    def plot_lr_finder(self, learning_rates, losses):\n",
        "        \"\"\"\n",
        "        Plot and save the learning rate search results.\n",
        "\n",
        "        Args:\n",
        "            learning_rates: List of tested learning rates.\n",
        "            losses: Corresponding list of loss values for each learning rate.\n",
        "\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Plot loss vs. learning rate on a log scale\n",
        "        plt.plot(learning_rates, losses)\n",
        "        plt.xscale('log')  # Logarithmic scale for better visualization of LR changes\n",
        "        plt.xlabel('Learning Rate')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Learning Rate Finder')\n",
        "\n",
        "        # Save the plot\n",
        "        save_path = os.path.join(\n",
        "            self.visualization_dir,\n",
        "            f'lr_finder_{timestamp}.png'\n",
        "        )\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        print(f\"Learning rate finder plot saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "DCRHn8IguTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to perform breed enhancement using incremental learning.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize performance evaluator\n",
        "    evaluator = PerformanceEvaluator(\n",
        "        save_dir='/content/drive/Othercomputers/My MacBook Pro/Learning/ICARL_PawMatchAI/enhanced_models'\n",
        "    )\n",
        "\n",
        "    # Initialize the incremental learner\n",
        "    learner = ICARLIncrementalLearner(\n",
        "        model_path='/content/drive/Othercomputers/My MacBook Pro/Learning/ICARL_PawMatchAI/enhanced_models/enhanced_toy_poodle_20250201_063556.pth',\n",
        "        json_path='/content/drive/Othercomputers/My MacBook Pro/Learning/ICARL_PawMatchAI/data/dog_breeds.json',\n",
        "        evaluator=evaluator,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "\n",
        "    # Define image transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Load enhancement images\n",
        "    def load_enhancement_images(image_dir):\n",
        "        \"\"\"\n",
        "        Load and preprocess images for breed enhancement.\n",
        "\n",
        "        Args:\n",
        "            image_dir (str): Directory containing images.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A batch of processed images.\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        for img_name in os.listdir(image_dir):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(image_dir, img_name)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = transform(img)\n",
        "                images.append(img)\n",
        "        return torch.stack(images)\n",
        "\n",
        "    # Load images from the specified directory\n",
        "    image_dir = '/content/drive/Othercomputers/My MacBook Pro/Learning/ICARL_PawMatchAI/enhanced_images/bichon_frise'\n",
        "    enhancement_images = load_enhancement_images(image_dir)\n",
        "\n",
        "    # Perform enhancement learning\n",
        "\n",
        "    # Method 1: Automatically determine the learning rate\n",
        "    # learner.enhance_breed(\n",
        "    #     images=enhancement_images,\n",
        "    #     breed_name='havanese',\n",
        "    #     epochs=100,  # Default number of training epochs\n",
        "    #     batch_size=16  # Default batch size\n",
        "    # )\n",
        "\n",
        "    # Method 2: Manually set the learning rate\n",
        "    learner.enhance_breed(\n",
        "        images=enhancement_images,\n",
        "        breed_name='bichon_frise',\n",
        "        epochs=100,\n",
        "        lr=7e-7,  # Manually set the learning rate\n",
        "        batch_size=8\n",
        "    )\n",
        "\n",
        "    # Evaluate the enhancement results\n",
        "    if hasattr(learner, 'test_loader'):\n",
        "        results = learner.evaluator.evaluate(\n",
        "            learner.base_model,\n",
        "            learner.test_loader,\n",
        "            learner.breeds,\n",
        "            learner.device\n",
        "        )\n",
        "\n",
        "        # Retrieve evaluation summary\n",
        "        summary = learner.evaluator.get_evaluation_summary()\n",
        "        print(\"\\nEvaluation results saved in:\", summary['results_dir'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Aj8tTHviuTI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}